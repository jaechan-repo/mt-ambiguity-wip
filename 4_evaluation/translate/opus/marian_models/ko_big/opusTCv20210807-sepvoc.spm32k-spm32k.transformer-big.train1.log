[2022-07-18 20:27:46] [marian] Marian v1.10.24; 12a1bfa 2021-10-11 16:59:52 +0100
[2022-07-18 20:27:46] [marian] Running on r17g08.bullx as process 81992 with command line:
[2022-07-18 20:27:46] [marian] /projappl/project_2001194/marian-dev/build/marian --task transformer-big --optimizer-delay 2 --early-stopping 10 --valid-freq 10000 --valid-sets /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/val/Tatoeba-dev-v2021-08-07.src.spm32k /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/val/Tatoeba-dev-v2021-08-07.trg.spm32k --valid-metrics perplexity --valid-mini-batch 16 --valid-max-length 100 --valid-log /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.valid1.log --beam-size 6 --normalize 1 --allow-unk --workspace 15000 --model /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz --train-sets /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/train/opusTCv20210807.src.clean.spm32k.gz /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/train/opusTCv20210807.trg.clean.spm32k.gz --vocabs /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.src.vocab /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.trg.vocab --save-freq 10000 --disp-freq 10000 --log /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.train1.log --devices 0 --seed 1111 --tempdir /run/nvme/job_12526548/tmp --shuffle batches --sharding local --overwrite --keep-best
[2022-07-18 20:27:46] [config] after: 0e
[2022-07-18 20:27:46] [config] after-batches: 0
[2022-07-18 20:27:46] [config] after-epochs: 0
[2022-07-18 20:27:46] [config] all-caps-every: 0
[2022-07-18 20:27:46] [config] allow-unk: true
[2022-07-18 20:27:46] [config] authors: false
[2022-07-18 20:27:46] [config] beam-size: 6
[2022-07-18 20:27:46] [config] bert-class-symbol: "[CLS]"
[2022-07-18 20:27:46] [config] bert-mask-symbol: "[MASK]"
[2022-07-18 20:27:46] [config] bert-masking-fraction: 0.15
[2022-07-18 20:27:46] [config] bert-sep-symbol: "[SEP]"
[2022-07-18 20:27:46] [config] bert-train-type-embeddings: true
[2022-07-18 20:27:46] [config] bert-type-vocab-size: 2
[2022-07-18 20:27:46] [config] build-info: ""
[2022-07-18 20:27:46] [config] check-gradient-nan: false
[2022-07-18 20:27:46] [config] check-nan: false
[2022-07-18 20:27:46] [config] cite: false
[2022-07-18 20:27:46] [config] clip-norm: 0
[2022-07-18 20:27:46] [config] cost-scaling:
[2022-07-18 20:27:46] [config]   []
[2022-07-18 20:27:46] [config] cost-type: ce-mean-words
[2022-07-18 20:27:46] [config] cpu-threads: 0
[2022-07-18 20:27:46] [config] data-weighting: ""
[2022-07-18 20:27:46] [config] data-weighting-type: sentence
[2022-07-18 20:27:46] [config] dec-cell: gru
[2022-07-18 20:27:46] [config] dec-cell-base-depth: 2
[2022-07-18 20:27:46] [config] dec-cell-high-depth: 1
[2022-07-18 20:27:46] [config] dec-depth: 6
[2022-07-18 20:27:46] [config] devices:
[2022-07-18 20:27:46] [config]   - 0
[2022-07-18 20:27:46] [config] dim-emb: 1024
[2022-07-18 20:27:46] [config] dim-rnn: 1024
[2022-07-18 20:27:46] [config] dim-vocabs:
[2022-07-18 20:27:46] [config]   - 0
[2022-07-18 20:27:46] [config]   - 0
[2022-07-18 20:27:46] [config] disp-first: 0
[2022-07-18 20:27:46] [config] disp-freq: 10000
[2022-07-18 20:27:46] [config] disp-label-counts: true
[2022-07-18 20:27:46] [config] dropout-rnn: 0
[2022-07-18 20:27:46] [config] dropout-src: 0
[2022-07-18 20:27:46] [config] dropout-trg: 0
[2022-07-18 20:27:46] [config] dump-config: ""
[2022-07-18 20:27:46] [config] dynamic-gradient-scaling:
[2022-07-18 20:27:46] [config]   []
[2022-07-18 20:27:46] [config] early-stopping: 10
[2022-07-18 20:27:46] [config] early-stopping-on: first
[2022-07-18 20:27:46] [config] embedding-fix-src: false
[2022-07-18 20:27:46] [config] embedding-fix-trg: false
[2022-07-18 20:27:46] [config] embedding-normalization: false
[2022-07-18 20:27:46] [config] embedding-vectors:
[2022-07-18 20:27:46] [config]   []
[2022-07-18 20:27:46] [config] enc-cell: gru
[2022-07-18 20:27:46] [config] enc-cell-depth: 1
[2022-07-18 20:27:46] [config] enc-depth: 6
[2022-07-18 20:27:46] [config] enc-type: bidirectional
[2022-07-18 20:27:46] [config] english-title-case-every: 0
[2022-07-18 20:27:46] [config] exponential-smoothing: 0.0001
[2022-07-18 20:27:46] [config] factor-weight: 1
[2022-07-18 20:27:46] [config] factors-combine: sum
[2022-07-18 20:27:46] [config] factors-dim-emb: 0
[2022-07-18 20:27:46] [config] gradient-checkpointing: false
[2022-07-18 20:27:46] [config] gradient-norm-average-window: 100
[2022-07-18 20:27:46] [config] guided-alignment: none
[2022-07-18 20:27:46] [config] guided-alignment-cost: mse
[2022-07-18 20:27:46] [config] guided-alignment-weight: 0.1
[2022-07-18 20:27:46] [config] ignore-model-config: false
[2022-07-18 20:27:46] [config] input-types:
[2022-07-18 20:27:46] [config]   []
[2022-07-18 20:27:46] [config] interpolate-env-vars: false
[2022-07-18 20:27:46] [config] keep-best: true
[2022-07-18 20:27:46] [config] label-smoothing: 0.1
[2022-07-18 20:27:46] [config] layer-normalization: false
[2022-07-18 20:27:46] [config] learn-rate: 0.0002
[2022-07-18 20:27:46] [config] lemma-dependency: ""
[2022-07-18 20:27:46] [config] lemma-dim-emb: 0
[2022-07-18 20:27:46] [config] log: /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.train1.log
[2022-07-18 20:27:46] [config] log-level: info
[2022-07-18 20:27:46] [config] log-time-zone: ""
[2022-07-18 20:27:46] [config] logical-epoch:
[2022-07-18 20:27:46] [config]   - 1e
[2022-07-18 20:27:46] [config]   - 0
[2022-07-18 20:27:46] [config] lr-decay: 0
[2022-07-18 20:27:46] [config] lr-decay-freq: 50000
[2022-07-18 20:27:46] [config] lr-decay-inv-sqrt:
[2022-07-18 20:27:46] [config]   - 8000
[2022-07-18 20:27:46] [config] lr-decay-repeat-warmup: false
[2022-07-18 20:27:46] [config] lr-decay-reset-optimizer: false
[2022-07-18 20:27:46] [config] lr-decay-start:
[2022-07-18 20:27:46] [config]   - 10
[2022-07-18 20:27:46] [config]   - 1
[2022-07-18 20:27:46] [config] lr-decay-strategy: epoch+stalled
[2022-07-18 20:27:46] [config] lr-report: false
[2022-07-18 20:27:46] [config] lr-warmup: 8000
[2022-07-18 20:27:46] [config] lr-warmup-at-reload: false
[2022-07-18 20:27:46] [config] lr-warmup-cycle: false
[2022-07-18 20:27:46] [config] lr-warmup-start-rate: 0
[2022-07-18 20:27:46] [config] max-length: 100
[2022-07-18 20:27:46] [config] max-length-crop: false
[2022-07-18 20:27:46] [config] max-length-factor: 3
[2022-07-18 20:27:46] [config] maxi-batch: 1000
[2022-07-18 20:27:46] [config] maxi-batch-sort: trg
[2022-07-18 20:27:46] [config] mini-batch: 1000
[2022-07-18 20:27:46] [config] mini-batch-fit: true
[2022-07-18 20:27:46] [config] mini-batch-fit-step: 10
[2022-07-18 20:27:46] [config] mini-batch-round-up: true
[2022-07-18 20:27:46] [config] mini-batch-track-lr: false
[2022-07-18 20:27:46] [config] mini-batch-warmup: 0
[2022-07-18 20:27:46] [config] mini-batch-words: 0
[2022-07-18 20:27:46] [config] mini-batch-words-ref: 0
[2022-07-18 20:27:46] [config] model: /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz
[2022-07-18 20:27:46] [config] multi-loss-type: sum
[2022-07-18 20:27:46] [config] n-best: false
[2022-07-18 20:27:46] [config] no-nccl: false
[2022-07-18 20:27:46] [config] no-reload: false
[2022-07-18 20:27:46] [config] no-restore-corpus: false
[2022-07-18 20:27:46] [config] normalize: 1
[2022-07-18 20:27:46] [config] normalize-gradient: false
[2022-07-18 20:27:46] [config] num-devices: 0
[2022-07-18 20:27:46] [config] optimizer: adam
[2022-07-18 20:27:46] [config] optimizer-delay: 2
[2022-07-18 20:27:46] [config] optimizer-params:
[2022-07-18 20:27:46] [config]   - 0.9
[2022-07-18 20:27:46] [config]   - 0.998
[2022-07-18 20:27:46] [config]   - 1e-09
[2022-07-18 20:27:46] [config] output-omit-bias: false
[2022-07-18 20:27:46] [config] overwrite: true
[2022-07-18 20:27:46] [config] precision:
[2022-07-18 20:27:46] [config]   - float32
[2022-07-18 20:27:46] [config]   - float32
[2022-07-18 20:27:46] [config] pretrained-model: ""
[2022-07-18 20:27:46] [config] quantize-biases: false
[2022-07-18 20:27:46] [config] quantize-bits: 0
[2022-07-18 20:27:46] [config] quantize-log-based: false
[2022-07-18 20:27:46] [config] quantize-optimization-steps: 0
[2022-07-18 20:27:46] [config] quiet: false
[2022-07-18 20:27:46] [config] quiet-translation: false
[2022-07-18 20:27:46] [config] relative-paths: false
[2022-07-18 20:27:46] [config] right-left: false
[2022-07-18 20:27:46] [config] save-freq: 10000
[2022-07-18 20:27:46] [config] seed: 1111
[2022-07-18 20:27:46] [config] sentencepiece-alphas:
[2022-07-18 20:27:46] [config]   []
[2022-07-18 20:27:46] [config] sentencepiece-max-lines: 2000000
[2022-07-18 20:27:46] [config] sentencepiece-options: ""
[2022-07-18 20:27:46] [config] sharding: local
[2022-07-18 20:27:46] [config] shuffle: batches
[2022-07-18 20:27:46] [config] shuffle-in-ram: false
[2022-07-18 20:27:46] [config] sigterm: save-and-exit
[2022-07-18 20:27:46] [config] skip: false
[2022-07-18 20:27:46] [config] sqlite: ""
[2022-07-18 20:27:46] [config] sqlite-drop: false
[2022-07-18 20:27:46] [config] sync-freq: 200u
[2022-07-18 20:27:46] [config] sync-sgd: true
[2022-07-18 20:27:46] [config] tempdir: /run/nvme/job_12526548/tmp
[2022-07-18 20:27:46] [config] tied-embeddings: false
[2022-07-18 20:27:46] [config] tied-embeddings-all: true
[2022-07-18 20:27:46] [config] tied-embeddings-src: false
[2022-07-18 20:27:46] [config] train-embedder-rank:
[2022-07-18 20:27:46] [config]   []
[2022-07-18 20:27:46] [config] train-sets:
[2022-07-18 20:27:46] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/train/opusTCv20210807.src.clean.spm32k.gz
[2022-07-18 20:27:46] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/train/opusTCv20210807.trg.clean.spm32k.gz
[2022-07-18 20:27:46] [config] transformer-aan-activation: swish
[2022-07-18 20:27:46] [config] transformer-aan-depth: 2
[2022-07-18 20:27:46] [config] transformer-aan-nogate: false
[2022-07-18 20:27:46] [config] transformer-decoder-autoreg: self-attention
[2022-07-18 20:27:46] [config] transformer-depth-scaling: false
[2022-07-18 20:27:46] [config] transformer-dim-aan: 2048
[2022-07-18 20:27:46] [config] transformer-dim-ffn: 4096
[2022-07-18 20:27:46] [config] transformer-dropout: 0.1
[2022-07-18 20:27:46] [config] transformer-dropout-attention: 0
[2022-07-18 20:27:46] [config] transformer-dropout-ffn: 0
[2022-07-18 20:27:46] [config] transformer-ffn-activation: relu
[2022-07-18 20:27:46] [config] transformer-ffn-depth: 2
[2022-07-18 20:27:46] [config] transformer-guided-alignment-layer: last
[2022-07-18 20:27:46] [config] transformer-heads: 16
[2022-07-18 20:27:46] [config] transformer-no-projection: false
[2022-07-18 20:27:46] [config] transformer-pool: false
[2022-07-18 20:27:46] [config] transformer-postprocess: dan
[2022-07-18 20:27:46] [config] transformer-postprocess-emb: d
[2022-07-18 20:27:46] [config] transformer-postprocess-top: ""
[2022-07-18 20:27:46] [config] transformer-preprocess: ""
[2022-07-18 20:27:46] [config] transformer-tied-layers:
[2022-07-18 20:27:46] [config]   []
[2022-07-18 20:27:46] [config] transformer-train-position-embeddings: false
[2022-07-18 20:27:46] [config] tsv: false
[2022-07-18 20:27:46] [config] tsv-fields: 0
[2022-07-18 20:27:46] [config] type: transformer
[2022-07-18 20:27:46] [config] ulr: false
[2022-07-18 20:27:46] [config] ulr-dim-emb: 0
[2022-07-18 20:27:46] [config] ulr-dropout: 0
[2022-07-18 20:27:46] [config] ulr-keys-vectors: ""
[2022-07-18 20:27:46] [config] ulr-query-vectors: ""
[2022-07-18 20:27:46] [config] ulr-softmax-temperature: 1
[2022-07-18 20:27:46] [config] ulr-trainable-transformation: false
[2022-07-18 20:27:46] [config] unlikelihood-loss: false
[2022-07-18 20:27:46] [config] valid-freq: 10000
[2022-07-18 20:27:46] [config] valid-log: /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.valid1.log
[2022-07-18 20:27:46] [config] valid-max-length: 100
[2022-07-18 20:27:46] [config] valid-metrics:
[2022-07-18 20:27:46] [config]   - perplexity
[2022-07-18 20:27:46] [config] valid-mini-batch: 16
[2022-07-18 20:27:46] [config] valid-reset-stalled: false
[2022-07-18 20:27:46] [config] valid-script-args:
[2022-07-18 20:27:46] [config]   []
[2022-07-18 20:27:46] [config] valid-script-path: ""
[2022-07-18 20:27:46] [config] valid-sets:
[2022-07-18 20:27:46] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/val/Tatoeba-dev-v2021-08-07.src.spm32k
[2022-07-18 20:27:46] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/val/Tatoeba-dev-v2021-08-07.trg.spm32k
[2022-07-18 20:27:46] [config] valid-translation-output: ""
[2022-07-18 20:27:46] [config] vocabs:
[2022-07-18 20:27:46] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.src.vocab
[2022-07-18 20:27:46] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.trg.vocab
[2022-07-18 20:27:46] [config] word-penalty: 0
[2022-07-18 20:27:46] [config] word-scores: false
[2022-07-18 20:27:46] [config] workspace: 15000
[2022-07-18 20:27:46] [config] Model is being created with Marian v1.10.24; 12a1bfa 2021-10-11 16:59:52 +0100
[2022-07-18 20:27:46] Using synchronous SGD
[2022-07-18 20:27:46] [comm] Compiled without MPI support. Running as a single process on r17g08.bullx
[2022-07-18 20:27:46] Synced seed 1111
[2022-07-18 20:27:46] [data] Loading vocabulary from text file /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.src.vocab
[2022-07-18 20:27:46] [data] Setting vocabulary size for input 0 to 32,000
[2022-07-18 20:27:46] [data] Loading vocabulary from text file /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.trg.vocab
[2022-07-18 20:27:46] [data] Setting vocabulary size for input 1 to 32,000
[2022-07-18 20:27:47] [batching] Collecting statistics for batch fitting with step size 10
[2022-07-18 20:27:47] [memory] Extending reserved space to 15104 MB (device gpu0)
[2022-07-18 20:27:48] [comm] Using NCCL 2.8.3 for GPU communication
[2022-07-18 20:27:48] [comm] Using global sharding
[2022-07-18 20:27:48] [comm] NCCLCommunicators constructed successfully
[2022-07-18 20:27:48] [training] Using 1 GPUs
[2022-07-18 20:27:48] [logits] Applying loss function for 1 factor(s)
[2022-07-18 20:27:48] [memory] Reserving 797 MB, device gpu0
[2022-07-18 20:27:49] [gpu] 16-bit TensorCores enabled for float32 matrix operations
[2022-07-18 20:27:49] [memory] Reserving 797 MB, device gpu0
[2022-07-18 20:28:13] [batching] Done. Typical MB size is 14,942 target words
[2022-07-18 20:28:13] [memory] Extending reserved space to 15104 MB (device gpu0)
[2022-07-18 20:28:13] [comm] Using NCCL 2.8.3 for GPU communication
[2022-07-18 20:28:13] [comm] Using global sharding
[2022-07-18 20:28:14] [comm] NCCLCommunicators constructed successfully
[2022-07-18 20:28:14] [training] Using 1 GPUs
[2022-07-18 20:28:14] Training started
[2022-07-18 20:28:35] [training] Batches are processed as 1 process(es) x 1 devices/process
[2022-07-18 20:28:35] [memory] Reserving 797 MB, device gpu0
[2022-07-18 20:28:35] [memory] Reserving 797 MB, device gpu0
[2022-07-18 20:28:36] Parameter type float32, optimization type float32, casting types false
[2022-07-18 20:28:36] Allocating memory for general optimizer shards
[2022-07-18 20:28:36] [memory] Reserving 797 MB, device gpu0
[2022-07-18 20:28:36] Allocating memory for Adam-specific shards
[2022-07-18 20:28:36] [memory] Reserving 1595 MB, device gpu0
[2022-07-18 22:36:24] Ep. 1 : Up. 10000 : Sen. 6,955,840 : Cost 5.81855249 : Time 7690.94s : 14589.17 words/s : gNorm 0.9839
[2022-07-18 22:36:24] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz
[2022-07-18 22:36:30] Saving Adam parameters
[2022-07-18 22:36:35] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-07-18 22:36:46] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-07-18 22:36:50] [valid] Ep. 1 : Up. 10000 : perplexity : 7.36416 : new best
[2022-07-19 00:44:27] Ep. 1 : Up. 20000 : Sen. 13,922,875 : Cost 4.20975685 : Time 7682.73s : 14604.29 words/s : gNorm 1.0445
[2022-07-19 00:44:27] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz
[2022-07-19 00:44:30] Saving Adam parameters
[2022-07-19 00:44:32] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-07-19 00:44:44] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-07-19 00:44:47] [valid] Ep. 1 : Up. 20000 : perplexity : 4.04134 : new best
[2022-07-19 02:52:17] Ep. 1 : Up. 30000 : Sen. 20,925,197 : Cost 3.43695903 : Time 7670.29s : 14588.55 words/s : gNorm 0.9208
[2022-07-19 02:52:17] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz
[2022-07-19 02:52:21] Saving Adam parameters
[2022-07-19 02:52:24] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-07-19 02:52:35] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-07-19 02:52:37] [valid] Ep. 1 : Up. 30000 : perplexity : 3.33847 : new best
[2022-07-19 04:59:56] Ep. 1 : Up. 40000 : Sen. 28,093,840 : Cost 3.19690013 : Time 7658.37s : 14530.72 words/s : gNorm 0.6826
[2022-07-19 04:59:56] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz
[2022-07-19 04:59:59] Saving Adam parameters
[2022-07-19 05:00:01] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-07-19 05:00:13] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-07-19 05:00:15] [valid] Ep. 1 : Up. 40000 : perplexity : 3.25252 : new best
[2022-07-19 05:49:42] Seen 30,921,555 samples
[2022-07-19 05:49:42] Starting data epoch 2 in logical epoch 2
[2022-07-19 07:07:32] Ep. 2 : Up. 50000 : Sen. 4,205,007 : Cost 3.12791634 : Time 7656.33s : 14587.35 words/s : gNorm 0.6772
[2022-07-19 07:07:32] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz
[2022-07-19 07:07:36] Saving Adam parameters
[2022-07-19 07:07:39] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-07-19 07:07:49] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-07-19 07:07:52] [valid] Ep. 2 : Up. 50000 : perplexity : 3.03522 : new best
[2022-07-19 09:15:28] Ep. 2 : Up. 60000 : Sen. 11,163,544 : Cost 3.06925559 : Time 7675.89s : 14609.89 words/s : gNorm 0.7097
[2022-07-19 09:15:28] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz
[2022-07-19 09:15:32] Saving Adam parameters
[2022-07-19 09:15:35] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-07-19 09:15:41] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-07-19 09:15:43] [valid] Ep. 2 : Up. 60000 : perplexity : 2.9095 : new best
[2022-07-19 11:23:29] Ep. 2 : Up. 70000 : Sen. 18,120,017 : Cost 3.01823401 : Time 7680.71s : 14608.59 words/s : gNorm 0.6656
[2022-07-19 11:23:29] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz
[2022-07-19 11:23:33] Saving Adam parameters
[2022-07-19 11:23:36] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-07-19 11:23:43] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-07-19 11:23:45] [valid] Ep. 2 : Up. 70000 : perplexity : 2.84566 : new best
[2022-07-19 13:30:58] Ep. 2 : Up. 80000 : Sen. 25,268,561 : Cost 2.96825552 : Time 7649.34s : 14553.16 words/s : gNorm 0.6526
[2022-07-19 13:30:58] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz
[2022-07-19 13:31:01] Saving Adam parameters
[2022-07-19 13:31:04] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-07-19 13:31:11] [valid] Ep. 2 : Up. 80000 : perplexity : 2.8826 : stalled 1 times (last best: 2.84566)
[2022-07-19 15:11:03] Seen 30,921,555 samples
[2022-07-19 15:11:03] Starting data epoch 3 in logical epoch 3
[2022-07-19 15:38:19] Ep. 3 : Up. 90000 : Sen. 1,462,215 : Cost 2.93643093 : Time 7640.78s : 14566.16 words/s : gNorm 0.6877
[2022-07-19 15:38:19] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz
[2022-07-19 15:38:24] Saving Adam parameters
[2022-07-19 15:38:27] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-07-19 15:38:45] [valid] Ep. 3 : Up. 90000 : perplexity : 2.88449 : stalled 2 times (last best: 2.84566)
[2022-07-19 17:46:30] Ep. 3 : Up. 100000 : Sen. 8,419,505 : Cost 2.93309236 : Time 7690.90s : 14604.59 words/s : gNorm 0.6568
[2022-07-19 17:46:30] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz
[2022-07-19 17:46:36] Saving Adam parameters
[2022-07-19 17:46:38] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-07-19 17:46:46] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-07-19 17:46:48] [valid] Ep. 3 : Up. 100000 : perplexity : 2.79084 : new best
[2022-07-19 19:54:44] Ep. 3 : Up. 110000 : Sen. 15,366,328 : Cost 2.90729833 : Time 7693.99s : 14576.45 words/s : gNorm 0.6623
[2022-07-19 19:54:44] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz
[2022-07-19 19:54:49] Saving Adam parameters
[2022-07-19 19:54:52] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-07-19 19:55:04] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-07-19 19:55:06] [valid] Ep. 3 : Up. 110000 : perplexity : 2.74123 : new best
[2022-07-19 22:02:27] Ep. 3 : Up. 120000 : Sen. 22,451,715 : Cost 2.88561010 : Time 7663.51s : 14570.65 words/s : gNorm 0.6692
[2022-07-19 22:02:27] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz
[2022-07-19 22:02:31] Saving Adam parameters
[2022-07-19 22:02:34] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-07-19 22:02:45] [valid] Ep. 3 : Up. 120000 : perplexity : 2.75143 : stalled 1 times (last best: 2.74123)
[2022-07-20 00:09:39] Ep. 3 : Up. 130000 : Sen. 29,591,146 : Cost 2.85578275 : Time 7631.99s : 14572.47 words/s : gNorm 0.6911
[2022-07-20 00:09:39] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz
[2022-07-20 00:09:42] Saving Adam parameters
[2022-07-20 00:09:45] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-07-20 00:09:56] [valid] Ep. 3 : Up. 130000 : perplexity : 2.8213 : stalled 2 times (last best: 2.74123)
[2022-07-20 00:33:02] Seen 30,921,555 samples
[2022-07-20 00:33:02] Starting data epoch 4 in logical epoch 4
[2022-07-20 02:17:31] Ep. 4 : Up. 140000 : Sen. 5,668,320 : Cost 2.86202025 : Time 7672.10s : 14600.91 words/s : gNorm 0.6816
[2022-07-20 02:17:31] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz
[2022-07-20 02:17:35] Saving Adam parameters
[2022-07-20 02:17:38] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-07-20 02:17:50] [valid] Ep. 4 : Up. 140000 : perplexity : 2.7505 : stalled 3 times (last best: 2.74123)
[2022-07-20 04:25:10] Ep. 4 : Up. 150000 : Sen. 12,633,454 : Cost 2.84719539 : Time 7658.74s : 14636.62 words/s : gNorm 0.7096
[2022-07-20 04:25:10] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz
[2022-07-20 04:25:13] Saving Adam parameters
[2022-07-20 04:25:16] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-07-20 04:25:26] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-07-20 04:25:29] [valid] Ep. 4 : Up. 150000 : perplexity : 2.69815 : new best
[2022-07-20 06:33:02] Ep. 4 : Up. 160000 : Sen. 19,604,641 : Cost 2.83529830 : Time 7671.80s : 14618.80 words/s : gNorm 0.6727
[2022-07-20 06:33:02] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz
[2022-07-20 06:33:06] Saving Adam parameters
[2022-07-20 06:33:09] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-07-20 06:33:19] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-07-20 06:33:21] [valid] Ep. 4 : Up. 160000 : perplexity : 2.67654 : new best
[2022-07-20 08:40:27] Ep. 4 : Up. 170000 : Sen. 26,760,750 : Cost 2.81412053 : Time 7645.39s : 14540.86 words/s : gNorm 0.6992
[2022-07-20 08:40:27] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz
[2022-07-20 08:40:31] Saving Adam parameters
[2022-07-20 08:40:35] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-07-20 08:40:45] [valid] Ep. 4 : Up. 170000 : perplexity : 2.75855 : stalled 1 times (last best: 2.67654)
[2022-07-20 09:54:19] Seen 30,921,555 samples
[2022-07-20 09:54:19] Starting data epoch 5 in logical epoch 5
[2022-07-20 10:48:28] Ep. 5 : Up. 180000 : Sen. 2,917,665 : Cost 2.80931306 : Time 7680.21s : 14524.69 words/s : gNorm 0.7020
[2022-07-20 10:48:28] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz
[2022-07-20 10:48:32] Saving Adam parameters
[2022-07-20 10:48:35] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-07-20 10:48:49] [valid] Ep. 5 : Up. 180000 : perplexity : 2.74562 : stalled 2 times (last best: 2.67654)
[2022-07-20 12:56:58] Ep. 5 : Up. 190000 : Sen. 9,881,016 : Cost 2.80858922 : Time 7710.71s : 14559.02 words/s : gNorm 0.7278
[2022-07-20 12:56:58] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz
[2022-07-20 12:57:02] Saving Adam parameters
[2022-07-20 12:57:05] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-07-20 12:57:16] [valid] Ep. 5 : Up. 190000 : perplexity : 2.67942 : stalled 3 times (last best: 2.67654)
[2022-07-20 15:05:27] Ep. 5 : Up. 200000 : Sen. 16,838,494 : Cost 2.79872036 : Time 7708.59s : 14546.83 words/s : gNorm 0.7635
[2022-07-20 15:05:27] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz
[2022-07-20 15:05:32] Saving Adam parameters
[2022-07-20 15:05:35] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-07-20 15:05:42] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-07-20 15:05:45] [valid] Ep. 5 : Up. 200000 : perplexity : 2.65078 : new best
[2022-07-20 17:13:35] Ep. 5 : Up. 210000 : Sen. 23,933,392 : Cost 2.78607512 : Time 7688.14s : 14503.12 words/s : gNorm 0.7151
[2022-07-20 17:13:35] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz
[2022-07-20 17:13:39] Saving Adam parameters
[2022-07-20 17:13:42] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-07-20 17:13:49] [valid] Ep. 5 : Up. 210000 : perplexity : 2.69662 : stalled 1 times (last best: 2.65078)
[2022-07-20 19:17:51] Seen 30,921,555 samples
[2022-07-20 19:17:51] Starting data epoch 6 in logical epoch 6
[2022-07-20 19:21:38] Ep. 6 : Up. 220000 : Sen. 171,470 : Cost 2.77178311 : Time 7683.13s : 14462.79 words/s : gNorm 0.7343
[2022-07-20 19:21:38] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz
[2022-07-20 19:21:43] Saving Adam parameters
[2022-07-20 19:21:46] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-07-20 19:21:56] [valid] Ep. 6 : Up. 220000 : perplexity : 2.75855 : stalled 2 times (last best: 2.65078)
[2022-07-20 21:30:00] Ep. 6 : Up. 230000 : Sen. 7,128,278 : Cost 2.78214097 : Time 7701.49s : 14579.85 words/s : gNorm 0.7471
[2022-07-20 21:30:00] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz
[2022-07-20 21:30:04] Saving Adam parameters
[2022-07-20 21:30:07] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-07-20 21:30:18] [valid] Ep. 6 : Up. 230000 : perplexity : 2.67315 : stalled 3 times (last best: 2.65078)
[2022-07-20 23:38:09] Ep. 6 : Up. 240000 : Sen. 14,096,366 : Cost 2.77093697 : Time 7689.43s : 14584.33 words/s : gNorm 0.7505
[2022-07-20 23:38:09] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz
[2022-07-20 23:38:12] Saving Adam parameters
[2022-07-20 23:38:15] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-07-20 23:38:27] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-07-20 23:38:29] [valid] Ep. 6 : Up. 240000 : perplexity : 2.63757 : new best
[2022-07-21 01:46:12] Ep. 6 : Up. 250000 : Sen. 21,097,602 : Cost 2.76422548 : Time 7682.80s : 14570.71 words/s : gNorm 0.7077
[2022-07-21 01:46:12] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz
[2022-07-21 01:46:17] Saving Adam parameters
[2022-07-21 01:46:19] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-07-21 01:46:27] [valid] Ep. 6 : Up. 250000 : perplexity : 2.64212 : stalled 1 times (last best: 2.63757)
[2022-07-21 03:52:25] Ep. 6 : Up. 260000 : Sen. 28,282,024 : Cost 2.75101662 : Time 7573.07s : 14673.21 words/s : gNorm 0.7300
[2022-07-21 03:52:25] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz
[2022-07-21 03:52:28] Saving Adam parameters
[2022-07-21 03:52:31] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-07-21 03:52:37] [valid] Ep. 6 : Up. 260000 : perplexity : 2.72945 : stalled 2 times (last best: 2.63757)
[2022-07-21 04:38:48] Seen 30,921,555 samples
[2022-07-21 04:38:48] Starting data epoch 7 in logical epoch 7
[2022-07-21 05:58:49] Ep. 7 : Up. 270000 : Sen. 4,379,283 : Cost 2.75219440 : Time 7583.35s : 14753.65 words/s : gNorm 0.7312
[2022-07-21 05:58:49] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz
[2022-07-21 05:58:52] Saving Adam parameters
[2022-07-21 05:58:55] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-07-21 05:59:07] [valid] Ep. 7 : Up. 270000 : perplexity : 2.68892 : stalled 3 times (last best: 2.63757)
[2022-07-21 08:05:16] Ep. 7 : Up. 280000 : Sen. 11,347,321 : Cost 2.75043082 : Time 7587.15s : 14779.17 words/s : gNorm 0.7246
[2022-07-21 08:05:16] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz
[2022-07-21 08:05:19] Saving Adam parameters
[2022-07-21 08:05:22] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-07-21 08:05:33] [valid] Ep. 7 : Up. 280000 : perplexity : 2.64066 : stalled 4 times (last best: 2.63757)
[2022-07-21 10:11:58] Ep. 7 : Up. 290000 : Sen. 18,283,763 : Cost 2.74466181 : Time 7602.60s : 14754.84 words/s : gNorm 0.7447
[2022-07-21 10:11:58] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz
[2022-07-21 10:12:02] Saving Adam parameters
[2022-07-21 10:12:05] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-07-21 10:12:11] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-07-21 10:12:13] [valid] Ep. 7 : Up. 290000 : perplexity : 2.6187 : new best
[2022-07-21 12:18:18] Ep. 7 : Up. 300000 : Sen. 25,449,005 : Cost 2.73456955 : Time 7580.07s : 14695.72 words/s : gNorm 0.7312
[2022-07-21 12:18:18] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz
[2022-07-21 12:18:21] Saving Adam parameters
[2022-07-21 12:18:24] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-07-21 12:18:30] [valid] Ep. 7 : Up. 300000 : perplexity : 2.68842 : stalled 1 times (last best: 2.6187)
[2022-07-21 13:54:26] Seen 30,921,555 samples
[2022-07-21 13:54:26] Starting data epoch 8 in logical epoch 8
[2022-07-21 14:24:29] Ep. 8 : Up. 310000 : Sen. 1,644,688 : Cost 2.72953939 : Time 7570.64s : 14706.27 words/s : gNorm 0.7689
[2022-07-21 14:24:29] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz
[2022-07-21 14:24:32] Saving Adam parameters
[2022-07-21 14:24:34] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-07-21 14:24:40] [valid] Ep. 8 : Up. 310000 : perplexity : 2.71338 : stalled 2 times (last best: 2.6187)
[2022-07-21 16:31:18] Ep. 8 : Up. 320000 : Sen. 8,595,373 : Cost 2.73347521 : Time 7609.24s : 14745.58 words/s : gNorm 0.7385
[2022-07-21 16:31:18] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz
[2022-07-21 16:31:21] Saving Adam parameters
[2022-07-21 16:31:24] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-07-21 16:31:30] [valid] Ep. 8 : Up. 320000 : perplexity : 2.64029 : stalled 3 times (last best: 2.6187)
[2022-07-21 18:37:53] Ep. 8 : Up. 330000 : Sen. 15,548,542 : Cost 2.72928691 : Time 7594.53s : 14756.11 words/s : gNorm 0.8142
[2022-07-21 18:37:53] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz
[2022-07-21 18:37:56] Saving Adam parameters
[2022-07-21 18:37:59] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-07-21 18:38:05] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-07-21 18:38:07] [valid] Ep. 8 : Up. 330000 : perplexity : 2.61447 : new best
[2022-07-22 03:22:55] [marian] Marian v1.10.24; 12a1bfa 2021-10-11 16:59:52 +0100
[2022-07-22 03:22:55] [marian] Running on r18g03.bullx as process 28519 with command line:
[2022-07-22 03:22:55] [marian] /projappl/project_2001194/marian-dev/build/marian --task transformer-big --optimizer-delay 2 --early-stopping 10 --valid-freq 10000 --valid-sets /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/val/Tatoeba-dev-v2021-08-07.src.spm32k /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/val/Tatoeba-dev-v2021-08-07.trg.spm32k --valid-metrics perplexity --valid-mini-batch 16 --valid-max-length 100 --valid-log /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.valid1.log --beam-size 6 --normalize 1 --allow-unk --workspace 15000 --model /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz --train-sets /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/train/opusTCv20210807.src.clean.spm32k.gz /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/train/opusTCv20210807.trg.clean.spm32k.gz --vocabs /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.src.vocab /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.trg.vocab --save-freq 10000 --disp-freq 10000 --log /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.train1.log --devices 0 1 --seed 1111 --tempdir /run/nvme/job_12526782/tmp --shuffle batches --sharding local --overwrite --keep-best
[2022-07-22 03:22:58] [config] after: 0e
[2022-07-22 03:22:58] [config] after-batches: 0
[2022-07-22 03:22:58] [config] after-epochs: 0
[2022-07-22 03:22:58] [config] all-caps-every: 0
[2022-07-22 03:22:58] [config] allow-unk: true
[2022-07-22 03:22:58] [config] authors: false
[2022-07-22 03:22:58] [config] beam-size: 6
[2022-07-22 03:22:58] [config] bert-class-symbol: "[CLS]"
[2022-07-22 03:22:58] [config] bert-mask-symbol: "[MASK]"
[2022-07-22 03:22:58] [config] bert-masking-fraction: 0.15
[2022-07-22 03:22:58] [config] bert-sep-symbol: "[SEP]"
[2022-07-22 03:22:58] [config] bert-train-type-embeddings: true
[2022-07-22 03:22:58] [config] bert-type-vocab-size: 2
[2022-07-22 03:22:58] [config] build-info: ""
[2022-07-22 03:22:58] [config] check-gradient-nan: false
[2022-07-22 03:22:58] [config] check-nan: false
[2022-07-22 03:22:58] [config] cite: false
[2022-07-22 03:22:58] [config] clip-norm: 0
[2022-07-22 03:22:58] [config] cost-scaling:
[2022-07-22 03:22:58] [config]   []
[2022-07-22 03:22:58] [config] cost-type: ce-mean-words
[2022-07-22 03:22:58] [config] cpu-threads: 0
[2022-07-22 03:22:58] [config] data-weighting: ""
[2022-07-22 03:22:58] [config] data-weighting-type: sentence
[2022-07-22 03:22:58] [config] dec-cell: gru
[2022-07-22 03:22:58] [config] dec-cell-base-depth: 2
[2022-07-22 03:22:58] [config] dec-cell-high-depth: 1
[2022-07-22 03:22:58] [config] dec-depth: 6
[2022-07-22 03:22:58] [config] devices:
[2022-07-22 03:22:58] [config]   - 0
[2022-07-22 03:22:58] [config]   - 1
[2022-07-22 03:22:58] [config] dim-emb: 1024
[2022-07-22 03:22:58] [config] dim-rnn: 1024
[2022-07-22 03:22:58] [config] dim-vocabs:
[2022-07-22 03:22:58] [config]   - 32000
[2022-07-22 03:22:58] [config]   - 32000
[2022-07-22 03:22:58] [config] disp-first: 0
[2022-07-22 03:22:58] [config] disp-freq: 10000
[2022-07-22 03:22:58] [config] disp-label-counts: true
[2022-07-22 03:22:58] [config] dropout-rnn: 0
[2022-07-22 03:22:58] [config] dropout-src: 0
[2022-07-22 03:22:58] [config] dropout-trg: 0
[2022-07-22 03:22:58] [config] dump-config: ""
[2022-07-22 03:22:58] [config] dynamic-gradient-scaling:
[2022-07-22 03:22:58] [config]   []
[2022-07-22 03:22:58] [config] early-stopping: 10
[2022-07-22 03:22:58] [config] early-stopping-on: first
[2022-07-22 03:22:58] [config] embedding-fix-src: false
[2022-07-22 03:22:58] [config] embedding-fix-trg: false
[2022-07-22 03:22:58] [config] embedding-normalization: false
[2022-07-22 03:22:58] [config] embedding-vectors:
[2022-07-22 03:22:58] [config]   []
[2022-07-22 03:22:58] [config] enc-cell: gru
[2022-07-22 03:22:58] [config] enc-cell-depth: 1
[2022-07-22 03:22:58] [config] enc-depth: 6
[2022-07-22 03:22:58] [config] enc-type: bidirectional
[2022-07-22 03:22:58] [config] english-title-case-every: 0
[2022-07-22 03:22:58] [config] exponential-smoothing: 0.0001
[2022-07-22 03:22:58] [config] factor-weight: 1
[2022-07-22 03:22:58] [config] factors-combine: sum
[2022-07-22 03:22:58] [config] factors-dim-emb: 0
[2022-07-22 03:22:58] [config] gradient-checkpointing: false
[2022-07-22 03:22:58] [config] gradient-norm-average-window: 100
[2022-07-22 03:22:58] [config] guided-alignment: none
[2022-07-22 03:22:58] [config] guided-alignment-cost: mse
[2022-07-22 03:22:58] [config] guided-alignment-weight: 0.1
[2022-07-22 03:22:58] [config] ignore-model-config: false
[2022-07-22 03:22:58] [config] input-types:
[2022-07-22 03:22:58] [config]   []
[2022-07-22 03:22:58] [config] interpolate-env-vars: false
[2022-07-22 03:22:58] [config] keep-best: true
[2022-07-22 03:22:58] [config] label-smoothing: 0.1
[2022-07-22 03:22:58] [config] layer-normalization: false
[2022-07-22 03:22:58] [config] learn-rate: 0.0002
[2022-07-22 03:22:58] [config] lemma-dependency: ""
[2022-07-22 03:22:58] [config] lemma-dim-emb: 0
[2022-07-22 03:22:58] [config] log: /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.train1.log
[2022-07-22 03:22:58] [config] log-level: info
[2022-07-22 03:22:58] [config] log-time-zone: ""
[2022-07-22 03:22:58] [config] logical-epoch:
[2022-07-22 03:22:58] [config]   - 1e
[2022-07-22 03:22:58] [config]   - 0
[2022-07-22 03:22:58] [config] lr-decay: 0
[2022-07-22 03:22:58] [config] lr-decay-freq: 50000
[2022-07-22 03:22:58] [config] lr-decay-inv-sqrt:
[2022-07-22 03:22:58] [config]   - 8000
[2022-07-22 03:22:58] [config] lr-decay-repeat-warmup: false
[2022-07-22 03:22:58] [config] lr-decay-reset-optimizer: false
[2022-07-22 03:22:58] [config] lr-decay-start:
[2022-07-22 03:22:58] [config]   - 10
[2022-07-22 03:22:58] [config]   - 1
[2022-07-22 03:22:58] [config] lr-decay-strategy: epoch+stalled
[2022-07-22 03:22:58] [config] lr-report: false
[2022-07-22 03:22:58] [config] lr-warmup: 8000
[2022-07-22 03:22:58] [config] lr-warmup-at-reload: false
[2022-07-22 03:22:58] [config] lr-warmup-cycle: false
[2022-07-22 03:22:58] [config] lr-warmup-start-rate: 0
[2022-07-22 03:22:58] [config] max-length: 100
[2022-07-22 03:22:58] [config] max-length-crop: false
[2022-07-22 03:22:58] [config] max-length-factor: 3
[2022-07-22 03:22:58] [config] maxi-batch: 1000
[2022-07-22 03:22:58] [config] maxi-batch-sort: trg
[2022-07-22 03:22:58] [config] mini-batch: 1000
[2022-07-22 03:22:58] [config] mini-batch-fit: true
[2022-07-22 03:22:58] [config] mini-batch-fit-step: 10
[2022-07-22 03:22:58] [config] mini-batch-round-up: true
[2022-07-22 03:22:58] [config] mini-batch-track-lr: false
[2022-07-22 03:22:58] [config] mini-batch-warmup: 0
[2022-07-22 03:22:58] [config] mini-batch-words: 0
[2022-07-22 03:22:58] [config] mini-batch-words-ref: 0
[2022-07-22 03:22:58] [config] model: /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz
[2022-07-22 03:22:58] [config] multi-loss-type: sum
[2022-07-22 03:22:58] [config] n-best: false
[2022-07-22 03:22:58] [config] no-nccl: false
[2022-07-22 03:22:58] [config] no-reload: false
[2022-07-22 03:22:58] [config] no-restore-corpus: false
[2022-07-22 03:22:58] [config] normalize: 1
[2022-07-22 03:22:58] [config] normalize-gradient: false
[2022-07-22 03:22:58] [config] num-devices: 0
[2022-07-22 03:22:58] [config] optimizer: adam
[2022-07-22 03:22:58] [config] optimizer-delay: 2
[2022-07-22 03:22:58] [config] optimizer-params:
[2022-07-22 03:22:58] [config]   - 0.9
[2022-07-22 03:22:58] [config]   - 0.998
[2022-07-22 03:22:58] [config]   - 1e-09
[2022-07-22 03:22:58] [config] output-omit-bias: false
[2022-07-22 03:22:58] [config] overwrite: true
[2022-07-22 03:22:58] [config] precision:
[2022-07-22 03:22:58] [config]   - float32
[2022-07-22 03:22:58] [config]   - float32
[2022-07-22 03:22:58] [config] pretrained-model: ""
[2022-07-22 03:22:58] [config] quantize-biases: false
[2022-07-22 03:22:58] [config] quantize-bits: 0
[2022-07-22 03:22:58] [config] quantize-log-based: false
[2022-07-22 03:22:58] [config] quantize-optimization-steps: 0
[2022-07-22 03:22:58] [config] quiet: false
[2022-07-22 03:22:58] [config] quiet-translation: false
[2022-07-22 03:22:58] [config] relative-paths: false
[2022-07-22 03:22:58] [config] right-left: false
[2022-07-22 03:22:58] [config] save-freq: 10000
[2022-07-22 03:22:58] [config] seed: 1111
[2022-07-22 03:22:58] [config] sentencepiece-alphas:
[2022-07-22 03:22:58] [config]   []
[2022-07-22 03:22:58] [config] sentencepiece-max-lines: 2000000
[2022-07-22 03:22:58] [config] sentencepiece-options: ""
[2022-07-22 03:22:58] [config] sharding: local
[2022-07-22 03:22:58] [config] shuffle: batches
[2022-07-22 03:22:58] [config] shuffle-in-ram: false
[2022-07-22 03:22:58] [config] sigterm: save-and-exit
[2022-07-22 03:22:58] [config] skip: false
[2022-07-22 03:22:58] [config] sqlite: ""
[2022-07-22 03:22:58] [config] sqlite-drop: false
[2022-07-22 03:22:58] [config] sync-freq: 200u
[2022-07-22 03:22:58] [config] sync-sgd: true
[2022-07-22 03:22:58] [config] tempdir: /run/nvme/job_12526782/tmp
[2022-07-22 03:22:58] [config] tied-embeddings: false
[2022-07-22 03:22:58] [config] tied-embeddings-all: true
[2022-07-22 03:22:58] [config] tied-embeddings-src: false
[2022-07-22 03:22:58] [config] train-embedder-rank:
[2022-07-22 03:22:58] [config]   []
[2022-07-22 03:22:58] [config] train-sets:
[2022-07-22 03:22:58] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/train/opusTCv20210807.src.clean.spm32k.gz
[2022-07-22 03:22:58] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/train/opusTCv20210807.trg.clean.spm32k.gz
[2022-07-22 03:22:58] [config] transformer-aan-activation: swish
[2022-07-22 03:22:58] [config] transformer-aan-depth: 2
[2022-07-22 03:22:58] [config] transformer-aan-nogate: false
[2022-07-22 03:22:58] [config] transformer-decoder-autoreg: self-attention
[2022-07-22 03:22:58] [config] transformer-depth-scaling: false
[2022-07-22 03:22:58] [config] transformer-dim-aan: 2048
[2022-07-22 03:22:58] [config] transformer-dim-ffn: 4096
[2022-07-22 03:22:58] [config] transformer-dropout: 0.1
[2022-07-22 03:22:58] [config] transformer-dropout-attention: 0
[2022-07-22 03:22:58] [config] transformer-dropout-ffn: 0
[2022-07-22 03:22:58] [config] transformer-ffn-activation: relu
[2022-07-22 03:22:58] [config] transformer-ffn-depth: 2
[2022-07-22 03:22:58] [config] transformer-guided-alignment-layer: last
[2022-07-22 03:22:58] [config] transformer-heads: 16
[2022-07-22 03:22:58] [config] transformer-no-projection: false
[2022-07-22 03:22:58] [config] transformer-pool: false
[2022-07-22 03:22:58] [config] transformer-postprocess: dan
[2022-07-22 03:22:58] [config] transformer-postprocess-emb: d
[2022-07-22 03:22:58] [config] transformer-postprocess-top: ""
[2022-07-22 03:22:58] [config] transformer-preprocess: ""
[2022-07-22 03:22:58] [config] transformer-tied-layers:
[2022-07-22 03:22:58] [config]   []
[2022-07-22 03:22:58] [config] transformer-train-position-embeddings: false
[2022-07-22 03:22:58] [config] tsv: false
[2022-07-22 03:22:58] [config] tsv-fields: 0
[2022-07-22 03:22:58] [config] type: transformer
[2022-07-22 03:22:58] [config] ulr: false
[2022-07-22 03:22:58] [config] ulr-dim-emb: 0
[2022-07-22 03:22:58] [config] ulr-dropout: 0
[2022-07-22 03:22:58] [config] ulr-keys-vectors: ""
[2022-07-22 03:22:58] [config] ulr-query-vectors: ""
[2022-07-22 03:22:58] [config] ulr-softmax-temperature: 1
[2022-07-22 03:22:58] [config] ulr-trainable-transformation: false
[2022-07-22 03:22:58] [config] unlikelihood-loss: false
[2022-07-22 03:22:58] [config] valid-freq: 10000
[2022-07-22 03:22:58] [config] valid-log: /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.valid1.log
[2022-07-22 03:22:58] [config] valid-max-length: 100
[2022-07-22 03:22:58] [config] valid-metrics:
[2022-07-22 03:22:58] [config]   - perplexity
[2022-07-22 03:22:58] [config] valid-mini-batch: 16
[2022-07-22 03:22:58] [config] valid-reset-stalled: false
[2022-07-22 03:22:58] [config] valid-script-args:
[2022-07-22 03:22:58] [config]   []
[2022-07-22 03:22:58] [config] valid-script-path: ""
[2022-07-22 03:22:58] [config] valid-sets:
[2022-07-22 03:22:58] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/val/Tatoeba-dev-v2021-08-07.src.spm32k
[2022-07-22 03:22:58] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/val/Tatoeba-dev-v2021-08-07.trg.spm32k
[2022-07-22 03:22:58] [config] valid-translation-output: ""
[2022-07-22 03:22:58] [config] version: v1.10.24; 12a1bfa 2021-10-11 16:59:52 +0100
[2022-07-22 03:22:58] [config] vocabs:
[2022-07-22 03:22:58] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.src.vocab
[2022-07-22 03:22:58] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.trg.vocab
[2022-07-22 03:22:58] [config] word-penalty: 0
[2022-07-22 03:22:58] [config] word-scores: false
[2022-07-22 03:22:58] [config] workspace: 15000
[2022-07-22 03:22:58] [config] Loaded model has been created with Marian v1.10.24; 12a1bfa 2021-10-11 16:59:52 +0100
[2022-07-22 03:22:58] Using synchronous SGD
[2022-07-22 03:22:58] [comm] Compiled without MPI support. Running as a single process on r18g03.bullx
[2022-07-22 03:22:58] Synced seed 1111
[2022-07-22 03:22:58] [data] Loading vocabulary from text file /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.src.vocab
[2022-07-22 03:22:58] [data] Setting vocabulary size for input 0 to 32,000
[2022-07-22 03:22:58] [data] Loading vocabulary from text file /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.trg.vocab
[2022-07-22 03:22:58] [data] Setting vocabulary size for input 1 to 32,000
[2022-07-22 03:22:58] [batching] Collecting statistics for batch fitting with step size 10
[2022-07-22 03:22:59] [memory] Extending reserved space to 15104 MB (device gpu0)
[2022-07-22 03:23:01] [memory] Extending reserved space to 15104 MB (device gpu1)
[2022-07-22 03:23:01] [comm] Using NCCL 2.8.3 for GPU communication
[2022-07-22 03:23:01] [comm] Using global sharding
[2022-07-22 03:23:02] [comm] NCCLCommunicators constructed successfully
[2022-07-22 03:23:02] [training] Using 2 GPUs
[2022-07-22 03:23:02] [logits] Applying loss function for 1 factor(s)
[2022-07-22 03:23:02] [memory] Reserving 797 MB, device gpu0
[2022-07-22 03:23:05] [gpu] 16-bit TensorCores enabled for float32 matrix operations
[2022-07-22 03:23:05] [memory] Reserving 797 MB, device gpu0
[2022-07-22 03:23:29] [batching] Done. Typical MB size is 29,884 target words
[2022-07-22 03:23:29] [memory] Extending reserved space to 15104 MB (device gpu0)
[2022-07-22 03:23:29] [memory] Extending reserved space to 15104 MB (device gpu1)
[2022-07-22 03:23:29] [comm] Using NCCL 2.8.3 for GPU communication
[2022-07-22 03:23:29] [comm] Using global sharding
[2022-07-22 03:23:29] [comm] NCCLCommunicators constructed successfully
[2022-07-22 03:23:29] [training] Using 2 GPUs
[2022-07-22 03:23:29] Loading model from /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz
[2022-07-22 03:23:32] Loading model from /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz
[2022-07-22 03:23:38] Allocating memory for general optimizer shards
[2022-07-22 03:23:38] [memory] Reserving 398 MB, device gpu0
[2022-07-22 03:23:38] [memory] Reserving 398 MB, device gpu1
[2022-07-22 03:23:38] Loading Adam parameters
[2022-07-22 03:23:39] [memory] Reserving 797 MB, device gpu0
[2022-07-22 03:23:39] [memory] Reserving 797 MB, device gpu1
[2022-07-22 03:23:39] [memory] Reserving 797 MB, device gpu0
[2022-07-22 03:23:40] [memory] Reserving 797 MB, device gpu1
[2022-07-22 03:23:40] [training] Master parameters and optimizers restored from training checkpoint /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-07-22 03:23:40] [data] Restoring the corpus state to epoch 8, batch 330000
[2022-07-22 03:31:42] Training started
[2022-07-22 03:31:42] [training] Batches are processed as 1 process(es) x 2 devices/process
[2022-07-22 03:31:42] [memory] Reserving 797 MB, device gpu0
[2022-07-22 03:31:42] [memory] Reserving 797 MB, device gpu1
[2022-07-22 03:31:43] Parameter type float32, optimization type float32, casting types false
[2022-07-22 03:36:21] Seen 15,878,150 samples
[2022-07-22 03:36:21] Starting data epoch 9 in logical epoch 9
[2022-07-22 07:09:54] Ep. 9 : Up. 340000 : Sen. 13,271,808 : Cost 2.70321941 : Time 13584.91s : 16088.72 words/s : gNorm 0.6420
[2022-07-22 07:09:54] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz
[2022-07-22 07:09:57] Saving Adam parameters
[2022-07-22 07:10:00] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-07-22 07:10:06] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-07-22 07:10:08] [valid] Ep. 9 : Up. 340000 : perplexity : 2.60971 : new best
[2022-07-22 10:47:31] Ep. 9 : Up. 350000 : Sen. 27,044,142 : Cost 2.71009660 : Time 13057.62s : 16654.00 words/s : gNorm 0.5934
[2022-07-22 10:47:31] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz
[2022-07-22 10:47:34] Saving Adam parameters
[2022-07-22 10:47:37] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-07-22 10:47:44] [valid] Ep. 9 : Up. 350000 : perplexity : 2.65418 : stalled 1 times (last best: 2.60971)
[2022-07-22 11:47:33] Seen 30,921,555 samples
[2022-07-22 11:47:33] Starting data epoch 10 in logical epoch 10
[2022-07-22 14:25:06] Ep. 10 : Up. 360000 : Sen. 9,799,663 : Cost 2.69691396 : Time 13054.62s : 16689.01 words/s : gNorm 0.5937
[2022-07-22 14:25:06] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz
[2022-07-22 14:25:10] Saving Adam parameters
[2022-07-22 14:25:14] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-07-22 14:25:20] [valid] Ep. 10 : Up. 360000 : perplexity : 2.63285 : stalled 2 times (last best: 2.60971)
[2022-07-22 18:03:16] Ep. 10 : Up. 370000 : Sen. 23,462,599 : Cost 2.69406343 : Time 13090.05s : 16672.96 words/s : gNorm 0.5719
[2022-07-22 18:03:16] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz
[2022-07-22 18:03:20] Saving Adam parameters
[2022-07-22 18:03:23] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-07-22 18:03:30] [valid] Ep. 10 : Up. 370000 : perplexity : 2.63642 : stalled 3 times (last best: 2.60971)
[2022-07-22 19:58:59] Seen 30,921,555 samples
[2022-07-22 19:58:59] Starting data epoch 11 in logical epoch 11
[2022-07-22 21:40:51] Ep. 11 : Up. 380000 : Sen. 6,312,624 : Cost 2.69059134 : Time 13055.22s : 16630.03 words/s : gNorm 0.7208
[2022-07-22 21:40:51] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz
[2022-07-22 21:40:57] Saving Adam parameters
[2022-07-22 21:41:00] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-07-22 21:41:15] [valid] Ep. 11 : Up. 380000 : perplexity : 2.65096 : stalled 4 times (last best: 2.60971)
[2022-07-23 01:18:51] Ep. 11 : Up. 390000 : Sen. 19,897,457 : Cost 2.68493724 : Time 13079.91s : 16710.98 words/s : gNorm 0.6509
[2022-07-23 01:18:51] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz
[2022-07-23 01:18:54] Saving Adam parameters
[2022-07-23 01:18:57] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-07-23 01:19:11] [valid] Ep. 11 : Up. 390000 : perplexity : 2.6142 : stalled 5 times (last best: 2.60971)
[2022-07-23 04:10:17] Seen 30,921,555 samples
[2022-07-23 04:10:17] Starting data epoch 12 in logical epoch 12
[2022-07-23 04:56:16] Ep. 12 : Up. 400000 : Sen. 2,854,460 : Cost 2.68207097 : Time 13044.36s : 16629.72 words/s : gNorm 0.6357
[2022-07-23 04:56:16] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz
[2022-07-23 04:56:19] Saving Adam parameters
[2022-07-23 04:56:22] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-07-23 04:56:28] [valid] Ep. 12 : Up. 400000 : perplexity : 2.66857 : stalled 6 times (last best: 2.60971)
[2022-07-23 08:34:05] Ep. 12 : Up. 410000 : Sen. 16,414,378 : Cost 2.67654681 : Time 13068.84s : 16722.68 words/s : gNorm 0.7259
[2022-07-23 08:34:05] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz
[2022-07-23 08:34:08] Saving Adam parameters
[2022-07-23 08:34:11] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-07-23 08:34:22] [valid] Ep. 12 : Up. 410000 : perplexity : 2.61374 : stalled 7 times (last best: 2.60971)
[2022-07-23 12:11:08] Ep. 12 : Up. 420000 : Sen. 30,294,554 : Cost 2.67572665 : Time 13023.68s : 16653.84 words/s : gNorm 0.6547
[2022-07-23 12:11:08] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz
[2022-07-23 12:11:11] Saving Adam parameters
[2022-07-23 12:11:14] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-07-23 12:11:20] [valid] Ep. 12 : Up. 420000 : perplexity : 2.67683 : stalled 8 times (last best: 2.60971)
[2022-07-23 12:21:17] Seen 30,921,555 samples
[2022-07-23 12:21:17] Starting data epoch 13 in logical epoch 13
[2022-07-23 15:49:07] Ep. 13 : Up. 430000 : Sen. 12,925,641 : Cost 2.66965508 : Time 13078.60s : 16708.21 words/s : gNorm 0.6850
[2022-07-23 15:49:07] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz
[2022-07-23 15:49:10] Saving Adam parameters
[2022-07-23 15:49:13] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-07-23 15:49:19] [valid] Ep. 13 : Up. 430000 : perplexity : 2.62174 : stalled 9 times (last best: 2.60971)
[2022-07-23 19:26:40] Ep. 13 : Up. 440000 : Sen. 26,707,614 : Cost 2.66962552 : Time 13053.14s : 16661.28 words/s : gNorm 0.7457
[2022-07-23 19:26:40] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz
[2022-07-23 19:26:43] Saving Adam parameters
[2022-07-23 19:26:46] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-07-23 19:26:52] [valid] Ep. 13 : Up. 440000 : perplexity : 2.65794 : stalled 10 times (last best: 2.60971)
[2022-07-23 19:26:52] Training finished
[2022-07-23 19:26:52] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz
[2022-07-23 19:26:55] Saving Adam parameters
[2022-07-23 19:26:58] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/kor-eng/opusTCv20210807-sepvoc.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
