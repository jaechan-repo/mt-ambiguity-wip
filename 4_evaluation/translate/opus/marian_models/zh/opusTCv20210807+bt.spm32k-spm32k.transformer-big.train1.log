[2022-04-24 14:16:18] [marian] Marian v1.10.24; 12a1bfa 2021-10-11 16:59:52 +0100
[2022-04-24 14:16:18] [marian] Running on r18g04.bullx as process 100609 with command line:
[2022-04-24 14:16:18] [marian] /projappl/project_2001194/marian-dev/build/marian --task transformer-big --optimizer-delay 2 --early-stopping 10 --valid-freq 10000 --valid-sets /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/val/Tatoeba-dev-v2021-08-07.src.spm32k /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/val/Tatoeba-dev-v2021-08-07.trg.spm32k --valid-metrics perplexity --valid-mini-batch 16 --valid-max-length 100 --valid-log /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.valid1.log --beam-size 6 --normalize 1 --allow-unk --workspace 15000 --model /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz --train-sets /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/train/opusTCv20210807+bt.src.clean.spm32k.gz /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/train/opusTCv20210807+bt.trg.clean.spm32k.gz --vocabs /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.vocab.yml /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.vocab.yml --save-freq 10000 --disp-freq 10000 --log /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.train1.log --devices 0 --seed 1111 --tempdir /run/nvme/job_11462240/tmp --shuffle batches --sharding local --overwrite --keep-best
[2022-04-24 14:16:18] [config] after: 0e
[2022-04-24 14:16:18] [config] after-batches: 0
[2022-04-24 14:16:18] [config] after-epochs: 0
[2022-04-24 14:16:18] [config] all-caps-every: 0
[2022-04-24 14:16:18] [config] allow-unk: true
[2022-04-24 14:16:18] [config] authors: false
[2022-04-24 14:16:18] [config] beam-size: 6
[2022-04-24 14:16:18] [config] bert-class-symbol: "[CLS]"
[2022-04-24 14:16:18] [config] bert-mask-symbol: "[MASK]"
[2022-04-24 14:16:18] [config] bert-masking-fraction: 0.15
[2022-04-24 14:16:18] [config] bert-sep-symbol: "[SEP]"
[2022-04-24 14:16:18] [config] bert-train-type-embeddings: true
[2022-04-24 14:16:18] [config] bert-type-vocab-size: 2
[2022-04-24 14:16:18] [config] build-info: ""
[2022-04-24 14:16:18] [config] check-gradient-nan: false
[2022-04-24 14:16:18] [config] check-nan: false
[2022-04-24 14:16:18] [config] cite: false
[2022-04-24 14:16:18] [config] clip-norm: 0
[2022-04-24 14:16:18] [config] cost-scaling:
[2022-04-24 14:16:18] [config]   []
[2022-04-24 14:16:18] [config] cost-type: ce-mean-words
[2022-04-24 14:16:18] [config] cpu-threads: 0
[2022-04-24 14:16:18] [config] data-weighting: ""
[2022-04-24 14:16:18] [config] data-weighting-type: sentence
[2022-04-24 14:16:18] [config] dec-cell: gru
[2022-04-24 14:16:18] [config] dec-cell-base-depth: 2
[2022-04-24 14:16:18] [config] dec-cell-high-depth: 1
[2022-04-24 14:16:18] [config] dec-depth: 6
[2022-04-24 14:16:18] [config] devices:
[2022-04-24 14:16:18] [config]   - 0
[2022-04-24 14:16:18] [config] dim-emb: 1024
[2022-04-24 14:16:18] [config] dim-rnn: 1024
[2022-04-24 14:16:18] [config] dim-vocabs:
[2022-04-24 14:16:18] [config]   - 0
[2022-04-24 14:16:18] [config]   - 0
[2022-04-24 14:16:18] [config] disp-first: 0
[2022-04-24 14:16:18] [config] disp-freq: 10000
[2022-04-24 14:16:18] [config] disp-label-counts: true
[2022-04-24 14:16:18] [config] dropout-rnn: 0
[2022-04-24 14:16:18] [config] dropout-src: 0
[2022-04-24 14:16:18] [config] dropout-trg: 0
[2022-04-24 14:16:18] [config] dump-config: ""
[2022-04-24 14:16:18] [config] dynamic-gradient-scaling:
[2022-04-24 14:16:18] [config]   []
[2022-04-24 14:16:18] [config] early-stopping: 10
[2022-04-24 14:16:18] [config] early-stopping-on: first
[2022-04-24 14:16:18] [config] embedding-fix-src: false
[2022-04-24 14:16:18] [config] embedding-fix-trg: false
[2022-04-24 14:16:18] [config] embedding-normalization: false
[2022-04-24 14:16:18] [config] embedding-vectors:
[2022-04-24 14:16:18] [config]   []
[2022-04-24 14:16:18] [config] enc-cell: gru
[2022-04-24 14:16:18] [config] enc-cell-depth: 1
[2022-04-24 14:16:18] [config] enc-depth: 6
[2022-04-24 14:16:18] [config] enc-type: bidirectional
[2022-04-24 14:16:18] [config] english-title-case-every: 0
[2022-04-24 14:16:18] [config] exponential-smoothing: 0.0001
[2022-04-24 14:16:18] [config] factor-weight: 1
[2022-04-24 14:16:18] [config] factors-combine: sum
[2022-04-24 14:16:18] [config] factors-dim-emb: 0
[2022-04-24 14:16:18] [config] gradient-checkpointing: false
[2022-04-24 14:16:18] [config] gradient-norm-average-window: 100
[2022-04-24 14:16:18] [config] guided-alignment: none
[2022-04-24 14:16:18] [config] guided-alignment-cost: mse
[2022-04-24 14:16:18] [config] guided-alignment-weight: 0.1
[2022-04-24 14:16:18] [config] ignore-model-config: false
[2022-04-24 14:16:18] [config] input-types:
[2022-04-24 14:16:18] [config]   []
[2022-04-24 14:16:18] [config] interpolate-env-vars: false
[2022-04-24 14:16:18] [config] keep-best: true
[2022-04-24 14:16:18] [config] label-smoothing: 0.1
[2022-04-24 14:16:18] [config] layer-normalization: false
[2022-04-24 14:16:18] [config] learn-rate: 0.0002
[2022-04-24 14:16:18] [config] lemma-dependency: ""
[2022-04-24 14:16:18] [config] lemma-dim-emb: 0
[2022-04-24 14:16:18] [config] log: /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.train1.log
[2022-04-24 14:16:18] [config] log-level: info
[2022-04-24 14:16:18] [config] log-time-zone: ""
[2022-04-24 14:16:18] [config] logical-epoch:
[2022-04-24 14:16:18] [config]   - 1e
[2022-04-24 14:16:18] [config]   - 0
[2022-04-24 14:16:18] [config] lr-decay: 0
[2022-04-24 14:16:18] [config] lr-decay-freq: 50000
[2022-04-24 14:16:18] [config] lr-decay-inv-sqrt:
[2022-04-24 14:16:18] [config]   - 8000
[2022-04-24 14:16:18] [config] lr-decay-repeat-warmup: false
[2022-04-24 14:16:18] [config] lr-decay-reset-optimizer: false
[2022-04-24 14:16:18] [config] lr-decay-start:
[2022-04-24 14:16:18] [config]   - 10
[2022-04-24 14:16:18] [config]   - 1
[2022-04-24 14:16:18] [config] lr-decay-strategy: epoch+stalled
[2022-04-24 14:16:18] [config] lr-report: false
[2022-04-24 14:16:18] [config] lr-warmup: 8000
[2022-04-24 14:16:18] [config] lr-warmup-at-reload: false
[2022-04-24 14:16:18] [config] lr-warmup-cycle: false
[2022-04-24 14:16:18] [config] lr-warmup-start-rate: 0
[2022-04-24 14:16:18] [config] max-length: 100
[2022-04-24 14:16:18] [config] max-length-crop: false
[2022-04-24 14:16:18] [config] max-length-factor: 3
[2022-04-24 14:16:18] [config] maxi-batch: 1000
[2022-04-24 14:16:18] [config] maxi-batch-sort: trg
[2022-04-24 14:16:18] [config] mini-batch: 1000
[2022-04-24 14:16:18] [config] mini-batch-fit: true
[2022-04-24 14:16:18] [config] mini-batch-fit-step: 10
[2022-04-24 14:16:18] [config] mini-batch-round-up: true
[2022-04-24 14:16:18] [config] mini-batch-track-lr: false
[2022-04-24 14:16:18] [config] mini-batch-warmup: 0
[2022-04-24 14:16:18] [config] mini-batch-words: 0
[2022-04-24 14:16:18] [config] mini-batch-words-ref: 0
[2022-04-24 14:16:18] [config] model: /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-24 14:16:18] [config] multi-loss-type: sum
[2022-04-24 14:16:18] [config] n-best: false
[2022-04-24 14:16:18] [config] no-nccl: false
[2022-04-24 14:16:18] [config] no-reload: false
[2022-04-24 14:16:18] [config] no-restore-corpus: false
[2022-04-24 14:16:18] [config] normalize: 1
[2022-04-24 14:16:18] [config] normalize-gradient: false
[2022-04-24 14:16:18] [config] num-devices: 0
[2022-04-24 14:16:18] [config] optimizer: adam
[2022-04-24 14:16:18] [config] optimizer-delay: 2
[2022-04-24 14:16:18] [config] optimizer-params:
[2022-04-24 14:16:18] [config]   - 0.9
[2022-04-24 14:16:18] [config]   - 0.998
[2022-04-24 14:16:18] [config]   - 1e-09
[2022-04-24 14:16:18] [config] output-omit-bias: false
[2022-04-24 14:16:18] [config] overwrite: true
[2022-04-24 14:16:18] [config] precision:
[2022-04-24 14:16:18] [config]   - float32
[2022-04-24 14:16:18] [config]   - float32
[2022-04-24 14:16:18] [config] pretrained-model: ""
[2022-04-24 14:16:18] [config] quantize-biases: false
[2022-04-24 14:16:18] [config] quantize-bits: 0
[2022-04-24 14:16:18] [config] quantize-log-based: false
[2022-04-24 14:16:18] [config] quantize-optimization-steps: 0
[2022-04-24 14:16:18] [config] quiet: false
[2022-04-24 14:16:18] [config] quiet-translation: false
[2022-04-24 14:16:18] [config] relative-paths: false
[2022-04-24 14:16:18] [config] right-left: false
[2022-04-24 14:16:18] [config] save-freq: 10000
[2022-04-24 14:16:18] [config] seed: 1111
[2022-04-24 14:16:18] [config] sentencepiece-alphas:
[2022-04-24 14:16:18] [config]   []
[2022-04-24 14:16:18] [config] sentencepiece-max-lines: 2000000
[2022-04-24 14:16:18] [config] sentencepiece-options: ""
[2022-04-24 14:16:18] [config] sharding: local
[2022-04-24 14:16:18] [config] shuffle: batches
[2022-04-24 14:16:18] [config] shuffle-in-ram: false
[2022-04-24 14:16:18] [config] sigterm: save-and-exit
[2022-04-24 14:16:18] [config] skip: false
[2022-04-24 14:16:18] [config] sqlite: ""
[2022-04-24 14:16:18] [config] sqlite-drop: false
[2022-04-24 14:16:18] [config] sync-freq: 200u
[2022-04-24 14:16:18] [config] sync-sgd: true
[2022-04-24 14:16:18] [config] tempdir: /run/nvme/job_11462240/tmp
[2022-04-24 14:16:18] [config] tied-embeddings: false
[2022-04-24 14:16:18] [config] tied-embeddings-all: true
[2022-04-24 14:16:18] [config] tied-embeddings-src: false
[2022-04-24 14:16:18] [config] train-embedder-rank:
[2022-04-24 14:16:18] [config]   []
[2022-04-24 14:16:18] [config] train-sets:
[2022-04-24 14:16:18] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/train/opusTCv20210807+bt.src.clean.spm32k.gz
[2022-04-24 14:16:18] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/train/opusTCv20210807+bt.trg.clean.spm32k.gz
[2022-04-24 14:16:18] [config] transformer-aan-activation: swish
[2022-04-24 14:16:18] [config] transformer-aan-depth: 2
[2022-04-24 14:16:18] [config] transformer-aan-nogate: false
[2022-04-24 14:16:18] [config] transformer-decoder-autoreg: self-attention
[2022-04-24 14:16:18] [config] transformer-depth-scaling: false
[2022-04-24 14:16:18] [config] transformer-dim-aan: 2048
[2022-04-24 14:16:18] [config] transformer-dim-ffn: 4096
[2022-04-24 14:16:18] [config] transformer-dropout: 0.1
[2022-04-24 14:16:18] [config] transformer-dropout-attention: 0
[2022-04-24 14:16:18] [config] transformer-dropout-ffn: 0
[2022-04-24 14:16:18] [config] transformer-ffn-activation: relu
[2022-04-24 14:16:18] [config] transformer-ffn-depth: 2
[2022-04-24 14:16:18] [config] transformer-guided-alignment-layer: last
[2022-04-24 14:16:18] [config] transformer-heads: 16
[2022-04-24 14:16:18] [config] transformer-no-projection: false
[2022-04-24 14:16:18] [config] transformer-pool: false
[2022-04-24 14:16:18] [config] transformer-postprocess: dan
[2022-04-24 14:16:18] [config] transformer-postprocess-emb: d
[2022-04-24 14:16:18] [config] transformer-postprocess-top: ""
[2022-04-24 14:16:18] [config] transformer-preprocess: ""
[2022-04-24 14:16:18] [config] transformer-tied-layers:
[2022-04-24 14:16:18] [config]   []
[2022-04-24 14:16:18] [config] transformer-train-position-embeddings: false
[2022-04-24 14:16:18] [config] tsv: false
[2022-04-24 14:16:18] [config] tsv-fields: 0
[2022-04-24 14:16:18] [config] type: transformer
[2022-04-24 14:16:18] [config] ulr: false
[2022-04-24 14:16:18] [config] ulr-dim-emb: 0
[2022-04-24 14:16:18] [config] ulr-dropout: 0
[2022-04-24 14:16:18] [config] ulr-keys-vectors: ""
[2022-04-24 14:16:18] [config] ulr-query-vectors: ""
[2022-04-24 14:16:18] [config] ulr-softmax-temperature: 1
[2022-04-24 14:16:18] [config] ulr-trainable-transformation: false
[2022-04-24 14:16:18] [config] unlikelihood-loss: false
[2022-04-24 14:16:18] [config] valid-freq: 10000
[2022-04-24 14:16:18] [config] valid-log: /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.valid1.log
[2022-04-24 14:16:18] [config] valid-max-length: 100
[2022-04-24 14:16:18] [config] valid-metrics:
[2022-04-24 14:16:18] [config]   - perplexity
[2022-04-24 14:16:18] [config] valid-mini-batch: 16
[2022-04-24 14:16:18] [config] valid-reset-stalled: false
[2022-04-24 14:16:18] [config] valid-script-args:
[2022-04-24 14:16:18] [config]   []
[2022-04-24 14:16:18] [config] valid-script-path: ""
[2022-04-24 14:16:18] [config] valid-sets:
[2022-04-24 14:16:18] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/val/Tatoeba-dev-v2021-08-07.src.spm32k
[2022-04-24 14:16:18] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/val/Tatoeba-dev-v2021-08-07.trg.spm32k
[2022-04-24 14:16:18] [config] valid-translation-output: ""
[2022-04-24 14:16:18] [config] vocabs:
[2022-04-24 14:16:18] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-04-24 14:16:18] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-04-24 14:16:18] [config] word-penalty: 0
[2022-04-24 14:16:18] [config] word-scores: false
[2022-04-24 14:16:18] [config] workspace: 15000
[2022-04-24 14:16:18] [config] Model is being created with Marian v1.10.24; 12a1bfa 2021-10-11 16:59:52 +0100
[2022-04-24 14:16:18] Using synchronous SGD
[2022-04-24 14:16:18] [comm] Compiled without MPI support. Running as a single process on r18g04.bullx
[2022-04-24 14:16:18] Synced seed 1111
[2022-04-24 14:16:18] [data] Loading vocabulary from JSON/Yaml file /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-04-24 14:16:19] [data] Setting vocabulary size for input 0 to 58,791
[2022-04-24 14:16:19] [data] Loading vocabulary from JSON/Yaml file /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-04-24 14:16:19] [data] Setting vocabulary size for input 1 to 58,791
[2022-04-24 14:16:19] [batching] Collecting statistics for batch fitting with step size 10
[2022-04-24 14:16:20] [memory] Extending reserved space to 15104 MB (device gpu0)
[2022-04-24 14:16:22] [comm] Using NCCL 2.8.3 for GPU communication
[2022-04-24 14:16:22] [comm] Using global sharding
[2022-04-24 14:16:22] [comm] NCCLCommunicators constructed successfully
[2022-04-24 14:16:22] [training] Using 1 GPUs
[2022-04-24 14:16:23] [logits] Applying loss function for 1 factor(s)
[2022-04-24 14:16:23] [memory] Reserving 902 MB, device gpu0
[2022-04-24 14:16:26] [gpu] 16-bit TensorCores enabled for float32 matrix operations
[2022-04-24 14:16:26] [memory] Reserving 902 MB, device gpu0
[2022-04-24 14:16:56] [batching] Done. Typical MB size is 13,560 target words
[2022-04-24 14:16:56] [memory] Extending reserved space to 15104 MB (device gpu0)
[2022-04-24 14:16:56] [comm] Using NCCL 2.8.3 for GPU communication
[2022-04-24 14:16:56] [comm] Using global sharding
[2022-04-24 14:16:57] [comm] NCCLCommunicators constructed successfully
[2022-04-24 14:16:57] [training] Using 1 GPUs
[2022-04-24 14:16:57] Training started
[2022-04-24 14:17:10] [training] Batches are processed as 1 process(es) x 1 devices/process
[2022-04-24 14:17:10] [memory] Reserving 902 MB, device gpu0
[2022-04-24 14:17:10] [memory] Reserving 902 MB, device gpu0
[2022-04-24 14:17:11] Parameter type float32, optimization type float32, casting types false
[2022-04-24 14:17:11] Allocating memory for general optimizer shards
[2022-04-24 14:17:11] [memory] Reserving 902 MB, device gpu0
[2022-04-24 14:17:11] Allocating memory for Adam-specific shards
[2022-04-24 14:17:11] [memory] Reserving 1805 MB, device gpu0
[2022-04-24 16:41:22] Ep. 1 : Up. 10000 : Sen. 7,427,159 : Cost 6.47108841 : Time 8665.35s : 10587.89 words/s : gNorm 1.0707
[2022-04-24 16:41:22] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-24 16:41:33] Saving Adam parameters
[2022-04-24 16:41:37] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-24 16:41:48] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-04-24 16:41:50] [valid] Ep. 1 : Up. 10000 : perplexity : 84.1354 : new best
[2022-04-24 19:05:44] Ep. 1 : Up. 20000 : Sen. 14,822,773 : Cost 4.80542564 : Time 8661.79s : 10580.21 words/s : gNorm 1.0358
[2022-04-24 19:05:44] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-24 19:05:47] Saving Adam parameters
[2022-04-24 19:05:50] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-24 19:06:00] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-04-24 19:06:02] [valid] Ep. 1 : Up. 20000 : perplexity : 48.5456 : new best
[2022-04-24 21:29:54] Ep. 1 : Up. 30000 : Sen. 22,240,863 : Cost 4.36295128 : Time 8649.79s : 10591.29 words/s : gNorm 1.1362
[2022-04-24 21:29:56] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-24 21:30:00] Saving Adam parameters
[2022-04-24 21:30:04] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-24 21:30:15] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-04-24 21:30:17] [valid] Ep. 1 : Up. 30000 : perplexity : 39.222 : new best
[2022-04-24 23:54:50] Ep. 1 : Up. 40000 : Sen. 29,538,988 : Cost 3.84725857 : Time 8694.97s : 10607.76 words/s : gNorm 0.9030
[2022-04-24 23:54:50] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-24 23:55:01] Saving Adam parameters
[2022-04-24 23:55:04] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-24 23:55:14] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-04-24 23:55:16] [valid] Ep. 1 : Up. 40000 : perplexity : 32.4066 : new best
[2022-04-25 02:20:11] Ep. 1 : Up. 50000 : Sen. 36,773,462 : Cost 3.60589767 : Time 8720.43s : 10591.30 words/s : gNorm 0.8357
[2022-04-25 02:20:11] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-25 02:20:14] Saving Adam parameters
[2022-04-25 02:20:18] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-25 02:20:28] [valid] Ep. 1 : Up. 50000 : perplexity : 33.9999 : stalled 1 times (last best: 32.4066)
[2022-04-25 04:45:33] Ep. 1 : Up. 60000 : Sen. 43,996,148 : Cost 3.50137353 : Time 8722.61s : 10586.77 words/s : gNorm 0.8450
[2022-04-25 04:45:33] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-25 04:45:37] Saving Adam parameters
[2022-04-25 04:45:40] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-25 04:45:52] [valid] Ep. 1 : Up. 60000 : perplexity : 33.9595 : stalled 2 times (last best: 32.4066)
[2022-04-25 07:10:21] Ep. 1 : Up. 70000 : Sen. 51,250,830 : Cost 3.43227506 : Time 8687.60s : 10624.14 words/s : gNorm 0.7711
[2022-04-25 07:10:21] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-25 07:10:24] Saving Adam parameters
[2022-04-25 07:10:27] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-25 07:10:38] [valid] Ep. 1 : Up. 70000 : perplexity : 33.4235 : stalled 3 times (last best: 32.4066)
[2022-04-25 09:34:59] Ep. 1 : Up. 80000 : Sen. 58,494,841 : Cost 3.38383889 : Time 8677.42s : 10636.98 words/s : gNorm 0.7993
[2022-04-25 09:34:59] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-25 09:35:02] Saving Adam parameters
[2022-04-25 09:35:05] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-25 09:35:16] [valid] Ep. 1 : Up. 80000 : perplexity : 33.4178 : stalled 4 times (last best: 32.4066)
[2022-04-25 11:59:57] Ep. 1 : Up. 90000 : Sen. 65,754,001 : Cost 3.34668183 : Time 8698.29s : 10630.88 words/s : gNorm 0.7739
[2022-04-25 11:59:57] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-25 12:00:02] Saving Adam parameters
[2022-04-25 12:00:05] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-25 12:00:15] [valid] Ep. 1 : Up. 90000 : perplexity : 33.1091 : stalled 5 times (last best: 32.4066)
[2022-04-25 13:18:59] Seen 69,730,539 samples
[2022-04-25 13:18:59] Starting data epoch 2 in logical epoch 2
[2022-04-25 14:24:15] Ep. 2 : Up. 100000 : Sen. 3,360,664 : Cost 3.32317019 : Time 8658.42s : 10614.21 words/s : gNorm 0.7773
[2022-04-25 14:24:15] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-25 14:24:20] Saving Adam parameters
[2022-04-25 14:24:23] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-25 14:24:34] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-04-25 14:24:36] [valid] Ep. 2 : Up. 100000 : perplexity : 29.7554 : new best
[2022-04-25 16:47:58] Ep. 2 : Up. 110000 : Sen. 10,785,768 : Cost 3.30634093 : Time 8622.53s : 10635.55 words/s : gNorm 0.7795
[2022-04-25 16:47:58] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-25 16:48:02] Saving Adam parameters
[2022-04-25 16:48:05] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-25 16:48:16] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-04-25 16:48:19] [valid] Ep. 2 : Up. 110000 : perplexity : 20.8974 : new best
[2022-04-25 19:11:59] Ep. 2 : Up. 120000 : Sen. 18,182,016 : Cost 3.28381991 : Time 8640.43s : 10602.28 words/s : gNorm 0.7539
[2022-04-25 19:11:59] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-25 19:12:04] Saving Adam parameters
[2022-04-25 19:12:07] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-25 19:12:20] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-04-25 19:12:22] [valid] Ep. 2 : Up. 120000 : perplexity : 18.7375 : new best
[2022-04-25 21:36:22] Ep. 2 : Up. 130000 : Sen. 25,567,915 : Cost 3.26413918 : Time 8663.48s : 10598.35 words/s : gNorm 0.7464
[2022-04-25 21:36:22] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-25 21:36:26] Saving Adam parameters
[2022-04-25 21:36:29] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-25 21:36:40] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-04-25 21:36:42] [valid] Ep. 2 : Up. 130000 : perplexity : 18.0152 : new best
[2022-04-26 00:01:53] Ep. 2 : Up. 140000 : Sen. 32,808,656 : Cost 3.23388362 : Time 8730.75s : 10589.83 words/s : gNorm 0.7612
[2022-04-26 00:01:53] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-26 00:01:57] Saving Adam parameters
[2022-04-26 00:02:00] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-26 00:02:12] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-04-26 00:02:14] [valid] Ep. 2 : Up. 140000 : perplexity : 17.6536 : new best
[2022-04-26 02:27:08] Ep. 2 : Up. 150000 : Sen. 40,037,362 : Cost 3.21996927 : Time 8714.84s : 10592.09 words/s : gNorm 0.7677
[2022-04-26 02:27:08] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-26 02:27:12] Saving Adam parameters
[2022-04-26 02:27:15] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-26 02:27:25] [valid] Ep. 2 : Up. 150000 : perplexity : 18.5698 : stalled 1 times (last best: 17.6536)
[2022-04-26 04:51:51] Ep. 2 : Up. 160000 : Sen. 47,278,307 : Cost 3.20361257 : Time 8682.75s : 10642.36 words/s : gNorm 0.7948
[2022-04-26 04:51:51] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-26 04:51:54] Saving Adam parameters
[2022-04-26 04:51:57] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-26 04:52:07] [valid] Ep. 2 : Up. 160000 : perplexity : 20.9503 : stalled 2 times (last best: 17.6536)
[2022-04-26 07:16:24] Ep. 2 : Up. 170000 : Sen. 54,526,621 : Cost 3.19016719 : Time 8672.82s : 10643.50 words/s : gNorm 0.7471
[2022-04-26 07:16:24] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-26 07:16:27] Saving Adam parameters
[2022-04-26 07:16:30] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-26 07:16:40] [valid] Ep. 2 : Up. 170000 : perplexity : 23.1938 : stalled 3 times (last best: 17.6536)
[2022-04-26 09:41:12] Ep. 2 : Up. 180000 : Sen. 61,762,422 : Cost 3.17741704 : Time 8687.59s : 10630.62 words/s : gNorm 0.7669
[2022-04-26 09:41:12] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-26 09:41:16] Saving Adam parameters
[2022-04-26 09:41:19] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-26 09:41:29] [valid] Ep. 2 : Up. 180000 : perplexity : 26.3099 : stalled 4 times (last best: 17.6536)
[2022-04-26 12:05:46] Ep. 2 : Up. 190000 : Sen. 69,030,896 : Cost 3.16877174 : Time 8674.06s : 10642.00 words/s : gNorm 0.7913
[2022-04-26 12:05:46] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-26 12:05:57] Saving Adam parameters
[2022-04-26 12:06:00] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-26 12:06:39] [valid] Ep. 2 : Up. 190000 : perplexity : 28.0778 : stalled 5 times (last best: 17.6536)
[2022-04-26 12:20:07] Seen 69,730,539 samples
[2022-04-26 12:20:07] Starting data epoch 3 in logical epoch 3
[2022-04-26 14:30:11] Ep. 3 : Up. 200000 : Sen. 6,703,828 : Cost 3.17056727 : Time 8665.24s : 10591.62 words/s : gNorm 0.8098
[2022-04-26 14:30:11] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-26 14:30:14] Saving Adam parameters
[2022-04-26 14:30:17] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-26 14:30:27] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-04-26 14:30:29] [valid] Ep. 3 : Up. 200000 : perplexity : 17.1077 : new best
[2022-04-26 16:53:51] Ep. 3 : Up. 210000 : Sen. 14,120,350 : Cost 3.16623807 : Time 8619.98s : 10630.87 words/s : gNorm 0.8144
[2022-04-26 16:53:51] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-26 16:53:54] Saving Adam parameters
[2022-04-26 16:53:58] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-26 16:54:09] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-04-26 16:54:11] [valid] Ep. 3 : Up. 210000 : perplexity : 16.1058 : new best
[2022-04-26 19:17:25] Ep. 3 : Up. 220000 : Sen. 21,527,635 : Cost 3.15631533 : Time 8613.33s : 10635.43 words/s : gNorm 0.8398
[2022-04-26 19:17:25] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-26 19:17:28] Saving Adam parameters
[2022-04-26 19:17:31] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-26 19:17:41] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-04-26 19:17:43] [valid] Ep. 3 : Up. 220000 : perplexity : 15.7244 : new best
[2022-04-26 21:41:36] Ep. 3 : Up. 230000 : Sen. 28,840,284 : Cost 3.14276147 : Time 8650.85s : 10647.69 words/s : gNorm 0.7822
[2022-04-26 21:41:36] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-26 21:41:39] Saving Adam parameters
[2022-04-26 21:41:42] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-26 21:41:51] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-04-26 21:41:54] [valid] Ep. 3 : Up. 230000 : perplexity : 15.555 : new best
[2022-04-27 00:06:24] Ep. 3 : Up. 240000 : Sen. 36,086,149 : Cost 3.13002896 : Time 8688.89s : 10628.48 words/s : gNorm 0.7784
[2022-04-27 00:06:24] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-27 00:06:27] Saving Adam parameters
[2022-04-27 00:06:30] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-27 00:06:41] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-04-27 00:06:44] [valid] Ep. 3 : Up. 240000 : perplexity : 15.384 : new best
[2022-04-27 02:31:11] Ep. 3 : Up. 250000 : Sen. 43,297,299 : Cost 3.12131763 : Time 8686.82s : 10641.44 words/s : gNorm 0.7873
[2022-04-27 02:31:11] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-27 02:31:14] Saving Adam parameters
[2022-04-27 02:31:17] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-27 02:31:34] [valid] Ep. 3 : Up. 250000 : perplexity : 15.9995 : stalled 1 times (last best: 15.384)
[2022-04-27 04:55:47] Ep. 3 : Up. 260000 : Sen. 50,552,636 : Cost 3.11421561 : Time 8676.13s : 10639.23 words/s : gNorm 0.8237
[2022-04-27 04:55:47] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-27 04:55:50] Saving Adam parameters
[2022-04-27 04:55:54] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-27 04:56:03] [valid] Ep. 3 : Up. 260000 : perplexity : 16.8458 : stalled 2 times (last best: 15.384)
[2022-04-27 07:20:26] Ep. 3 : Up. 270000 : Sen. 57,804,939 : Cost 3.10612798 : Time 8678.37s : 10639.23 words/s : gNorm 0.8083
[2022-04-27 07:20:26] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-27 07:20:29] Saving Adam parameters
[2022-04-27 07:20:32] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-27 07:20:44] [valid] Ep. 3 : Up. 270000 : perplexity : 17.2917 : stalled 3 times (last best: 15.384)
[2022-04-27 09:45:12] Ep. 3 : Up. 280000 : Sen. 65,058,025 : Cost 3.10111523 : Time 8686.18s : 10632.71 words/s : gNorm 0.8168
[2022-04-27 09:45:12] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-27 09:45:15] Saving Adam parameters
[2022-04-27 09:45:18] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-27 09:45:28] [valid] Ep. 3 : Up. 280000 : perplexity : 17.9078 : stalled 4 times (last best: 15.384)
[2022-04-27 11:47:32] Seen 69,730,539 samples
[2022-04-27 11:47:32] Starting data epoch 4 in logical epoch 4
[2022-04-27 12:39:12] Ep. 4 : Up. 290000 : Sen. 2,642,742 : Cost 3.09965658 : Time 10439.37s : 8819.56 words/s : gNorm 0.7840
[2022-04-27 12:39:12] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-27 12:39:15] Saving Adam parameters
[2022-04-27 12:39:18] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-27 12:39:27] [valid] Ep. 4 : Up. 290000 : perplexity : 16.1996 : stalled 5 times (last best: 15.384)
[2022-04-27 15:12:53] [marian] Marian v1.10.24; 12a1bfa 2021-10-11 16:59:52 +0100
[2022-04-27 15:12:53] [marian] Running on r02g05.bullx as process 26653 with command line:
[2022-04-27 15:12:53] [marian] /projappl/project_2001194/marian-dev/build/marian --task transformer-big --optimizer-delay 2 --early-stopping 10 --valid-freq 10000 --valid-sets /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/val/Tatoeba-dev-v2021-08-07.src.spm32k /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/val/Tatoeba-dev-v2021-08-07.trg.spm32k --valid-metrics perplexity --valid-mini-batch 16 --valid-max-length 100 --valid-log /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.valid1.log --beam-size 6 --normalize 1 --allow-unk --workspace 15000 --model /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz --train-sets /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/train/opusTCv20210807+bt.src.clean.spm32k.gz /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/train/opusTCv20210807+bt.trg.clean.spm32k.gz --vocabs /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.vocab.yml /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.vocab.yml --save-freq 10000 --disp-freq 10000 --log /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.train1.log --devices 0 1 --seed 1111 --tempdir /run/nvme/job_11462556/tmp --shuffle batches --sharding local --overwrite --keep-best
[2022-04-27 15:12:55] [config] after: 0e
[2022-04-27 15:12:55] [config] after-batches: 0
[2022-04-27 15:12:55] [config] after-epochs: 0
[2022-04-27 15:12:55] [config] all-caps-every: 0
[2022-04-27 15:12:55] [config] allow-unk: true
[2022-04-27 15:12:55] [config] authors: false
[2022-04-27 15:12:55] [config] beam-size: 6
[2022-04-27 15:12:55] [config] bert-class-symbol: "[CLS]"
[2022-04-27 15:12:55] [config] bert-mask-symbol: "[MASK]"
[2022-04-27 15:12:55] [config] bert-masking-fraction: 0.15
[2022-04-27 15:12:55] [config] bert-sep-symbol: "[SEP]"
[2022-04-27 15:12:55] [config] bert-train-type-embeddings: true
[2022-04-27 15:12:55] [config] bert-type-vocab-size: 2
[2022-04-27 15:12:55] [config] build-info: ""
[2022-04-27 15:12:55] [config] check-gradient-nan: false
[2022-04-27 15:12:55] [config] check-nan: false
[2022-04-27 15:12:55] [config] cite: false
[2022-04-27 15:12:55] [config] clip-norm: 0
[2022-04-27 15:12:55] [config] cost-scaling:
[2022-04-27 15:12:55] [config]   []
[2022-04-27 15:12:55] [config] cost-type: ce-mean-words
[2022-04-27 15:12:55] [config] cpu-threads: 0
[2022-04-27 15:12:55] [config] data-weighting: ""
[2022-04-27 15:12:55] [config] data-weighting-type: sentence
[2022-04-27 15:12:55] [config] dec-cell: gru
[2022-04-27 15:12:55] [config] dec-cell-base-depth: 2
[2022-04-27 15:12:55] [config] dec-cell-high-depth: 1
[2022-04-27 15:12:55] [config] dec-depth: 6
[2022-04-27 15:12:55] [config] devices:
[2022-04-27 15:12:55] [config]   - 0
[2022-04-27 15:12:55] [config]   - 1
[2022-04-27 15:12:55] [config] dim-emb: 1024
[2022-04-27 15:12:55] [config] dim-rnn: 1024
[2022-04-27 15:12:55] [config] dim-vocabs:
[2022-04-27 15:12:55] [config]   - 58791
[2022-04-27 15:12:55] [config]   - 58791
[2022-04-27 15:12:55] [config] disp-first: 0
[2022-04-27 15:12:55] [config] disp-freq: 10000
[2022-04-27 15:12:55] [config] disp-label-counts: true
[2022-04-27 15:12:55] [config] dropout-rnn: 0
[2022-04-27 15:12:55] [config] dropout-src: 0
[2022-04-27 15:12:55] [config] dropout-trg: 0
[2022-04-27 15:12:55] [config] dump-config: ""
[2022-04-27 15:12:55] [config] dynamic-gradient-scaling:
[2022-04-27 15:12:55] [config]   []
[2022-04-27 15:12:55] [config] early-stopping: 10
[2022-04-27 15:12:55] [config] early-stopping-on: first
[2022-04-27 15:12:55] [config] embedding-fix-src: false
[2022-04-27 15:12:55] [config] embedding-fix-trg: false
[2022-04-27 15:12:55] [config] embedding-normalization: false
[2022-04-27 15:12:55] [config] embedding-vectors:
[2022-04-27 15:12:55] [config]   []
[2022-04-27 15:12:55] [config] enc-cell: gru
[2022-04-27 15:12:55] [config] enc-cell-depth: 1
[2022-04-27 15:12:55] [config] enc-depth: 6
[2022-04-27 15:12:55] [config] enc-type: bidirectional
[2022-04-27 15:12:55] [config] english-title-case-every: 0
[2022-04-27 15:12:55] [config] exponential-smoothing: 0.0001
[2022-04-27 15:12:55] [config] factor-weight: 1
[2022-04-27 15:12:55] [config] factors-combine: sum
[2022-04-27 15:12:55] [config] factors-dim-emb: 0
[2022-04-27 15:12:55] [config] gradient-checkpointing: false
[2022-04-27 15:12:55] [config] gradient-norm-average-window: 100
[2022-04-27 15:12:55] [config] guided-alignment: none
[2022-04-27 15:12:55] [config] guided-alignment-cost: mse
[2022-04-27 15:12:55] [config] guided-alignment-weight: 0.1
[2022-04-27 15:12:55] [config] ignore-model-config: false
[2022-04-27 15:12:55] [config] input-types:
[2022-04-27 15:12:55] [config]   []
[2022-04-27 15:12:55] [config] interpolate-env-vars: false
[2022-04-27 15:12:55] [config] keep-best: true
[2022-04-27 15:12:55] [config] label-smoothing: 0.1
[2022-04-27 15:12:55] [config] layer-normalization: false
[2022-04-27 15:12:55] [config] learn-rate: 0.0002
[2022-04-27 15:12:55] [config] lemma-dependency: ""
[2022-04-27 15:12:55] [config] lemma-dim-emb: 0
[2022-04-27 15:12:55] [config] log: /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.train1.log
[2022-04-27 15:12:55] [config] log-level: info
[2022-04-27 15:12:55] [config] log-time-zone: ""
[2022-04-27 15:12:55] [config] logical-epoch:
[2022-04-27 15:12:55] [config]   - 1e
[2022-04-27 15:12:55] [config]   - 0
[2022-04-27 15:12:55] [config] lr-decay: 0
[2022-04-27 15:12:55] [config] lr-decay-freq: 50000
[2022-04-27 15:12:55] [config] lr-decay-inv-sqrt:
[2022-04-27 15:12:55] [config]   - 8000
[2022-04-27 15:12:55] [config] lr-decay-repeat-warmup: false
[2022-04-27 15:12:55] [config] lr-decay-reset-optimizer: false
[2022-04-27 15:12:55] [config] lr-decay-start:
[2022-04-27 15:12:55] [config]   - 10
[2022-04-27 15:12:55] [config]   - 1
[2022-04-27 15:12:55] [config] lr-decay-strategy: epoch+stalled
[2022-04-27 15:12:55] [config] lr-report: false
[2022-04-27 15:12:55] [config] lr-warmup: 8000
[2022-04-27 15:12:55] [config] lr-warmup-at-reload: false
[2022-04-27 15:12:55] [config] lr-warmup-cycle: false
[2022-04-27 15:12:55] [config] lr-warmup-start-rate: 0
[2022-04-27 15:12:55] [config] max-length: 100
[2022-04-27 15:12:55] [config] max-length-crop: false
[2022-04-27 15:12:55] [config] max-length-factor: 3
[2022-04-27 15:12:55] [config] maxi-batch: 1000
[2022-04-27 15:12:55] [config] maxi-batch-sort: trg
[2022-04-27 15:12:55] [config] mini-batch: 1000
[2022-04-27 15:12:55] [config] mini-batch-fit: true
[2022-04-27 15:12:55] [config] mini-batch-fit-step: 10
[2022-04-27 15:12:55] [config] mini-batch-round-up: true
[2022-04-27 15:12:55] [config] mini-batch-track-lr: false
[2022-04-27 15:12:55] [config] mini-batch-warmup: 0
[2022-04-27 15:12:55] [config] mini-batch-words: 0
[2022-04-27 15:12:55] [config] mini-batch-words-ref: 0
[2022-04-27 15:12:55] [config] model: /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-27 15:12:55] [config] multi-loss-type: sum
[2022-04-27 15:12:55] [config] n-best: false
[2022-04-27 15:12:55] [config] no-nccl: false
[2022-04-27 15:12:55] [config] no-reload: false
[2022-04-27 15:12:55] [config] no-restore-corpus: false
[2022-04-27 15:12:55] [config] normalize: 1
[2022-04-27 15:12:55] [config] normalize-gradient: false
[2022-04-27 15:12:55] [config] num-devices: 0
[2022-04-27 15:12:55] [config] optimizer: adam
[2022-04-27 15:12:55] [config] optimizer-delay: 2
[2022-04-27 15:12:55] [config] optimizer-params:
[2022-04-27 15:12:55] [config]   - 0.9
[2022-04-27 15:12:55] [config]   - 0.998
[2022-04-27 15:12:55] [config]   - 1e-09
[2022-04-27 15:12:55] [config] output-omit-bias: false
[2022-04-27 15:12:55] [config] overwrite: true
[2022-04-27 15:12:55] [config] precision:
[2022-04-27 15:12:55] [config]   - float32
[2022-04-27 15:12:55] [config]   - float32
[2022-04-27 15:12:55] [config] pretrained-model: ""
[2022-04-27 15:12:55] [config] quantize-biases: false
[2022-04-27 15:12:55] [config] quantize-bits: 0
[2022-04-27 15:12:55] [config] quantize-log-based: false
[2022-04-27 15:12:55] [config] quantize-optimization-steps: 0
[2022-04-27 15:12:55] [config] quiet: false
[2022-04-27 15:12:55] [config] quiet-translation: false
[2022-04-27 15:12:55] [config] relative-paths: false
[2022-04-27 15:12:55] [config] right-left: false
[2022-04-27 15:12:55] [config] save-freq: 10000
[2022-04-27 15:12:55] [config] seed: 1111
[2022-04-27 15:12:55] [config] sentencepiece-alphas:
[2022-04-27 15:12:55] [config]   []
[2022-04-27 15:12:55] [config] sentencepiece-max-lines: 2000000
[2022-04-27 15:12:55] [config] sentencepiece-options: ""
[2022-04-27 15:12:55] [config] sharding: local
[2022-04-27 15:12:55] [config] shuffle: batches
[2022-04-27 15:12:55] [config] shuffle-in-ram: false
[2022-04-27 15:12:55] [config] sigterm: save-and-exit
[2022-04-27 15:12:55] [config] skip: false
[2022-04-27 15:12:55] [config] sqlite: ""
[2022-04-27 15:12:55] [config] sqlite-drop: false
[2022-04-27 15:12:55] [config] sync-freq: 200u
[2022-04-27 15:12:55] [config] sync-sgd: true
[2022-04-27 15:12:55] [config] tempdir: /run/nvme/job_11462556/tmp
[2022-04-27 15:12:55] [config] tied-embeddings: false
[2022-04-27 15:12:55] [config] tied-embeddings-all: true
[2022-04-27 15:12:55] [config] tied-embeddings-src: false
[2022-04-27 15:12:55] [config] train-embedder-rank:
[2022-04-27 15:12:55] [config]   []
[2022-04-27 15:12:55] [config] train-sets:
[2022-04-27 15:12:55] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/train/opusTCv20210807+bt.src.clean.spm32k.gz
[2022-04-27 15:12:55] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/train/opusTCv20210807+bt.trg.clean.spm32k.gz
[2022-04-27 15:12:55] [config] transformer-aan-activation: swish
[2022-04-27 15:12:55] [config] transformer-aan-depth: 2
[2022-04-27 15:12:55] [config] transformer-aan-nogate: false
[2022-04-27 15:12:55] [config] transformer-decoder-autoreg: self-attention
[2022-04-27 15:12:55] [config] transformer-depth-scaling: false
[2022-04-27 15:12:55] [config] transformer-dim-aan: 2048
[2022-04-27 15:12:55] [config] transformer-dim-ffn: 4096
[2022-04-27 15:12:55] [config] transformer-dropout: 0.1
[2022-04-27 15:12:55] [config] transformer-dropout-attention: 0
[2022-04-27 15:12:55] [config] transformer-dropout-ffn: 0
[2022-04-27 15:12:55] [config] transformer-ffn-activation: relu
[2022-04-27 15:12:55] [config] transformer-ffn-depth: 2
[2022-04-27 15:12:55] [config] transformer-guided-alignment-layer: last
[2022-04-27 15:12:55] [config] transformer-heads: 16
[2022-04-27 15:12:55] [config] transformer-no-projection: false
[2022-04-27 15:12:55] [config] transformer-pool: false
[2022-04-27 15:12:55] [config] transformer-postprocess: dan
[2022-04-27 15:12:55] [config] transformer-postprocess-emb: d
[2022-04-27 15:12:55] [config] transformer-postprocess-top: ""
[2022-04-27 15:12:55] [config] transformer-preprocess: ""
[2022-04-27 15:12:55] [config] transformer-tied-layers:
[2022-04-27 15:12:55] [config]   []
[2022-04-27 15:12:55] [config] transformer-train-position-embeddings: false
[2022-04-27 15:12:55] [config] tsv: false
[2022-04-27 15:12:55] [config] tsv-fields: 0
[2022-04-27 15:12:55] [config] type: transformer
[2022-04-27 15:12:55] [config] ulr: false
[2022-04-27 15:12:55] [config] ulr-dim-emb: 0
[2022-04-27 15:12:55] [config] ulr-dropout: 0
[2022-04-27 15:12:55] [config] ulr-keys-vectors: ""
[2022-04-27 15:12:55] [config] ulr-query-vectors: ""
[2022-04-27 15:12:55] [config] ulr-softmax-temperature: 1
[2022-04-27 15:12:55] [config] ulr-trainable-transformation: false
[2022-04-27 15:12:55] [config] unlikelihood-loss: false
[2022-04-27 15:12:55] [config] valid-freq: 10000
[2022-04-27 15:12:55] [config] valid-log: /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.valid1.log
[2022-04-27 15:12:55] [config] valid-max-length: 100
[2022-04-27 15:12:55] [config] valid-metrics:
[2022-04-27 15:12:55] [config]   - perplexity
[2022-04-27 15:12:55] [config] valid-mini-batch: 16
[2022-04-27 15:12:55] [config] valid-reset-stalled: false
[2022-04-27 15:12:55] [config] valid-script-args:
[2022-04-27 15:12:55] [config]   []
[2022-04-27 15:12:55] [config] valid-script-path: ""
[2022-04-27 15:12:55] [config] valid-sets:
[2022-04-27 15:12:55] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/val/Tatoeba-dev-v2021-08-07.src.spm32k
[2022-04-27 15:12:55] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/val/Tatoeba-dev-v2021-08-07.trg.spm32k
[2022-04-27 15:12:55] [config] valid-translation-output: ""
[2022-04-27 15:12:55] [config] version: v1.10.24; 12a1bfa 2021-10-11 16:59:52 +0100
[2022-04-27 15:12:55] [config] vocabs:
[2022-04-27 15:12:55] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-04-27 15:12:55] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-04-27 15:12:55] [config] word-penalty: 0
[2022-04-27 15:12:55] [config] word-scores: false
[2022-04-27 15:12:55] [config] workspace: 15000
[2022-04-27 15:12:55] [config] Loaded model has been created with Marian v1.10.24; 12a1bfa 2021-10-11 16:59:52 +0100
[2022-04-27 15:12:55] Using synchronous SGD
[2022-04-27 15:12:55] [comm] Compiled without MPI support. Running as a single process on r02g05.bullx
[2022-04-27 15:12:55] Synced seed 1111
[2022-04-27 15:12:55] [data] Loading vocabulary from JSON/Yaml file /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-04-27 15:12:56] [data] Setting vocabulary size for input 0 to 58,791
[2022-04-27 15:12:56] [data] Loading vocabulary from JSON/Yaml file /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-04-27 15:12:56] [data] Setting vocabulary size for input 1 to 58,791
[2022-04-27 15:12:56] [batching] Collecting statistics for batch fitting with step size 10
[2022-04-27 15:12:57] [memory] Extending reserved space to 15104 MB (device gpu0)
[2022-04-27 15:13:00] [memory] Extending reserved space to 15104 MB (device gpu1)
[2022-04-27 15:13:00] [comm] Using NCCL 2.8.3 for GPU communication
[2022-04-27 15:13:00] [comm] Using global sharding
[2022-04-27 15:13:01] [comm] NCCLCommunicators constructed successfully
[2022-04-27 15:13:01] [training] Using 2 GPUs
[2022-04-27 15:13:02] [logits] Applying loss function for 1 factor(s)
[2022-04-27 15:13:02] [memory] Reserving 902 MB, device gpu0
[2022-04-27 15:13:05] [gpu] 16-bit TensorCores enabled for float32 matrix operations
[2022-04-27 15:13:06] [memory] Reserving 902 MB, device gpu0
[2022-04-27 15:13:36] [batching] Done. Typical MB size is 27,120 target words
[2022-04-27 15:13:36] [memory] Extending reserved space to 15104 MB (device gpu0)
[2022-04-27 15:13:37] [memory] Extending reserved space to 15104 MB (device gpu1)
[2022-04-27 15:13:37] [comm] Using NCCL 2.8.3 for GPU communication
[2022-04-27 15:13:37] [comm] Using global sharding
[2022-04-27 15:13:37] [comm] NCCLCommunicators constructed successfully
[2022-04-27 15:13:37] [training] Using 2 GPUs
[2022-04-27 15:13:37] Loading model from /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-27 15:13:40] Loading model from /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-27 15:13:54] Allocating memory for general optimizer shards
[2022-04-27 15:13:54] [memory] Reserving 451 MB, device gpu0
[2022-04-27 15:13:54] [memory] Reserving 451 MB, device gpu1
[2022-04-27 15:13:54] Loading Adam parameters
[2022-04-27 15:13:55] [memory] Reserving 902 MB, device gpu0
[2022-04-27 15:13:55] [memory] Reserving 902 MB, device gpu1
[2022-04-27 15:13:56] [memory] Reserving 902 MB, device gpu0
[2022-04-27 15:13:57] [memory] Reserving 902 MB, device gpu1
[2022-04-27 15:13:57] [training] Master parameters and optimizers restored from training checkpoint /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-27 15:13:57] [data] Restoring the corpus state to epoch 4, batch 290000
[2022-04-27 15:15:32] Training started
[2022-04-27 15:15:32] [training] Batches are processed as 1 process(es) x 2 devices/process
[2022-04-27 15:15:33] [memory] Reserving 902 MB, device gpu0
[2022-04-27 15:15:33] [memory] Reserving 902 MB, device gpu1
[2022-04-27 15:15:35] Parameter type float32, optimization type float32, casting types false
[2022-04-27 19:04:49] Ep. 4 : Up. 300000 : Sen. 17,133,685 : Cost 3.09511113 : Time 13872.91s : 12898.55 words/s : gNorm 0.7175
[2022-04-27 19:04:49] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-27 19:04:53] Saving Adam parameters
[2022-04-27 19:04:57] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-27 19:05:10] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-04-27 19:05:12] [valid] Ep. 4 : Up. 300000 : perplexity : 14.9955 : new best
[2022-04-27 22:54:30] Ep. 4 : Up. 310000 : Sen. 31,342,276 : Cost 3.07570171 : Time 13780.52s : 13046.92 words/s : gNorm 0.6965
[2022-04-27 22:54:30] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-27 22:54:35] Saving Adam parameters
[2022-04-27 22:54:38] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-27 22:54:49] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-04-27 22:54:51] [valid] Ep. 4 : Up. 310000 : perplexity : 14.7873 : new best
[2022-04-28 02:45:20] Ep. 4 : Up. 320000 : Sen. 45,460,534 : Cost 3.06281662 : Time 13849.77s : 13017.42 words/s : gNorm 0.6487
[2022-04-28 02:45:20] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-28 02:45:23] Saving Adam parameters
[2022-04-28 02:45:27] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-28 02:45:38] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-04-28 02:45:40] [valid] Ep. 4 : Up. 320000 : perplexity : 14.5022 : new best
[2022-04-28 06:35:56] Ep. 4 : Up. 330000 : Sen. 59,621,480 : Cost 3.05238008 : Time 13836.48s : 13033.06 words/s : gNorm 0.6485
[2022-04-28 06:35:56] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-28 06:36:09] Saving Adam parameters
[2022-04-28 06:36:13] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-28 06:36:23] [valid] Ep. 4 : Up. 330000 : perplexity : 14.8917 : stalled 1 times (last best: 14.5022)
[2022-04-28 08:38:31] Seen 67,197,513 samples
[2022-04-28 08:38:31] Starting data epoch 5 in logical epoch 5
[2022-04-28 10:24:55] Ep. 5 : Up. 340000 : Sen. 6,735,967 : Cost 3.05539083 : Time 13738.37s : 13054.99 words/s : gNorm 0.6969
[2022-04-28 10:24:55] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-28 10:24:58] Saving Adam parameters
[2022-04-28 10:25:02] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-28 10:25:12] [valid] Ep. 5 : Up. 340000 : perplexity : 14.5181 : stalled 2 times (last best: 14.5022)
[2022-04-28 14:14:34] Ep. 5 : Up. 350000 : Sen. 21,211,622 : Cost 3.05899930 : Time 13779.42s : 12992.18 words/s : gNorm 0.7444
[2022-04-28 14:14:34] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-28 14:14:38] Saving Adam parameters
[2022-04-28 14:14:42] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-28 14:14:54] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-04-28 14:14:56] [valid] Ep. 5 : Up. 350000 : perplexity : 14.2179 : new best
[2022-04-28 18:05:27] Ep. 5 : Up. 360000 : Sen. 35,398,928 : Cost 3.04456067 : Time 13852.25s : 12985.45 words/s : gNorm 0.6764
[2022-04-28 18:05:27] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-28 18:05:31] Saving Adam parameters
[2022-04-28 18:05:34] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-28 18:05:45] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-04-28 18:05:51] [valid] Ep. 5 : Up. 360000 : perplexity : 14.13 : new best
[2022-04-28 21:55:11] Ep. 5 : Up. 370000 : Sen. 49,525,828 : Cost 3.03430462 : Time 13783.98s : 13075.91 words/s : gNorm 0.7101
[2022-04-28 21:55:11] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-28 21:55:15] Saving Adam parameters
[2022-04-28 21:55:18] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-28 21:55:28] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-04-28 21:55:31] [valid] Ep. 5 : Up. 370000 : perplexity : 13.8857 : new best
[2022-04-29 01:45:02] Ep. 5 : Up. 380000 : Sen. 63,672,389 : Cost 3.02671337 : Time 13790.78s : 13075.09 words/s : gNorm 0.7292
[2022-04-29 01:45:02] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-29 01:45:05] Saving Adam parameters
[2022-04-29 01:45:09] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-29 01:45:36] [valid] Ep. 5 : Up. 380000 : perplexity : 14.103 : stalled 1 times (last best: 13.8857)
[2022-04-29 03:22:53] Seen 69,730,539 samples
[2022-04-29 03:22:53] Starting data epoch 6 in logical epoch 6
[2022-04-29 05:33:30] Ep. 6 : Up. 390000 : Sen. 8,286,076 : Cost 3.03291059 : Time 13707.98s : 13082.16 words/s : gNorm 0.7302
[2022-04-29 05:33:30] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-29 05:33:40] Saving Adam parameters
[2022-04-29 05:33:43] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-29 05:33:53] [valid] Ep. 6 : Up. 390000 : perplexity : 13.9675 : stalled 2 times (last best: 13.8857)
[2022-04-29 09:21:12] Ep. 6 : Up. 400000 : Sen. 22,761,911 : Cost 3.03571939 : Time 13662.23s : 13099.93 words/s : gNorm 0.7204
[2022-04-29 09:21:12] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-29 09:21:18] Saving Adam parameters
[2022-04-29 09:21:22] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-29 09:21:37] [valid] Ep. 6 : Up. 400000 : perplexity : 13.9344 : stalled 3 times (last best: 13.8857)
[2022-04-29 13:12:22] Ep. 6 : Up. 410000 : Sen. 36,923,870 : Cost 3.01993179 : Time 13869.55s : 12978.12 words/s : gNorm 0.7267
[2022-04-29 13:12:22] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-29 13:12:26] Saving Adam parameters
[2022-04-29 13:12:29] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-29 13:12:40] [valid] Ep. 6 : Up. 410000 : perplexity : 13.9498 : stalled 4 times (last best: 13.8857)
[2022-04-29 17:20:14] Ep. 6 : Up. 420000 : Sen. 51,036,508 : Cost 3.01296067 : Time 14872.38s : 12121.54 words/s : gNorm 0.7306
[2022-04-29 17:20:14] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-29 17:20:18] Saving Adam parameters
[2022-04-29 17:20:21] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-29 17:20:32] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-04-29 17:20:34] [valid] Ep. 6 : Up. 420000 : perplexity : 13.7303 : new best
[2022-04-29 21:09:47] Ep. 6 : Up. 430000 : Sen. 65,198,377 : Cost 3.00744748 : Time 13772.70s : 13094.94 words/s : gNorm 0.7709
[2022-04-29 21:09:47] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-29 21:09:51] Saving Adam parameters
[2022-04-29 21:09:54] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-29 21:10:04] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-04-29 21:10:07] [valid] Ep. 6 : Up. 430000 : perplexity : 13.7285 : new best
[2022-04-29 22:22:39] Seen 69,730,539 samples
[2022-04-29 22:22:39] Starting data epoch 7 in logical epoch 7
[2022-04-30 00:57:54] Ep. 7 : Up. 440000 : Sen. 9,856,523 : Cost 3.01457381 : Time 13686.41s : 13092.07 words/s : gNorm 0.7485
[2022-04-30 00:57:54] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-30 00:57:57] Saving Adam parameters
[2022-04-30 00:58:01] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-30 00:58:12] [valid] Ep. 7 : Up. 440000 : perplexity : 13.7588 : stalled 1 times (last best: 13.7285)
[2022-04-30 04:46:42] Ep. 7 : Up. 450000 : Sen. 24,314,120 : Cost 3.01608348 : Time 13728.77s : 13040.31 words/s : gNorm 0.7009
[2022-04-30 04:46:42] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-30 04:46:46] Saving Adam parameters
[2022-04-30 04:46:50] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-30 04:47:01] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-04-30 04:47:03] [valid] Ep. 7 : Up. 450000 : perplexity : 13.7154 : new best
[2022-04-30 08:36:16] Ep. 7 : Up. 460000 : Sen. 38,439,378 : Cost 3.00154757 : Time 13773.50s : 13083.00 words/s : gNorm 0.6894
[2022-04-30 08:36:16] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-30 08:36:19] Saving Adam parameters
[2022-04-30 08:36:23] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-30 08:37:03] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-04-30 08:37:05] [valid] Ep. 7 : Up. 460000 : perplexity : 13.5861 : new best
[2022-04-30 12:28:37] Ep. 7 : Up. 470000 : Sen. 52,564,832 : Cost 2.99500966 : Time 13940.69s : 12928.15 words/s : gNorm 0.7275
[2022-04-30 12:28:37] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-30 12:28:40] Saving Adam parameters
[2022-04-30 12:28:44] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-30 12:28:54] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-04-30 12:28:56] [valid] Ep. 7 : Up. 470000 : perplexity : 13.3368 : new best
[2022-04-30 15:13:14] [marian] Marian v1.10.24; 12a1bfa 2021-10-11 16:59:52 +0100
[2022-04-30 15:13:14] [marian] Running on r15g08.bullx as process 85523 with command line:
[2022-04-30 15:13:14] [marian] /projappl/project_2001194/marian-dev/build/marian --task transformer-big --optimizer-delay 2 --early-stopping 10 --valid-freq 10000 --valid-sets /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/val/Tatoeba-dev-v2021-08-07.src.spm32k /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/val/Tatoeba-dev-v2021-08-07.trg.spm32k --valid-metrics perplexity --valid-mini-batch 16 --valid-max-length 100 --valid-log /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.valid1.log --beam-size 6 --normalize 1 --allow-unk --workspace 15000 --model /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz --train-sets /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/train/opusTCv20210807+bt.src.clean.spm32k.gz /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/train/opusTCv20210807+bt.trg.clean.spm32k.gz --vocabs /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.vocab.yml /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.vocab.yml --save-freq 10000 --disp-freq 10000 --log /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.train1.log --devices 0 1 --seed 1111 --tempdir /run/nvme/job_11520575/tmp --shuffle batches --sharding local --overwrite --keep-best
[2022-04-30 15:13:17] [config] after: 0e
[2022-04-30 15:13:17] [config] after-batches: 0
[2022-04-30 15:13:17] [config] after-epochs: 0
[2022-04-30 15:13:17] [config] all-caps-every: 0
[2022-04-30 15:13:17] [config] allow-unk: true
[2022-04-30 15:13:17] [config] authors: false
[2022-04-30 15:13:17] [config] beam-size: 6
[2022-04-30 15:13:17] [config] bert-class-symbol: "[CLS]"
[2022-04-30 15:13:17] [config] bert-mask-symbol: "[MASK]"
[2022-04-30 15:13:17] [config] bert-masking-fraction: 0.15
[2022-04-30 15:13:17] [config] bert-sep-symbol: "[SEP]"
[2022-04-30 15:13:17] [config] bert-train-type-embeddings: true
[2022-04-30 15:13:17] [config] bert-type-vocab-size: 2
[2022-04-30 15:13:17] [config] build-info: ""
[2022-04-30 15:13:17] [config] check-gradient-nan: false
[2022-04-30 15:13:17] [config] check-nan: false
[2022-04-30 15:13:17] [config] cite: false
[2022-04-30 15:13:17] [config] clip-norm: 0
[2022-04-30 15:13:17] [config] cost-scaling:
[2022-04-30 15:13:17] [config]   []
[2022-04-30 15:13:17] [config] cost-type: ce-mean-words
[2022-04-30 15:13:17] [config] cpu-threads: 0
[2022-04-30 15:13:17] [config] data-weighting: ""
[2022-04-30 15:13:17] [config] data-weighting-type: sentence
[2022-04-30 15:13:17] [config] dec-cell: gru
[2022-04-30 15:13:17] [config] dec-cell-base-depth: 2
[2022-04-30 15:13:17] [config] dec-cell-high-depth: 1
[2022-04-30 15:13:17] [config] dec-depth: 6
[2022-04-30 15:13:17] [config] devices:
[2022-04-30 15:13:17] [config]   - 0
[2022-04-30 15:13:17] [config]   - 1
[2022-04-30 15:13:17] [config] dim-emb: 1024
[2022-04-30 15:13:17] [config] dim-rnn: 1024
[2022-04-30 15:13:17] [config] dim-vocabs:
[2022-04-30 15:13:17] [config]   - 58791
[2022-04-30 15:13:17] [config]   - 58791
[2022-04-30 15:13:17] [config] disp-first: 0
[2022-04-30 15:13:17] [config] disp-freq: 10000
[2022-04-30 15:13:17] [config] disp-label-counts: true
[2022-04-30 15:13:17] [config] dropout-rnn: 0
[2022-04-30 15:13:17] [config] dropout-src: 0
[2022-04-30 15:13:17] [config] dropout-trg: 0
[2022-04-30 15:13:17] [config] dump-config: ""
[2022-04-30 15:13:17] [config] dynamic-gradient-scaling:
[2022-04-30 15:13:17] [config]   []
[2022-04-30 15:13:17] [config] early-stopping: 10
[2022-04-30 15:13:17] [config] early-stopping-on: first
[2022-04-30 15:13:17] [config] embedding-fix-src: false
[2022-04-30 15:13:17] [config] embedding-fix-trg: false
[2022-04-30 15:13:17] [config] embedding-normalization: false
[2022-04-30 15:13:17] [config] embedding-vectors:
[2022-04-30 15:13:17] [config]   []
[2022-04-30 15:13:17] [config] enc-cell: gru
[2022-04-30 15:13:17] [config] enc-cell-depth: 1
[2022-04-30 15:13:17] [config] enc-depth: 6
[2022-04-30 15:13:17] [config] enc-type: bidirectional
[2022-04-30 15:13:17] [config] english-title-case-every: 0
[2022-04-30 15:13:17] [config] exponential-smoothing: 0.0001
[2022-04-30 15:13:17] [config] factor-weight: 1
[2022-04-30 15:13:17] [config] factors-combine: sum
[2022-04-30 15:13:17] [config] factors-dim-emb: 0
[2022-04-30 15:13:17] [config] gradient-checkpointing: false
[2022-04-30 15:13:17] [config] gradient-norm-average-window: 100
[2022-04-30 15:13:17] [config] guided-alignment: none
[2022-04-30 15:13:17] [config] guided-alignment-cost: mse
[2022-04-30 15:13:17] [config] guided-alignment-weight: 0.1
[2022-04-30 15:13:17] [config] ignore-model-config: false
[2022-04-30 15:13:17] [config] input-types:
[2022-04-30 15:13:17] [config]   []
[2022-04-30 15:13:17] [config] interpolate-env-vars: false
[2022-04-30 15:13:17] [config] keep-best: true
[2022-04-30 15:13:17] [config] label-smoothing: 0.1
[2022-04-30 15:13:17] [config] layer-normalization: false
[2022-04-30 15:13:17] [config] learn-rate: 0.0002
[2022-04-30 15:13:17] [config] lemma-dependency: ""
[2022-04-30 15:13:17] [config] lemma-dim-emb: 0
[2022-04-30 15:13:17] [config] log: /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.train1.log
[2022-04-30 15:13:17] [config] log-level: info
[2022-04-30 15:13:17] [config] log-time-zone: ""
[2022-04-30 15:13:17] [config] logical-epoch:
[2022-04-30 15:13:17] [config]   - 1e
[2022-04-30 15:13:17] [config]   - 0
[2022-04-30 15:13:17] [config] lr-decay: 0
[2022-04-30 15:13:17] [config] lr-decay-freq: 50000
[2022-04-30 15:13:17] [config] lr-decay-inv-sqrt:
[2022-04-30 15:13:17] [config]   - 8000
[2022-04-30 15:13:17] [config] lr-decay-repeat-warmup: false
[2022-04-30 15:13:17] [config] lr-decay-reset-optimizer: false
[2022-04-30 15:13:17] [config] lr-decay-start:
[2022-04-30 15:13:17] [config]   - 10
[2022-04-30 15:13:17] [config]   - 1
[2022-04-30 15:13:17] [config] lr-decay-strategy: epoch+stalled
[2022-04-30 15:13:17] [config] lr-report: false
[2022-04-30 15:13:17] [config] lr-warmup: 8000
[2022-04-30 15:13:17] [config] lr-warmup-at-reload: false
[2022-04-30 15:13:17] [config] lr-warmup-cycle: false
[2022-04-30 15:13:17] [config] lr-warmup-start-rate: 0
[2022-04-30 15:13:17] [config] max-length: 100
[2022-04-30 15:13:17] [config] max-length-crop: false
[2022-04-30 15:13:17] [config] max-length-factor: 3
[2022-04-30 15:13:17] [config] maxi-batch: 1000
[2022-04-30 15:13:17] [config] maxi-batch-sort: trg
[2022-04-30 15:13:17] [config] mini-batch: 1000
[2022-04-30 15:13:17] [config] mini-batch-fit: true
[2022-04-30 15:13:17] [config] mini-batch-fit-step: 10
[2022-04-30 15:13:17] [config] mini-batch-round-up: true
[2022-04-30 15:13:17] [config] mini-batch-track-lr: false
[2022-04-30 15:13:17] [config] mini-batch-warmup: 0
[2022-04-30 15:13:17] [config] mini-batch-words: 0
[2022-04-30 15:13:17] [config] mini-batch-words-ref: 0
[2022-04-30 15:13:17] [config] model: /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-30 15:13:17] [config] multi-loss-type: sum
[2022-04-30 15:13:17] [config] n-best: false
[2022-04-30 15:13:17] [config] no-nccl: false
[2022-04-30 15:13:17] [config] no-reload: false
[2022-04-30 15:13:17] [config] no-restore-corpus: false
[2022-04-30 15:13:17] [config] normalize: 1
[2022-04-30 15:13:17] [config] normalize-gradient: false
[2022-04-30 15:13:17] [config] num-devices: 0
[2022-04-30 15:13:17] [config] optimizer: adam
[2022-04-30 15:13:17] [config] optimizer-delay: 2
[2022-04-30 15:13:17] [config] optimizer-params:
[2022-04-30 15:13:17] [config]   - 0.9
[2022-04-30 15:13:17] [config]   - 0.998
[2022-04-30 15:13:17] [config]   - 1e-09
[2022-04-30 15:13:17] [config] output-omit-bias: false
[2022-04-30 15:13:17] [config] overwrite: true
[2022-04-30 15:13:17] [config] precision:
[2022-04-30 15:13:17] [config]   - float32
[2022-04-30 15:13:17] [config]   - float32
[2022-04-30 15:13:17] [config] pretrained-model: ""
[2022-04-30 15:13:17] [config] quantize-biases: false
[2022-04-30 15:13:17] [config] quantize-bits: 0
[2022-04-30 15:13:17] [config] quantize-log-based: false
[2022-04-30 15:13:17] [config] quantize-optimization-steps: 0
[2022-04-30 15:13:17] [config] quiet: false
[2022-04-30 15:13:17] [config] quiet-translation: false
[2022-04-30 15:13:17] [config] relative-paths: false
[2022-04-30 15:13:17] [config] right-left: false
[2022-04-30 15:13:17] [config] save-freq: 10000
[2022-04-30 15:13:17] [config] seed: 1111
[2022-04-30 15:13:17] [config] sentencepiece-alphas:
[2022-04-30 15:13:17] [config]   []
[2022-04-30 15:13:17] [config] sentencepiece-max-lines: 2000000
[2022-04-30 15:13:17] [config] sentencepiece-options: ""
[2022-04-30 15:13:17] [config] sharding: local
[2022-04-30 15:13:17] [config] shuffle: batches
[2022-04-30 15:13:17] [config] shuffle-in-ram: false
[2022-04-30 15:13:17] [config] sigterm: save-and-exit
[2022-04-30 15:13:17] [config] skip: false
[2022-04-30 15:13:17] [config] sqlite: ""
[2022-04-30 15:13:17] [config] sqlite-drop: false
[2022-04-30 15:13:17] [config] sync-freq: 200u
[2022-04-30 15:13:17] [config] sync-sgd: true
[2022-04-30 15:13:17] [config] tempdir: /run/nvme/job_11520575/tmp
[2022-04-30 15:13:17] [config] tied-embeddings: false
[2022-04-30 15:13:17] [config] tied-embeddings-all: true
[2022-04-30 15:13:17] [config] tied-embeddings-src: false
[2022-04-30 15:13:17] [config] train-embedder-rank:
[2022-04-30 15:13:17] [config]   []
[2022-04-30 15:13:17] [config] train-sets:
[2022-04-30 15:13:17] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/train/opusTCv20210807+bt.src.clean.spm32k.gz
[2022-04-30 15:13:17] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/train/opusTCv20210807+bt.trg.clean.spm32k.gz
[2022-04-30 15:13:17] [config] transformer-aan-activation: swish
[2022-04-30 15:13:17] [config] transformer-aan-depth: 2
[2022-04-30 15:13:17] [config] transformer-aan-nogate: false
[2022-04-30 15:13:17] [config] transformer-decoder-autoreg: self-attention
[2022-04-30 15:13:17] [config] transformer-depth-scaling: false
[2022-04-30 15:13:17] [config] transformer-dim-aan: 2048
[2022-04-30 15:13:17] [config] transformer-dim-ffn: 4096
[2022-04-30 15:13:17] [config] transformer-dropout: 0.1
[2022-04-30 15:13:17] [config] transformer-dropout-attention: 0
[2022-04-30 15:13:17] [config] transformer-dropout-ffn: 0
[2022-04-30 15:13:17] [config] transformer-ffn-activation: relu
[2022-04-30 15:13:17] [config] transformer-ffn-depth: 2
[2022-04-30 15:13:17] [config] transformer-guided-alignment-layer: last
[2022-04-30 15:13:17] [config] transformer-heads: 16
[2022-04-30 15:13:17] [config] transformer-no-projection: false
[2022-04-30 15:13:17] [config] transformer-pool: false
[2022-04-30 15:13:17] [config] transformer-postprocess: dan
[2022-04-30 15:13:17] [config] transformer-postprocess-emb: d
[2022-04-30 15:13:17] [config] transformer-postprocess-top: ""
[2022-04-30 15:13:17] [config] transformer-preprocess: ""
[2022-04-30 15:13:17] [config] transformer-tied-layers:
[2022-04-30 15:13:17] [config]   []
[2022-04-30 15:13:17] [config] transformer-train-position-embeddings: false
[2022-04-30 15:13:17] [config] tsv: false
[2022-04-30 15:13:17] [config] tsv-fields: 0
[2022-04-30 15:13:17] [config] type: transformer
[2022-04-30 15:13:17] [config] ulr: false
[2022-04-30 15:13:17] [config] ulr-dim-emb: 0
[2022-04-30 15:13:17] [config] ulr-dropout: 0
[2022-04-30 15:13:17] [config] ulr-keys-vectors: ""
[2022-04-30 15:13:17] [config] ulr-query-vectors: ""
[2022-04-30 15:13:17] [config] ulr-softmax-temperature: 1
[2022-04-30 15:13:17] [config] ulr-trainable-transformation: false
[2022-04-30 15:13:17] [config] unlikelihood-loss: false
[2022-04-30 15:13:17] [config] valid-freq: 10000
[2022-04-30 15:13:17] [config] valid-log: /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.valid1.log
[2022-04-30 15:13:17] [config] valid-max-length: 100
[2022-04-30 15:13:17] [config] valid-metrics:
[2022-04-30 15:13:17] [config]   - perplexity
[2022-04-30 15:13:17] [config] valid-mini-batch: 16
[2022-04-30 15:13:17] [config] valid-reset-stalled: false
[2022-04-30 15:13:17] [config] valid-script-args:
[2022-04-30 15:13:17] [config]   []
[2022-04-30 15:13:17] [config] valid-script-path: ""
[2022-04-30 15:13:17] [config] valid-sets:
[2022-04-30 15:13:17] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/val/Tatoeba-dev-v2021-08-07.src.spm32k
[2022-04-30 15:13:17] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/val/Tatoeba-dev-v2021-08-07.trg.spm32k
[2022-04-30 15:13:17] [config] valid-translation-output: ""
[2022-04-30 15:13:17] [config] version: v1.10.24; 12a1bfa 2021-10-11 16:59:52 +0100
[2022-04-30 15:13:17] [config] vocabs:
[2022-04-30 15:13:17] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-04-30 15:13:17] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-04-30 15:13:17] [config] word-penalty: 0
[2022-04-30 15:13:17] [config] word-scores: false
[2022-04-30 15:13:17] [config] workspace: 15000
[2022-04-30 15:13:17] [config] Loaded model has been created with Marian v1.10.24; 12a1bfa 2021-10-11 16:59:52 +0100
[2022-04-30 15:13:17] Using synchronous SGD
[2022-04-30 15:13:17] [comm] Compiled without MPI support. Running as a single process on r15g08.bullx
[2022-04-30 15:13:17] Synced seed 1111
[2022-04-30 15:13:17] [data] Loading vocabulary from JSON/Yaml file /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-04-30 15:13:17] [data] Setting vocabulary size for input 0 to 58,791
[2022-04-30 15:13:17] [data] Loading vocabulary from JSON/Yaml file /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-04-30 15:13:18] [data] Setting vocabulary size for input 1 to 58,791
[2022-04-30 15:13:18] [batching] Collecting statistics for batch fitting with step size 10
[2022-04-30 15:13:20] [memory] Extending reserved space to 15104 MB (device gpu0)
[2022-04-30 15:13:23] [memory] Extending reserved space to 15104 MB (device gpu1)
[2022-04-30 15:13:23] [comm] Using NCCL 2.8.3 for GPU communication
[2022-04-30 15:13:23] [comm] Using global sharding
[2022-04-30 15:13:24] [comm] NCCLCommunicators constructed successfully
[2022-04-30 15:13:24] [training] Using 2 GPUs
[2022-04-30 15:13:25] [logits] Applying loss function for 1 factor(s)
[2022-04-30 15:13:25] [memory] Reserving 902 MB, device gpu0
[2022-04-30 15:13:28] [gpu] 16-bit TensorCores enabled for float32 matrix operations
[2022-04-30 15:13:28] [memory] Reserving 902 MB, device gpu0
[2022-04-30 15:13:59] [batching] Done. Typical MB size is 27,120 target words
[2022-04-30 15:13:59] [memory] Extending reserved space to 15104 MB (device gpu0)
[2022-04-30 15:13:59] [memory] Extending reserved space to 15104 MB (device gpu1)
[2022-04-30 15:13:59] [comm] Using NCCL 2.8.3 for GPU communication
[2022-04-30 15:13:59] [comm] Using global sharding
[2022-04-30 15:14:00] [comm] NCCLCommunicators constructed successfully
[2022-04-30 15:14:00] [training] Using 2 GPUs
[2022-04-30 15:14:00] Loading model from /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-30 15:14:03] Loading model from /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-30 15:14:13] Allocating memory for general optimizer shards
[2022-04-30 15:14:13] [memory] Reserving 451 MB, device gpu0
[2022-04-30 15:14:13] [memory] Reserving 451 MB, device gpu1
[2022-04-30 15:14:13] Loading Adam parameters
[2022-04-30 15:14:14] [memory] Reserving 902 MB, device gpu0
[2022-04-30 15:14:14] [memory] Reserving 902 MB, device gpu1
[2022-04-30 15:14:15] [memory] Reserving 902 MB, device gpu0
[2022-04-30 15:14:16] [memory] Reserving 902 MB, device gpu1
[2022-04-30 15:14:16] [training] Master parameters and optimizers restored from training checkpoint /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-30 15:14:16] [data] Restoring the corpus state to epoch 7, batch 470000
[2022-04-30 15:25:13] Training started
[2022-04-30 15:25:13] [training] Batches are processed as 1 process(es) x 2 devices/process
[2022-04-30 15:25:13] [memory] Reserving 902 MB, device gpu0
[2022-04-30 15:25:14] [memory] Reserving 902 MB, device gpu1
[2022-04-30 15:25:15] Parameter type float32, optimization type float32, casting types false
[2022-04-30 19:14:15] Ep. 7 : Up. 480000 : Sen. 66,730,611 : Cost 2.99088383 : Time 14415.65s : 12505.01 words/s : gNorm 0.7224
[2022-04-30 19:14:15] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-30 19:14:24] Saving Adam parameters
[2022-04-30 19:14:27] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-30 19:14:46] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-04-30 19:14:48] [valid] Ep. 7 : Up. 480000 : perplexity : 13.408 : new best
[2022-04-30 20:02:42] Seen 69,730,539 samples
[2022-04-30 20:02:42] Starting data epoch 8 in logical epoch 8
[2022-04-30 23:03:40] Ep. 8 : Up. 490000 : Sen. 11,406,001 : Cost 3.00018597 : Time 13759.37s : 13013.93 words/s : gNorm 0.7598
[2022-04-30 23:03:55] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-04-30 23:04:04] Saving Adam parameters
[2022-04-30 23:04:13] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-04-30 23:04:36] [valid] Ep. 8 : Up. 490000 : perplexity : 13.444 : stalled 1 times (last best: 13.408)
[2022-05-01 02:51:56] Ep. 8 : Up. 500000 : Sen. 25,862,020 : Cost 3.00016403 : Time 13691.65s : 13074.48 words/s : gNorm 0.7195
[2022-05-01 02:51:56] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-01 02:51:59] Saving Adam parameters
[2022-05-01 02:52:02] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-01 02:52:13] [valid] Ep. 8 : Up. 500000 : perplexity : 13.452 : stalled 2 times (last best: 13.408)
[2022-05-01 06:40:55] Ep. 8 : Up. 510000 : Sen. 39,965,889 : Cost 2.98520064 : Time 13739.68s : 13112.05 words/s : gNorm 0.8161
[2022-05-01 06:40:55] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-01 06:40:59] Saving Adam parameters
[2022-05-01 06:41:03] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-01 06:41:13] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-05-01 06:41:15] [valid] Ep. 8 : Up. 510000 : perplexity : 13.3814 : new best
[2022-05-01 10:30:12] Ep. 8 : Up. 520000 : Sen. 54,107,797 : Cost 2.98084235 : Time 13756.12s : 13106.17 words/s : gNorm 0.8468
[2022-05-01 10:30:12] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-01 10:30:15] Saving Adam parameters
[2022-05-01 10:30:19] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-01 10:30:29] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-05-01 10:30:31] [valid] Ep. 8 : Up. 520000 : perplexity : 13.2265 : new best
[2022-05-01 14:19:36] Ep. 8 : Up. 530000 : Sen. 68,264,669 : Cost 2.97663045 : Time 13764.72s : 13111.36 words/s : gNorm 0.7357
[2022-05-01 14:19:36] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-01 14:19:40] Saving Adam parameters
[2022-05-01 14:19:44] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-01 14:19:54] [valid] Ep. 8 : Up. 530000 : perplexity : 13.2528 : stalled 1 times (last best: 13.2265)
[2022-05-01 14:43:00] Seen 69,730,539 samples
[2022-05-01 14:43:00] Starting data epoch 9 in logical epoch 9
[2022-05-01 18:06:58] Ep. 9 : Up. 540000 : Sen. 12,979,373 : Cost 2.98843551 : Time 13642.06s : 13099.95 words/s : gNorm 0.7476
[2022-05-01 18:06:59] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-01 18:07:02] Saving Adam parameters
[2022-05-01 18:07:05] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-01 18:07:16] [valid] Ep. 9 : Up. 540000 : perplexity : 13.3415 : stalled 2 times (last best: 13.2265)
[2022-05-01 21:55:00] Ep. 9 : Up. 550000 : Sen. 27,383,949 : Cost 2.98505998 : Time 13681.21s : 13093.54 words/s : gNorm 0.7861
[2022-05-01 21:55:00] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-01 21:55:04] Saving Adam parameters
[2022-05-01 21:55:07] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-01 21:55:18] [valid] Ep. 9 : Up. 550000 : perplexity : 13.326 : stalled 3 times (last best: 13.2265)
[2022-05-02 01:44:12] Ep. 9 : Up. 560000 : Sen. 41,491,823 : Cost 2.97273088 : Time 13751.98s : 13112.97 words/s : gNorm 0.8261
[2022-05-02 01:44:12] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-02 01:44:21] Saving Adam parameters
[2022-05-02 01:44:25] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-02 01:44:35] [valid] Ep. 9 : Up. 560000 : perplexity : 13.2278 : stalled 4 times (last best: 13.2265)
[2022-05-02 05:33:34] Ep. 9 : Up. 570000 : Sen. 55,595,947 : Cost 2.96729231 : Time 13762.15s : 13095.75 words/s : gNorm 0.7769
[2022-05-02 05:33:34] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-02 05:33:38] Saving Adam parameters
[2022-05-02 05:33:41] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-02 05:33:55] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-05-02 05:33:58] [valid] Ep. 9 : Up. 570000 : perplexity : 13.1128 : new best
[2022-05-02 09:21:15] Seen 69,730,539 samples
[2022-05-02 09:21:16] Starting data epoch 10 in logical epoch 10
[2022-05-02 09:22:38] Ep. 10 : Up. 580000 : Sen. 62,932 : Cost 2.96450901 : Time 13743.97s : 13103.94 words/s : gNorm 0.8557
[2022-05-02 09:22:38] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-02 09:22:42] Saving Adam parameters
[2022-05-02 09:22:45] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-02 09:22:56] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-05-02 09:22:58] [valid] Ep. 10 : Up. 580000 : perplexity : 13.088 : new best
[2022-05-02 13:10:31] Ep. 10 : Up. 590000 : Sen. 14,518,216 : Cost 2.97939229 : Time 13672.93s : 13084.58 words/s : gNorm 0.8341
[2022-05-02 13:10:31] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-02 13:10:35] Saving Adam parameters
[2022-05-02 13:10:38] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-02 13:10:49] [valid] Ep. 10 : Up. 590000 : perplexity : 13.1401 : stalled 1 times (last best: 13.088)
[2022-05-02 16:58:54] Ep. 10 : Up. 600000 : Sen. 28,904,133 : Cost 2.97228479 : Time 13703.14s : 13094.85 words/s : gNorm 0.8182
[2022-05-02 16:58:54] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-02 16:58:58] Saving Adam parameters
[2022-05-02 16:59:02] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-02 16:59:12] [valid] Ep. 10 : Up. 600000 : perplexity : 13.2037 : stalled 2 times (last best: 13.088)
[2022-05-02 20:48:10] Ep. 10 : Up. 610000 : Sen. 43,007,164 : Cost 2.96111417 : Time 13755.62s : 13098.13 words/s : gNorm 0.8218
[2022-05-02 20:48:10] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-02 20:48:18] Saving Adam parameters
[2022-05-02 20:48:25] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-02 20:48:38] [valid] Ep. 10 : Up. 610000 : perplexity : 13.1337 : stalled 3 times (last best: 13.088)
[2022-05-03 00:37:35] Ep. 10 : Up. 620000 : Sen. 57,149,858 : Cost 2.95718718 : Time 13764.86s : 13096.62 words/s : gNorm 0.8129
[2022-05-03 00:37:35] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-03 00:37:39] Saving Adam parameters
[2022-05-03 00:37:43] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-03 00:37:54] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-05-03 00:37:56] [valid] Ep. 10 : Up. 620000 : perplexity : 12.9951 : new best
[2022-05-03 04:00:45] Seen 69,730,539 samples
[2022-05-03 04:00:45] Starting data epoch 11 in logical epoch 11
[2022-05-03 04:26:32] Ep. 11 : Up. 630000 : Sen. 1,622,494 : Cost 2.95519066 : Time 13737.07s : 13105.05 words/s : gNorm 0.8590
[2022-05-03 04:26:32] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-03 04:26:36] Saving Adam parameters
[2022-05-03 04:26:39] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-03 04:26:50] [valid] Ep. 11 : Up. 630000 : perplexity : 13.0163 : stalled 1 times (last best: 12.9951)
[2022-05-03 08:14:15] Ep. 11 : Up. 640000 : Sen. 16,092,772 : Cost 2.96900725 : Time 13662.69s : 13096.36 words/s : gNorm 0.8199
[2022-05-03 08:14:15] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-03 08:14:19] Saving Adam parameters
[2022-05-03 08:14:22] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-03 08:14:33] [valid] Ep. 11 : Up. 640000 : perplexity : 13.1038 : stalled 2 times (last best: 12.9951)
[2022-05-03 12:02:34] Ep. 11 : Up. 650000 : Sen. 30,419,490 : Cost 2.96092391 : Time 13699.10s : 13096.03 words/s : gNorm 0.8273
[2022-05-03 12:02:34] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-03 12:02:38] Saving Adam parameters
[2022-05-03 12:02:41] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-03 12:02:51] [valid] Ep. 11 : Up. 650000 : perplexity : 13.0691 : stalled 3 times (last best: 12.9951)
[2022-05-03 15:16:52] [marian] Marian v1.10.24; 12a1bfa 2021-10-11 16:59:52 +0100
[2022-05-03 15:16:52] [marian] Running on r18g06.bullx as process 112196 with command line:
[2022-05-03 15:16:52] [marian] /projappl/project_2001194/marian-dev/build/marian --task transformer-big --optimizer-delay 2 --early-stopping 10 --valid-freq 10000 --valid-sets /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/val/Tatoeba-dev-v2021-08-07.src.spm32k /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/val/Tatoeba-dev-v2021-08-07.trg.spm32k --valid-metrics perplexity --valid-mini-batch 16 --valid-max-length 100 --valid-log /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.valid1.log --beam-size 6 --normalize 1 --allow-unk --workspace 15000 --model /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz --train-sets /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/train/opusTCv20210807+bt.src.clean.spm32k.gz /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/train/opusTCv20210807+bt.trg.clean.spm32k.gz --vocabs /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.vocab.yml /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.vocab.yml --save-freq 10000 --disp-freq 10000 --log /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.train1.log --devices 0 1 --seed 1111 --tempdir /run/nvme/job_11572243/tmp --shuffle batches --sharding local --overwrite --keep-best
[2022-05-03 15:16:55] [config] after: 0e
[2022-05-03 15:16:55] [config] after-batches: 0
[2022-05-03 15:16:55] [config] after-epochs: 0
[2022-05-03 15:16:55] [config] all-caps-every: 0
[2022-05-03 15:16:55] [config] allow-unk: true
[2022-05-03 15:16:55] [config] authors: false
[2022-05-03 15:16:55] [config] beam-size: 6
[2022-05-03 15:16:55] [config] bert-class-symbol: "[CLS]"
[2022-05-03 15:16:55] [config] bert-mask-symbol: "[MASK]"
[2022-05-03 15:16:55] [config] bert-masking-fraction: 0.15
[2022-05-03 15:16:55] [config] bert-sep-symbol: "[SEP]"
[2022-05-03 15:16:55] [config] bert-train-type-embeddings: true
[2022-05-03 15:16:55] [config] bert-type-vocab-size: 2
[2022-05-03 15:16:55] [config] build-info: ""
[2022-05-03 15:16:55] [config] check-gradient-nan: false
[2022-05-03 15:16:55] [config] check-nan: false
[2022-05-03 15:16:55] [config] cite: false
[2022-05-03 15:16:55] [config] clip-norm: 0
[2022-05-03 15:16:55] [config] cost-scaling:
[2022-05-03 15:16:55] [config]   []
[2022-05-03 15:16:55] [config] cost-type: ce-mean-words
[2022-05-03 15:16:55] [config] cpu-threads: 0
[2022-05-03 15:16:55] [config] data-weighting: ""
[2022-05-03 15:16:55] [config] data-weighting-type: sentence
[2022-05-03 15:16:55] [config] dec-cell: gru
[2022-05-03 15:16:55] [config] dec-cell-base-depth: 2
[2022-05-03 15:16:55] [config] dec-cell-high-depth: 1
[2022-05-03 15:16:55] [config] dec-depth: 6
[2022-05-03 15:16:55] [config] devices:
[2022-05-03 15:16:55] [config]   - 0
[2022-05-03 15:16:55] [config]   - 1
[2022-05-03 15:16:55] [config] dim-emb: 1024
[2022-05-03 15:16:55] [config] dim-rnn: 1024
[2022-05-03 15:16:55] [config] dim-vocabs:
[2022-05-03 15:16:55] [config]   - 58791
[2022-05-03 15:16:55] [config]   - 58791
[2022-05-03 15:16:55] [config] disp-first: 0
[2022-05-03 15:16:55] [config] disp-freq: 10000
[2022-05-03 15:16:55] [config] disp-label-counts: true
[2022-05-03 15:16:55] [config] dropout-rnn: 0
[2022-05-03 15:16:55] [config] dropout-src: 0
[2022-05-03 15:16:55] [config] dropout-trg: 0
[2022-05-03 15:16:55] [config] dump-config: ""
[2022-05-03 15:16:55] [config] dynamic-gradient-scaling:
[2022-05-03 15:16:55] [config]   []
[2022-05-03 15:16:55] [config] early-stopping: 10
[2022-05-03 15:16:55] [config] early-stopping-on: first
[2022-05-03 15:16:55] [config] embedding-fix-src: false
[2022-05-03 15:16:55] [config] embedding-fix-trg: false
[2022-05-03 15:16:55] [config] embedding-normalization: false
[2022-05-03 15:16:55] [config] embedding-vectors:
[2022-05-03 15:16:55] [config]   []
[2022-05-03 15:16:55] [config] enc-cell: gru
[2022-05-03 15:16:55] [config] enc-cell-depth: 1
[2022-05-03 15:16:55] [config] enc-depth: 6
[2022-05-03 15:16:55] [config] enc-type: bidirectional
[2022-05-03 15:16:55] [config] english-title-case-every: 0
[2022-05-03 15:16:55] [config] exponential-smoothing: 0.0001
[2022-05-03 15:16:55] [config] factor-weight: 1
[2022-05-03 15:16:55] [config] factors-combine: sum
[2022-05-03 15:16:55] [config] factors-dim-emb: 0
[2022-05-03 15:16:55] [config] gradient-checkpointing: false
[2022-05-03 15:16:55] [config] gradient-norm-average-window: 100
[2022-05-03 15:16:55] [config] guided-alignment: none
[2022-05-03 15:16:55] [config] guided-alignment-cost: mse
[2022-05-03 15:16:55] [config] guided-alignment-weight: 0.1
[2022-05-03 15:16:55] [config] ignore-model-config: false
[2022-05-03 15:16:55] [config] input-types:
[2022-05-03 15:16:55] [config]   []
[2022-05-03 15:16:55] [config] interpolate-env-vars: false
[2022-05-03 15:16:55] [config] keep-best: true
[2022-05-03 15:16:55] [config] label-smoothing: 0.1
[2022-05-03 15:16:55] [config] layer-normalization: false
[2022-05-03 15:16:55] [config] learn-rate: 0.0002
[2022-05-03 15:16:55] [config] lemma-dependency: ""
[2022-05-03 15:16:55] [config] lemma-dim-emb: 0
[2022-05-03 15:16:55] [config] log: /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.train1.log
[2022-05-03 15:16:55] [config] log-level: info
[2022-05-03 15:16:55] [config] log-time-zone: ""
[2022-05-03 15:16:55] [config] logical-epoch:
[2022-05-03 15:16:55] [config]   - 1e
[2022-05-03 15:16:55] [config]   - 0
[2022-05-03 15:16:55] [config] lr-decay: 0
[2022-05-03 15:16:55] [config] lr-decay-freq: 50000
[2022-05-03 15:16:55] [config] lr-decay-inv-sqrt:
[2022-05-03 15:16:55] [config]   - 8000
[2022-05-03 15:16:55] [config] lr-decay-repeat-warmup: false
[2022-05-03 15:16:55] [config] lr-decay-reset-optimizer: false
[2022-05-03 15:16:55] [config] lr-decay-start:
[2022-05-03 15:16:55] [config]   - 10
[2022-05-03 15:16:55] [config]   - 1
[2022-05-03 15:16:55] [config] lr-decay-strategy: epoch+stalled
[2022-05-03 15:16:55] [config] lr-report: false
[2022-05-03 15:16:55] [config] lr-warmup: 8000
[2022-05-03 15:16:55] [config] lr-warmup-at-reload: false
[2022-05-03 15:16:55] [config] lr-warmup-cycle: false
[2022-05-03 15:16:55] [config] lr-warmup-start-rate: 0
[2022-05-03 15:16:55] [config] max-length: 100
[2022-05-03 15:16:55] [config] max-length-crop: false
[2022-05-03 15:16:55] [config] max-length-factor: 3
[2022-05-03 15:16:55] [config] maxi-batch: 1000
[2022-05-03 15:16:55] [config] maxi-batch-sort: trg
[2022-05-03 15:16:55] [config] mini-batch: 1000
[2022-05-03 15:16:55] [config] mini-batch-fit: true
[2022-05-03 15:16:55] [config] mini-batch-fit-step: 10
[2022-05-03 15:16:55] [config] mini-batch-round-up: true
[2022-05-03 15:16:55] [config] mini-batch-track-lr: false
[2022-05-03 15:16:55] [config] mini-batch-warmup: 0
[2022-05-03 15:16:55] [config] mini-batch-words: 0
[2022-05-03 15:16:55] [config] mini-batch-words-ref: 0
[2022-05-03 15:16:55] [config] model: /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-03 15:16:55] [config] multi-loss-type: sum
[2022-05-03 15:16:55] [config] n-best: false
[2022-05-03 15:16:55] [config] no-nccl: false
[2022-05-03 15:16:55] [config] no-reload: false
[2022-05-03 15:16:55] [config] no-restore-corpus: false
[2022-05-03 15:16:55] [config] normalize: 1
[2022-05-03 15:16:55] [config] normalize-gradient: false
[2022-05-03 15:16:55] [config] num-devices: 0
[2022-05-03 15:16:55] [config] optimizer: adam
[2022-05-03 15:16:55] [config] optimizer-delay: 2
[2022-05-03 15:16:55] [config] optimizer-params:
[2022-05-03 15:16:55] [config]   - 0.9
[2022-05-03 15:16:55] [config]   - 0.998
[2022-05-03 15:16:55] [config]   - 1e-09
[2022-05-03 15:16:55] [config] output-omit-bias: false
[2022-05-03 15:16:55] [config] overwrite: true
[2022-05-03 15:16:55] [config] precision:
[2022-05-03 15:16:55] [config]   - float32
[2022-05-03 15:16:55] [config]   - float32
[2022-05-03 15:16:55] [config] pretrained-model: ""
[2022-05-03 15:16:55] [config] quantize-biases: false
[2022-05-03 15:16:55] [config] quantize-bits: 0
[2022-05-03 15:16:55] [config] quantize-log-based: false
[2022-05-03 15:16:55] [config] quantize-optimization-steps: 0
[2022-05-03 15:16:55] [config] quiet: false
[2022-05-03 15:16:55] [config] quiet-translation: false
[2022-05-03 15:16:55] [config] relative-paths: false
[2022-05-03 15:16:55] [config] right-left: false
[2022-05-03 15:16:55] [config] save-freq: 10000
[2022-05-03 15:16:55] [config] seed: 1111
[2022-05-03 15:16:55] [config] sentencepiece-alphas:
[2022-05-03 15:16:55] [config]   []
[2022-05-03 15:16:55] [config] sentencepiece-max-lines: 2000000
[2022-05-03 15:16:55] [config] sentencepiece-options: ""
[2022-05-03 15:16:55] [config] sharding: local
[2022-05-03 15:16:55] [config] shuffle: batches
[2022-05-03 15:16:55] [config] shuffle-in-ram: false
[2022-05-03 15:16:55] [config] sigterm: save-and-exit
[2022-05-03 15:16:55] [config] skip: false
[2022-05-03 15:16:55] [config] sqlite: ""
[2022-05-03 15:16:55] [config] sqlite-drop: false
[2022-05-03 15:16:55] [config] sync-freq: 200u
[2022-05-03 15:16:55] [config] sync-sgd: true
[2022-05-03 15:16:55] [config] tempdir: /run/nvme/job_11572243/tmp
[2022-05-03 15:16:55] [config] tied-embeddings: false
[2022-05-03 15:16:55] [config] tied-embeddings-all: true
[2022-05-03 15:16:55] [config] tied-embeddings-src: false
[2022-05-03 15:16:55] [config] train-embedder-rank:
[2022-05-03 15:16:55] [config]   []
[2022-05-03 15:16:55] [config] train-sets:
[2022-05-03 15:16:55] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/train/opusTCv20210807+bt.src.clean.spm32k.gz
[2022-05-03 15:16:55] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/train/opusTCv20210807+bt.trg.clean.spm32k.gz
[2022-05-03 15:16:55] [config] transformer-aan-activation: swish
[2022-05-03 15:16:55] [config] transformer-aan-depth: 2
[2022-05-03 15:16:55] [config] transformer-aan-nogate: false
[2022-05-03 15:16:55] [config] transformer-decoder-autoreg: self-attention
[2022-05-03 15:16:55] [config] transformer-depth-scaling: false
[2022-05-03 15:16:55] [config] transformer-dim-aan: 2048
[2022-05-03 15:16:55] [config] transformer-dim-ffn: 4096
[2022-05-03 15:16:55] [config] transformer-dropout: 0.1
[2022-05-03 15:16:55] [config] transformer-dropout-attention: 0
[2022-05-03 15:16:55] [config] transformer-dropout-ffn: 0
[2022-05-03 15:16:55] [config] transformer-ffn-activation: relu
[2022-05-03 15:16:55] [config] transformer-ffn-depth: 2
[2022-05-03 15:16:55] [config] transformer-guided-alignment-layer: last
[2022-05-03 15:16:55] [config] transformer-heads: 16
[2022-05-03 15:16:55] [config] transformer-no-projection: false
[2022-05-03 15:16:55] [config] transformer-pool: false
[2022-05-03 15:16:55] [config] transformer-postprocess: dan
[2022-05-03 15:16:55] [config] transformer-postprocess-emb: d
[2022-05-03 15:16:55] [config] transformer-postprocess-top: ""
[2022-05-03 15:16:55] [config] transformer-preprocess: ""
[2022-05-03 15:16:55] [config] transformer-tied-layers:
[2022-05-03 15:16:55] [config]   []
[2022-05-03 15:16:55] [config] transformer-train-position-embeddings: false
[2022-05-03 15:16:55] [config] tsv: false
[2022-05-03 15:16:55] [config] tsv-fields: 0
[2022-05-03 15:16:55] [config] type: transformer
[2022-05-03 15:16:55] [config] ulr: false
[2022-05-03 15:16:55] [config] ulr-dim-emb: 0
[2022-05-03 15:16:55] [config] ulr-dropout: 0
[2022-05-03 15:16:55] [config] ulr-keys-vectors: ""
[2022-05-03 15:16:55] [config] ulr-query-vectors: ""
[2022-05-03 15:16:55] [config] ulr-softmax-temperature: 1
[2022-05-03 15:16:55] [config] ulr-trainable-transformation: false
[2022-05-03 15:16:55] [config] unlikelihood-loss: false
[2022-05-03 15:16:55] [config] valid-freq: 10000
[2022-05-03 15:16:55] [config] valid-log: /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.valid1.log
[2022-05-03 15:16:55] [config] valid-max-length: 100
[2022-05-03 15:16:55] [config] valid-metrics:
[2022-05-03 15:16:55] [config]   - perplexity
[2022-05-03 15:16:55] [config] valid-mini-batch: 16
[2022-05-03 15:16:55] [config] valid-reset-stalled: false
[2022-05-03 15:16:55] [config] valid-script-args:
[2022-05-03 15:16:55] [config]   []
[2022-05-03 15:16:55] [config] valid-script-path: ""
[2022-05-03 15:16:55] [config] valid-sets:
[2022-05-03 15:16:55] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/val/Tatoeba-dev-v2021-08-07.src.spm32k
[2022-05-03 15:16:55] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/val/Tatoeba-dev-v2021-08-07.trg.spm32k
[2022-05-03 15:16:55] [config] valid-translation-output: ""
[2022-05-03 15:16:55] [config] version: v1.10.24; 12a1bfa 2021-10-11 16:59:52 +0100
[2022-05-03 15:16:55] [config] vocabs:
[2022-05-03 15:16:55] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-05-03 15:16:55] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-05-03 15:16:55] [config] word-penalty: 0
[2022-05-03 15:16:55] [config] word-scores: false
[2022-05-03 15:16:55] [config] workspace: 15000
[2022-05-03 15:16:55] [config] Loaded model has been created with Marian v1.10.24; 12a1bfa 2021-10-11 16:59:52 +0100
[2022-05-03 15:16:55] Using synchronous SGD
[2022-05-03 15:16:55] [comm] Compiled without MPI support. Running as a single process on r18g06.bullx
[2022-05-03 15:16:55] Synced seed 1111
[2022-05-03 15:16:55] [data] Loading vocabulary from JSON/Yaml file /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-05-03 15:16:56] [data] Setting vocabulary size for input 0 to 58,791
[2022-05-03 15:16:56] [data] Loading vocabulary from JSON/Yaml file /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-05-03 15:16:56] [data] Setting vocabulary size for input 1 to 58,791
[2022-05-03 15:16:56] [batching] Collecting statistics for batch fitting with step size 10
[2022-05-03 15:16:58] [memory] Extending reserved space to 15104 MB (device gpu0)
[2022-05-03 15:17:00] [memory] Extending reserved space to 15104 MB (device gpu1)
[2022-05-03 15:17:00] [comm] Using NCCL 2.8.3 for GPU communication
[2022-05-03 15:17:00] [comm] Using global sharding
[2022-05-03 15:17:02] [comm] NCCLCommunicators constructed successfully
[2022-05-03 15:17:02] [training] Using 2 GPUs
[2022-05-03 15:17:02] [logits] Applying loss function for 1 factor(s)
[2022-05-03 15:17:02] [memory] Reserving 902 MB, device gpu0
[2022-05-03 15:17:08] [gpu] 16-bit TensorCores enabled for float32 matrix operations
[2022-05-03 15:17:08] [memory] Reserving 902 MB, device gpu0
[2022-05-03 15:17:39] [batching] Done. Typical MB size is 27,120 target words
[2022-05-03 15:17:39] [memory] Extending reserved space to 15104 MB (device gpu0)
[2022-05-03 15:17:39] [memory] Extending reserved space to 15104 MB (device gpu1)
[2022-05-03 15:17:39] [comm] Using NCCL 2.8.3 for GPU communication
[2022-05-03 15:17:39] [comm] Using global sharding
[2022-05-03 15:17:40] [comm] NCCLCommunicators constructed successfully
[2022-05-03 15:17:40] [training] Using 2 GPUs
[2022-05-03 15:17:40] Loading model from /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-03 15:17:42] Loading model from /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-03 15:17:51] Allocating memory for general optimizer shards
[2022-05-03 15:17:51] [memory] Reserving 451 MB, device gpu0
[2022-05-03 15:17:51] [memory] Reserving 451 MB, device gpu1
[2022-05-03 15:17:51] Loading Adam parameters
[2022-05-03 15:17:52] [memory] Reserving 902 MB, device gpu0
[2022-05-03 15:17:53] [memory] Reserving 902 MB, device gpu1
[2022-05-03 15:17:54] [memory] Reserving 902 MB, device gpu0
[2022-05-03 15:17:54] [memory] Reserving 902 MB, device gpu1
[2022-05-03 15:17:54] [training] Master parameters and optimizers restored from training checkpoint /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-03 15:17:54] [data] Restoring the corpus state to epoch 11, batch 650000
[2022-05-03 15:24:21] Training started
[2022-05-03 15:24:21] [training] Batches are processed as 1 process(es) x 2 devices/process
[2022-05-03 15:24:21] [memory] Reserving 902 MB, device gpu0
[2022-05-03 15:24:22] [memory] Reserving 902 MB, device gpu1
[2022-05-03 15:24:24] Parameter type float32, optimization type float32, casting types false
[2022-05-03 19:14:11] Ep. 11 : Up. 660000 : Sen. 44,508,317 : Cost 2.95104623 : Time 14192.82s : 12696.82 words/s : gNorm 0.8828
[2022-05-03 19:14:12] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-03 19:14:16] Saving Adam parameters
[2022-05-03 19:14:20] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-03 19:14:31] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-05-03 19:14:33] [valid] Ep. 11 : Up. 660000 : perplexity : 12.9244 : new best
[2022-05-03 23:04:42] Ep. 11 : Up. 670000 : Sen. 58,680,666 : Cost 2.94719219 : Time 13829.98s : 13042.41 words/s : gNorm 0.8079
[2022-05-03 23:04:42] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-03 23:04:45] Saving Adam parameters
[2022-05-03 23:04:48] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-03 23:04:59] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-05-03 23:05:02] [valid] Ep. 11 : Up. 670000 : perplexity : 12.86 : new best
[2022-05-04 02:04:07] Seen 69,730,539 samples
[2022-05-04 02:04:07] Starting data epoch 12 in logical epoch 12
[2022-05-04 02:54:52] Ep. 12 : Up. 680000 : Sen. 3,178,335 : Cost 2.94847178 : Time 13810.32s : 13015.76 words/s : gNorm 0.9086
[2022-05-04 02:54:52] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-04 02:54:56] Saving Adam parameters
[2022-05-04 02:54:59] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-04 02:55:10] [valid] Ep. 12 : Up. 680000 : perplexity : 12.952 : stalled 1 times (last best: 12.86)
[2022-05-04 06:43:43] Ep. 12 : Up. 690000 : Sen. 17,660,423 : Cost 2.95988011 : Time 13731.50s : 13027.34 words/s : gNorm 0.9073
[2022-05-04 06:43:43] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-04 06:43:47] Saving Adam parameters
[2022-05-04 06:43:50] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-04 06:44:01] [valid] Ep. 12 : Up. 690000 : perplexity : 13.0127 : stalled 2 times (last best: 12.86)
[2022-05-04 10:34:40] Ep. 12 : Up. 700000 : Sen. 31,939,213 : Cost 2.95055819 : Time 13856.80s : 12963.20 words/s : gNorm 0.9079
[2022-05-04 10:34:40] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-04 10:34:45] Saving Adam parameters
[2022-05-04 10:34:48] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-04 10:34:59] [valid] Ep. 12 : Up. 700000 : perplexity : 12.9376 : stalled 3 times (last best: 12.86)
[2022-05-04 14:29:08] Ep. 12 : Up. 710000 : Sen. 46,051,897 : Cost 2.94247985 : Time 14067.76s : 12813.80 words/s : gNorm 0.8527
[2022-05-04 14:29:08] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-04 14:29:12] Saving Adam parameters
[2022-05-04 14:29:15] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-04 14:29:26] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-05-04 14:29:28] [valid] Ep. 12 : Up. 710000 : perplexity : 12.857 : new best
[2022-05-04 18:18:59] Ep. 12 : Up. 720000 : Sen. 60,204,646 : Cost 2.93863487 : Time 13790.36s : 13077.92 words/s : gNorm 0.8794
[2022-05-04 18:18:59] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-04 18:19:02] Saving Adam parameters
[2022-05-04 18:19:06] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-04 18:19:16] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-05-04 18:19:19] [valid] Ep. 12 : Up. 720000 : perplexity : 12.7517 : new best
[2022-05-04 20:53:00] Seen 69,730,539 samples
[2022-05-04 20:53:00] Starting data epoch 13 in logical epoch 13
[2022-05-04 22:07:56] Ep. 13 : Up. 730000 : Sen. 4,742,309 : Cost 2.94220781 : Time 13737.65s : 13075.76 words/s : gNorm 0.8863
[2022-05-04 22:07:56] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-04 22:08:00] Saving Adam parameters
[2022-05-04 22:08:03] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-04 22:08:14] [valid] Ep. 13 : Up. 730000 : perplexity : 12.8267 : stalled 1 times (last best: 12.7517)
[2022-05-05 01:56:06] Ep. 13 : Up. 740000 : Sen. 19,210,185 : Cost 2.95007753 : Time 13689.64s : 13066.35 words/s : gNorm 0.8486
[2022-05-05 01:56:06] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-05 01:56:09] Saving Adam parameters
[2022-05-05 01:56:13] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-05 01:56:23] [valid] Ep. 13 : Up. 740000 : perplexity : 12.8327 : stalled 2 times (last best: 12.7517)
[2022-05-05 05:45:08] Ep. 13 : Up. 750000 : Sen. 33,462,581 : Cost 2.94187498 : Time 13741.46s : 13086.64 words/s : gNorm 0.9023
[2022-05-05 05:45:08] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-05 05:45:11] Saving Adam parameters
[2022-05-05 05:45:15] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-05 05:45:25] [valid] Ep. 13 : Up. 750000 : perplexity : 12.7776 : stalled 3 times (last best: 12.7517)
[2022-05-05 09:34:38] Ep. 13 : Up. 760000 : Sen. 47,559,950 : Cost 2.93515730 : Time 13770.75s : 13091.30 words/s : gNorm 0.8757
[2022-05-05 09:34:38] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-05 09:34:42] Saving Adam parameters
[2022-05-05 09:34:45] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-05 09:34:56] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-05-05 09:34:58] [valid] Ep. 13 : Up. 760000 : perplexity : 12.6187 : new best
[2022-05-05 13:24:24] Ep. 13 : Up. 770000 : Sen. 61,714,942 : Cost 2.93073773 : Time 13785.20s : 13078.19 words/s : gNorm 0.8735
[2022-05-05 13:24:24] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-05 13:24:27] Saving Adam parameters
[2022-05-05 13:24:31] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-05 13:24:41] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-05-05 13:24:44] [valid] Ep. 13 : Up. 770000 : perplexity : 12.5387 : new best
[2022-05-05 15:33:39] Seen 69,730,539 samples
[2022-05-05 15:33:39] Starting data epoch 14 in logical epoch 14
[2022-05-05 17:13:11] Ep. 14 : Up. 780000 : Sen. 6,303,810 : Cost 2.93575525 : Time 13726.96s : 13065.86 words/s : gNorm 0.8502
[2022-05-05 17:13:11] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-05 17:13:15] Saving Adam parameters
[2022-05-05 17:13:18] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-05 17:13:28] [valid] Ep. 14 : Up. 780000 : perplexity : 12.6784 : stalled 1 times (last best: 12.5387)
[2022-05-05 21:01:27] Ep. 14 : Up. 790000 : Sen. 20,777,535 : Cost 2.94321060 : Time 13695.84s : 13075.24 words/s : gNorm 0.8878
[2022-05-05 21:01:27] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-05 21:01:30] Saving Adam parameters
[2022-05-05 21:01:33] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-05 21:01:45] [valid] Ep. 14 : Up. 790000 : perplexity : 12.6974 : stalled 2 times (last best: 12.5387)
[2022-05-06 00:50:52] Ep. 14 : Up. 800000 : Sen. 34,973,616 : Cost 2.93327260 : Time 13765.76s : 13064.12 words/s : gNorm 0.9514
[2022-05-06 00:50:53] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-06 00:50:56] Saving Adam parameters
[2022-05-06 00:51:00] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-06 00:51:10] [valid] Ep. 14 : Up. 800000 : perplexity : 12.5925 : stalled 3 times (last best: 12.5387)
[2022-05-06 04:40:39] Ep. 14 : Up. 810000 : Sen. 49,084,517 : Cost 2.92745686 : Time 13785.94s : 13075.81 words/s : gNorm 0.8893
[2022-05-06 04:40:39] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-06 04:40:42] Saving Adam parameters
[2022-05-06 04:40:46] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-06 04:40:56] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-05-06 04:40:58] [valid] Ep. 14 : Up. 810000 : perplexity : 12.5091 : new best
[2022-05-06 08:30:23] Ep. 14 : Up. 820000 : Sen. 63,269,455 : Cost 2.92412019 : Time 13784.01s : 13079.57 words/s : gNorm 0.8428
[2022-05-06 08:30:23] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-06 08:30:26] Saving Adam parameters
[2022-05-06 08:30:30] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-06 08:30:41] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-05-06 08:30:43] [valid] Ep. 14 : Up. 820000 : perplexity : 12.4725 : new best
[2022-05-06 10:15:21] Seen 69,730,539 samples
[2022-05-06 10:15:21] Starting data epoch 15 in logical epoch 15
[2022-05-06 12:19:49] Ep. 15 : Up. 830000 : Sen. 7,869,032 : Cost 2.93075347 : Time 13766.23s : 13033.02 words/s : gNorm 0.8918
[2022-05-06 12:19:49] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-06 12:19:52] Saving Adam parameters
[2022-05-06 12:19:56] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-06 12:20:06] [valid] Ep. 15 : Up. 830000 : perplexity : 12.5792 : stalled 1 times (last best: 12.4725)
[2022-05-06 15:17:08] [marian] Marian v1.10.24; 12a1bfa 2021-10-11 16:59:52 +0100
[2022-05-06 15:17:08] [marian] Running on r18g06.bullx as process 85865 with command line:
[2022-05-06 15:17:08] [marian] /projappl/project_2001194/marian-dev/build/marian --task transformer-big --optimizer-delay 2 --early-stopping 10 --valid-freq 10000 --valid-sets /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/val/Tatoeba-dev-v2021-08-07.src.spm32k /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/val/Tatoeba-dev-v2021-08-07.trg.spm32k --valid-metrics perplexity --valid-mini-batch 16 --valid-max-length 100 --valid-log /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.valid1.log --beam-size 6 --normalize 1 --allow-unk --workspace 15000 --model /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz --train-sets /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/train/opusTCv20210807+bt.src.clean.spm32k.gz /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/train/opusTCv20210807+bt.trg.clean.spm32k.gz --vocabs /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.vocab.yml /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.vocab.yml --save-freq 10000 --disp-freq 10000 --log /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.train1.log --devices 0 1 --seed 1111 --tempdir /run/nvme/job_11630554/tmp --shuffle batches --sharding local --overwrite --keep-best
[2022-05-06 15:17:10] [config] after: 0e
[2022-05-06 15:17:10] [config] after-batches: 0
[2022-05-06 15:17:10] [config] after-epochs: 0
[2022-05-06 15:17:10] [config] all-caps-every: 0
[2022-05-06 15:17:10] [config] allow-unk: true
[2022-05-06 15:17:10] [config] authors: false
[2022-05-06 15:17:10] [config] beam-size: 6
[2022-05-06 15:17:10] [config] bert-class-symbol: "[CLS]"
[2022-05-06 15:17:10] [config] bert-mask-symbol: "[MASK]"
[2022-05-06 15:17:10] [config] bert-masking-fraction: 0.15
[2022-05-06 15:17:10] [config] bert-sep-symbol: "[SEP]"
[2022-05-06 15:17:10] [config] bert-train-type-embeddings: true
[2022-05-06 15:17:10] [config] bert-type-vocab-size: 2
[2022-05-06 15:17:10] [config] build-info: ""
[2022-05-06 15:17:10] [config] check-gradient-nan: false
[2022-05-06 15:17:10] [config] check-nan: false
[2022-05-06 15:17:10] [config] cite: false
[2022-05-06 15:17:10] [config] clip-norm: 0
[2022-05-06 15:17:10] [config] cost-scaling:
[2022-05-06 15:17:10] [config]   []
[2022-05-06 15:17:10] [config] cost-type: ce-mean-words
[2022-05-06 15:17:10] [config] cpu-threads: 0
[2022-05-06 15:17:10] [config] data-weighting: ""
[2022-05-06 15:17:10] [config] data-weighting-type: sentence
[2022-05-06 15:17:10] [config] dec-cell: gru
[2022-05-06 15:17:10] [config] dec-cell-base-depth: 2
[2022-05-06 15:17:10] [config] dec-cell-high-depth: 1
[2022-05-06 15:17:10] [config] dec-depth: 6
[2022-05-06 15:17:10] [config] devices:
[2022-05-06 15:17:10] [config]   - 0
[2022-05-06 15:17:10] [config]   - 1
[2022-05-06 15:17:10] [config] dim-emb: 1024
[2022-05-06 15:17:10] [config] dim-rnn: 1024
[2022-05-06 15:17:10] [config] dim-vocabs:
[2022-05-06 15:17:10] [config]   - 58791
[2022-05-06 15:17:10] [config]   - 58791
[2022-05-06 15:17:10] [config] disp-first: 0
[2022-05-06 15:17:10] [config] disp-freq: 10000
[2022-05-06 15:17:10] [config] disp-label-counts: true
[2022-05-06 15:17:10] [config] dropout-rnn: 0
[2022-05-06 15:17:10] [config] dropout-src: 0
[2022-05-06 15:17:10] [config] dropout-trg: 0
[2022-05-06 15:17:10] [config] dump-config: ""
[2022-05-06 15:17:10] [config] dynamic-gradient-scaling:
[2022-05-06 15:17:10] [config]   []
[2022-05-06 15:17:10] [config] early-stopping: 10
[2022-05-06 15:17:10] [config] early-stopping-on: first
[2022-05-06 15:17:10] [config] embedding-fix-src: false
[2022-05-06 15:17:10] [config] embedding-fix-trg: false
[2022-05-06 15:17:10] [config] embedding-normalization: false
[2022-05-06 15:17:10] [config] embedding-vectors:
[2022-05-06 15:17:10] [config]   []
[2022-05-06 15:17:10] [config] enc-cell: gru
[2022-05-06 15:17:10] [config] enc-cell-depth: 1
[2022-05-06 15:17:10] [config] enc-depth: 6
[2022-05-06 15:17:10] [config] enc-type: bidirectional
[2022-05-06 15:17:10] [config] english-title-case-every: 0
[2022-05-06 15:17:10] [config] exponential-smoothing: 0.0001
[2022-05-06 15:17:10] [config] factor-weight: 1
[2022-05-06 15:17:10] [config] factors-combine: sum
[2022-05-06 15:17:10] [config] factors-dim-emb: 0
[2022-05-06 15:17:10] [config] gradient-checkpointing: false
[2022-05-06 15:17:10] [config] gradient-norm-average-window: 100
[2022-05-06 15:17:10] [config] guided-alignment: none
[2022-05-06 15:17:10] [config] guided-alignment-cost: mse
[2022-05-06 15:17:10] [config] guided-alignment-weight: 0.1
[2022-05-06 15:17:10] [config] ignore-model-config: false
[2022-05-06 15:17:10] [config] input-types:
[2022-05-06 15:17:10] [config]   []
[2022-05-06 15:17:10] [config] interpolate-env-vars: false
[2022-05-06 15:17:10] [config] keep-best: true
[2022-05-06 15:17:10] [config] label-smoothing: 0.1
[2022-05-06 15:17:10] [config] layer-normalization: false
[2022-05-06 15:17:10] [config] learn-rate: 0.0002
[2022-05-06 15:17:10] [config] lemma-dependency: ""
[2022-05-06 15:17:10] [config] lemma-dim-emb: 0
[2022-05-06 15:17:10] [config] log: /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.train1.log
[2022-05-06 15:17:10] [config] log-level: info
[2022-05-06 15:17:10] [config] log-time-zone: ""
[2022-05-06 15:17:10] [config] logical-epoch:
[2022-05-06 15:17:10] [config]   - 1e
[2022-05-06 15:17:10] [config]   - 0
[2022-05-06 15:17:10] [config] lr-decay: 0
[2022-05-06 15:17:10] [config] lr-decay-freq: 50000
[2022-05-06 15:17:10] [config] lr-decay-inv-sqrt:
[2022-05-06 15:17:10] [config]   - 8000
[2022-05-06 15:17:10] [config] lr-decay-repeat-warmup: false
[2022-05-06 15:17:10] [config] lr-decay-reset-optimizer: false
[2022-05-06 15:17:10] [config] lr-decay-start:
[2022-05-06 15:17:10] [config]   - 10
[2022-05-06 15:17:10] [config]   - 1
[2022-05-06 15:17:10] [config] lr-decay-strategy: epoch+stalled
[2022-05-06 15:17:10] [config] lr-report: false
[2022-05-06 15:17:10] [config] lr-warmup: 8000
[2022-05-06 15:17:10] [config] lr-warmup-at-reload: false
[2022-05-06 15:17:10] [config] lr-warmup-cycle: false
[2022-05-06 15:17:10] [config] lr-warmup-start-rate: 0
[2022-05-06 15:17:10] [config] max-length: 100
[2022-05-06 15:17:10] [config] max-length-crop: false
[2022-05-06 15:17:10] [config] max-length-factor: 3
[2022-05-06 15:17:10] [config] maxi-batch: 1000
[2022-05-06 15:17:10] [config] maxi-batch-sort: trg
[2022-05-06 15:17:10] [config] mini-batch: 1000
[2022-05-06 15:17:10] [config] mini-batch-fit: true
[2022-05-06 15:17:10] [config] mini-batch-fit-step: 10
[2022-05-06 15:17:10] [config] mini-batch-round-up: true
[2022-05-06 15:17:10] [config] mini-batch-track-lr: false
[2022-05-06 15:17:10] [config] mini-batch-warmup: 0
[2022-05-06 15:17:10] [config] mini-batch-words: 0
[2022-05-06 15:17:10] [config] mini-batch-words-ref: 0
[2022-05-06 15:17:10] [config] model: /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-06 15:17:10] [config] multi-loss-type: sum
[2022-05-06 15:17:10] [config] n-best: false
[2022-05-06 15:17:10] [config] no-nccl: false
[2022-05-06 15:17:10] [config] no-reload: false
[2022-05-06 15:17:10] [config] no-restore-corpus: false
[2022-05-06 15:17:10] [config] normalize: 1
[2022-05-06 15:17:10] [config] normalize-gradient: false
[2022-05-06 15:17:10] [config] num-devices: 0
[2022-05-06 15:17:10] [config] optimizer: adam
[2022-05-06 15:17:10] [config] optimizer-delay: 2
[2022-05-06 15:17:10] [config] optimizer-params:
[2022-05-06 15:17:10] [config]   - 0.9
[2022-05-06 15:17:10] [config]   - 0.998
[2022-05-06 15:17:10] [config]   - 1e-09
[2022-05-06 15:17:10] [config] output-omit-bias: false
[2022-05-06 15:17:10] [config] overwrite: true
[2022-05-06 15:17:10] [config] precision:
[2022-05-06 15:17:10] [config]   - float32
[2022-05-06 15:17:10] [config]   - float32
[2022-05-06 15:17:10] [config] pretrained-model: ""
[2022-05-06 15:17:10] [config] quantize-biases: false
[2022-05-06 15:17:10] [config] quantize-bits: 0
[2022-05-06 15:17:10] [config] quantize-log-based: false
[2022-05-06 15:17:10] [config] quantize-optimization-steps: 0
[2022-05-06 15:17:10] [config] quiet: false
[2022-05-06 15:17:10] [config] quiet-translation: false
[2022-05-06 15:17:10] [config] relative-paths: false
[2022-05-06 15:17:10] [config] right-left: false
[2022-05-06 15:17:10] [config] save-freq: 10000
[2022-05-06 15:17:10] [config] seed: 1111
[2022-05-06 15:17:10] [config] sentencepiece-alphas:
[2022-05-06 15:17:10] [config]   []
[2022-05-06 15:17:10] [config] sentencepiece-max-lines: 2000000
[2022-05-06 15:17:10] [config] sentencepiece-options: ""
[2022-05-06 15:17:10] [config] sharding: local
[2022-05-06 15:17:10] [config] shuffle: batches
[2022-05-06 15:17:10] [config] shuffle-in-ram: false
[2022-05-06 15:17:10] [config] sigterm: save-and-exit
[2022-05-06 15:17:10] [config] skip: false
[2022-05-06 15:17:10] [config] sqlite: ""
[2022-05-06 15:17:10] [config] sqlite-drop: false
[2022-05-06 15:17:10] [config] sync-freq: 200u
[2022-05-06 15:17:10] [config] sync-sgd: true
[2022-05-06 15:17:10] [config] tempdir: /run/nvme/job_11630554/tmp
[2022-05-06 15:17:10] [config] tied-embeddings: false
[2022-05-06 15:17:10] [config] tied-embeddings-all: true
[2022-05-06 15:17:10] [config] tied-embeddings-src: false
[2022-05-06 15:17:10] [config] train-embedder-rank:
[2022-05-06 15:17:10] [config]   []
[2022-05-06 15:17:10] [config] train-sets:
[2022-05-06 15:17:10] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/train/opusTCv20210807+bt.src.clean.spm32k.gz
[2022-05-06 15:17:10] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/train/opusTCv20210807+bt.trg.clean.spm32k.gz
[2022-05-06 15:17:10] [config] transformer-aan-activation: swish
[2022-05-06 15:17:10] [config] transformer-aan-depth: 2
[2022-05-06 15:17:10] [config] transformer-aan-nogate: false
[2022-05-06 15:17:10] [config] transformer-decoder-autoreg: self-attention
[2022-05-06 15:17:10] [config] transformer-depth-scaling: false
[2022-05-06 15:17:10] [config] transformer-dim-aan: 2048
[2022-05-06 15:17:10] [config] transformer-dim-ffn: 4096
[2022-05-06 15:17:10] [config] transformer-dropout: 0.1
[2022-05-06 15:17:10] [config] transformer-dropout-attention: 0
[2022-05-06 15:17:10] [config] transformer-dropout-ffn: 0
[2022-05-06 15:17:10] [config] transformer-ffn-activation: relu
[2022-05-06 15:17:10] [config] transformer-ffn-depth: 2
[2022-05-06 15:17:10] [config] transformer-guided-alignment-layer: last
[2022-05-06 15:17:10] [config] transformer-heads: 16
[2022-05-06 15:17:10] [config] transformer-no-projection: false
[2022-05-06 15:17:10] [config] transformer-pool: false
[2022-05-06 15:17:10] [config] transformer-postprocess: dan
[2022-05-06 15:17:10] [config] transformer-postprocess-emb: d
[2022-05-06 15:17:10] [config] transformer-postprocess-top: ""
[2022-05-06 15:17:10] [config] transformer-preprocess: ""
[2022-05-06 15:17:10] [config] transformer-tied-layers:
[2022-05-06 15:17:10] [config]   []
[2022-05-06 15:17:10] [config] transformer-train-position-embeddings: false
[2022-05-06 15:17:10] [config] tsv: false
[2022-05-06 15:17:10] [config] tsv-fields: 0
[2022-05-06 15:17:10] [config] type: transformer
[2022-05-06 15:17:10] [config] ulr: false
[2022-05-06 15:17:10] [config] ulr-dim-emb: 0
[2022-05-06 15:17:10] [config] ulr-dropout: 0
[2022-05-06 15:17:10] [config] ulr-keys-vectors: ""
[2022-05-06 15:17:10] [config] ulr-query-vectors: ""
[2022-05-06 15:17:10] [config] ulr-softmax-temperature: 1
[2022-05-06 15:17:10] [config] ulr-trainable-transformation: false
[2022-05-06 15:17:10] [config] unlikelihood-loss: false
[2022-05-06 15:17:10] [config] valid-freq: 10000
[2022-05-06 15:17:10] [config] valid-log: /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.valid1.log
[2022-05-06 15:17:10] [config] valid-max-length: 100
[2022-05-06 15:17:10] [config] valid-metrics:
[2022-05-06 15:17:10] [config]   - perplexity
[2022-05-06 15:17:10] [config] valid-mini-batch: 16
[2022-05-06 15:17:10] [config] valid-reset-stalled: false
[2022-05-06 15:17:10] [config] valid-script-args:
[2022-05-06 15:17:10] [config]   []
[2022-05-06 15:17:10] [config] valid-script-path: ""
[2022-05-06 15:17:10] [config] valid-sets:
[2022-05-06 15:17:10] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/val/Tatoeba-dev-v2021-08-07.src.spm32k
[2022-05-06 15:17:10] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/val/Tatoeba-dev-v2021-08-07.trg.spm32k
[2022-05-06 15:17:10] [config] valid-translation-output: ""
[2022-05-06 15:17:10] [config] version: v1.10.24; 12a1bfa 2021-10-11 16:59:52 +0100
[2022-05-06 15:17:10] [config] vocabs:
[2022-05-06 15:17:10] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-05-06 15:17:10] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-05-06 15:17:10] [config] word-penalty: 0
[2022-05-06 15:17:10] [config] word-scores: false
[2022-05-06 15:17:10] [config] workspace: 15000
[2022-05-06 15:17:10] [config] Loaded model has been created with Marian v1.10.24; 12a1bfa 2021-10-11 16:59:52 +0100
[2022-05-06 15:17:10] Using synchronous SGD
[2022-05-06 15:17:10] [comm] Compiled without MPI support. Running as a single process on r18g06.bullx
[2022-05-06 15:17:10] Synced seed 1111
[2022-05-06 15:17:10] [data] Loading vocabulary from JSON/Yaml file /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-05-06 15:17:10] [data] Setting vocabulary size for input 0 to 58,791
[2022-05-06 15:17:10] [data] Loading vocabulary from JSON/Yaml file /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-05-06 15:17:11] [data] Setting vocabulary size for input 1 to 58,791
[2022-05-06 15:17:11] [batching] Collecting statistics for batch fitting with step size 10
[2022-05-06 15:17:12] [memory] Extending reserved space to 15104 MB (device gpu0)
[2022-05-06 15:17:14] [memory] Extending reserved space to 15104 MB (device gpu1)
[2022-05-06 15:17:14] [comm] Using NCCL 2.8.3 for GPU communication
[2022-05-06 15:17:14] [comm] Using global sharding
[2022-05-06 15:17:15] [comm] NCCLCommunicators constructed successfully
[2022-05-06 15:17:15] [training] Using 2 GPUs
[2022-05-06 15:17:15] [logits] Applying loss function for 1 factor(s)
[2022-05-06 15:17:15] [memory] Reserving 902 MB, device gpu0
[2022-05-06 15:17:17] [gpu] 16-bit TensorCores enabled for float32 matrix operations
[2022-05-06 15:17:17] [memory] Reserving 902 MB, device gpu0
[2022-05-06 15:17:47] [batching] Done. Typical MB size is 27,120 target words
[2022-05-06 15:17:48] [memory] Extending reserved space to 15104 MB (device gpu0)
[2022-05-06 15:17:48] [memory] Extending reserved space to 15104 MB (device gpu1)
[2022-05-06 15:17:48] [comm] Using NCCL 2.8.3 for GPU communication
[2022-05-06 15:17:48] [comm] Using global sharding
[2022-05-06 15:17:48] [comm] NCCLCommunicators constructed successfully
[2022-05-06 15:17:48] [training] Using 2 GPUs
[2022-05-06 15:17:48] Loading model from /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-06 15:17:50] Loading model from /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-06 15:17:59] Allocating memory for general optimizer shards
[2022-05-06 15:17:59] [memory] Reserving 451 MB, device gpu0
[2022-05-06 15:17:59] [memory] Reserving 451 MB, device gpu1
[2022-05-06 15:17:59] Loading Adam parameters
[2022-05-06 15:18:00] [memory] Reserving 902 MB, device gpu0
[2022-05-06 15:18:00] [memory] Reserving 902 MB, device gpu1
[2022-05-06 15:18:01] [memory] Reserving 902 MB, device gpu0
[2022-05-06 15:18:01] [memory] Reserving 902 MB, device gpu1
[2022-05-06 15:18:02] [training] Master parameters and optimizers restored from training checkpoint /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-06 15:18:02] [data] Restoring the corpus state to epoch 15, batch 830000
[2022-05-06 15:19:47] Training started
[2022-05-06 15:19:47] [training] Batches are processed as 1 process(es) x 2 devices/process
[2022-05-06 15:19:47] [memory] Reserving 902 MB, device gpu0
[2022-05-06 15:19:48] [memory] Reserving 902 MB, device gpu1
[2022-05-06 15:19:49] Parameter type float32, optimization type float32, casting types false
[2022-05-06 19:08:52] Ep. 15 : Up. 840000 : Sen. 22,343,696 : Cost 2.93656850 : Time 13864.56s : 12902.87 words/s : gNorm 0.9714
[2022-05-06 19:08:52] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-06 19:08:57] Saving Adam parameters
[2022-05-06 19:09:00] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-06 19:09:11] [valid] Ep. 15 : Up. 840000 : perplexity : 12.6177 : stalled 1 times (last best: 12.4725)
[2022-05-06 22:58:49] Ep. 15 : Up. 850000 : Sen. 36,521,935 : Cost 2.92571115 : Time 13796.12s : 13054.96 words/s : gNorm 0.8986
[2022-05-06 22:58:49] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-06 22:58:52] Saving Adam parameters
[2022-05-06 22:58:56] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-06 22:59:06] [valid] Ep. 15 : Up. 850000 : perplexity : 12.5756 : stalled 2 times (last best: 12.4725)
[2022-05-07 02:49:06] Ep. 15 : Up. 860000 : Sen. 50,596,829 : Cost 2.91881394 : Time 13817.07s : 13041.37 words/s : gNorm 0.8988
[2022-05-07 02:49:06] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-07 02:49:10] Saving Adam parameters
[2022-05-07 02:49:13] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-07 02:49:24] [valid] Ep. 15 : Up. 860000 : perplexity : 12.5064 : stalled 3 times (last best: 12.4725)
[2022-05-07 06:39:22] Ep. 15 : Up. 870000 : Sen. 64,783,916 : Cost 2.91934323 : Time 13815.80s : 13046.27 words/s : gNorm 0.8979
[2022-05-07 06:39:22] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-07 06:39:25] Saving Adam parameters
[2022-05-07 06:39:29] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-07 06:39:39] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-05-07 06:39:42] [valid] Ep. 15 : Up. 870000 : perplexity : 12.4668 : new best
[2022-05-07 07:59:25] Seen 69,730,539 samples
[2022-05-07 07:59:25] Starting data epoch 16 in logical epoch 16
[2022-05-07 10:28:20] Ep. 16 : Up. 880000 : Sen. 9,430,524 : Cost 2.92654371 : Time 13738.27s : 13052.03 words/s : gNorm 0.8893
[2022-05-07 10:28:20] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-07 10:28:24] Saving Adam parameters
[2022-05-07 10:28:27] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-07 10:28:37] [valid] Ep. 16 : Up. 880000 : perplexity : 12.5816 : stalled 1 times (last best: 12.4668)
[2022-05-07 14:16:33] Ep. 16 : Up. 890000 : Sen. 23,896,118 : Cost 2.92990184 : Time 13692.89s : 13064.96 words/s : gNorm 0.9398
[2022-05-07 14:16:33] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-07 14:16:36] Saving Adam parameters
[2022-05-07 14:16:40] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-07 14:16:51] [valid] Ep. 16 : Up. 890000 : perplexity : 12.5926 : stalled 2 times (last best: 12.4668)
[2022-05-07 18:06:27] Ep. 16 : Up. 900000 : Sen. 38,025,695 : Cost 2.91798210 : Time 13794.33s : 13056.32 words/s : gNorm 1.0311
[2022-05-07 18:06:27] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-07 18:06:34] Saving Adam parameters
[2022-05-07 18:06:38] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-07 18:06:49] [valid] Ep. 16 : Up. 900000 : perplexity : 12.5089 : stalled 3 times (last best: 12.4668)
[2022-05-07 21:56:32] Ep. 16 : Up. 910000 : Sen. 52,159,143 : Cost 2.91460919 : Time 13804.17s : 13064.15 words/s : gNorm 0.8893
[2022-05-07 21:56:32] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-07 21:56:36] Saving Adam parameters
[2022-05-07 21:56:39] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-07 21:56:50] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-05-07 21:56:52] [valid] Ep. 16 : Up. 910000 : perplexity : 12.413 : new best
[2022-05-08 01:46:18] Ep. 16 : Up. 920000 : Sen. 66,317,213 : Cost 2.91174912 : Time 13785.97s : 13072.65 words/s : gNorm 1.0020
[2022-05-08 01:46:18] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-08 01:46:22] Saving Adam parameters
[2022-05-08 01:46:25] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-08 01:46:36] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-05-08 01:46:39] [valid] Ep. 16 : Up. 920000 : perplexity : 12.3217 : new best
[2022-05-08 02:41:27] Seen 69,730,539 samples
[2022-05-08 02:41:27] Starting data epoch 17 in logical epoch 17
[2022-05-08 05:34:36] Ep. 17 : Up. 930000 : Sen. 10,984,976 : Cost 2.92257643 : Time 13698.01s : 13075.44 words/s : gNorm 0.9846
[2022-05-08 05:34:36] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-08 05:34:40] Saving Adam parameters
[2022-05-08 05:34:43] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-08 05:34:54] [valid] Ep. 17 : Up. 930000 : perplexity : 12.4512 : stalled 1 times (last best: 12.3217)
[2022-05-08 09:22:51] Ep. 17 : Up. 940000 : Sen. 25,441,706 : Cost 2.92411947 : Time 13695.22s : 13080.74 words/s : gNorm 0.8822
[2022-05-08 09:22:51] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-08 09:22:55] Saving Adam parameters
[2022-05-08 09:22:58] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-08 09:23:09] [valid] Ep. 17 : Up. 940000 : perplexity : 12.4114 : stalled 2 times (last best: 12.3217)
[2022-05-08 13:12:16] Ep. 17 : Up. 950000 : Sen. 39,518,756 : Cost 2.91216278 : Time 13764.32s : 13078.03 words/s : gNorm 0.9528
[2022-05-08 13:12:16] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-08 13:12:19] Saving Adam parameters
[2022-05-08 13:12:23] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-08 13:12:33] [valid] Ep. 17 : Up. 950000 : perplexity : 12.3605 : stalled 3 times (last best: 12.3217)
[2022-05-08 17:01:45] Ep. 17 : Up. 960000 : Sen. 53,686,004 : Cost 2.90784836 : Time 13769.83s : 13098.15 words/s : gNorm 0.8574
[2022-05-08 17:01:46] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-08 17:01:49] Saving Adam parameters
[2022-05-08 17:01:52] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-08 17:02:03] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-05-08 17:02:05] [valid] Ep. 17 : Up. 960000 : perplexity : 12.3065 : new best
[2022-05-08 20:51:20] Ep. 17 : Up. 970000 : Sen. 67,814,235 : Cost 2.90716267 : Time 13774.56s : 13083.17 words/s : gNorm 0.9143
[2022-05-08 20:51:20] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-08 20:51:24] Saving Adam parameters
[2022-05-08 20:51:27] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-08 20:51:38] [valid] Ep. 17 : Up. 970000 : perplexity : 12.3134 : stalled 1 times (last best: 12.3065)
[2022-05-08 21:21:46] Seen 69,730,539 samples
[2022-05-08 21:21:46] Starting data epoch 18 in logical epoch 18
[2022-05-09 00:39:28] Ep. 18 : Up. 980000 : Sen. 12,547,649 : Cost 2.91863704 : Time 13688.23s : 13082.60 words/s : gNorm 0.9633
[2022-05-09 00:39:29] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-09 00:39:32] Saving Adam parameters
[2022-05-09 00:39:36] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-09 00:39:47] [valid] Ep. 18 : Up. 980000 : perplexity : 12.4441 : stalled 2 times (last best: 12.3065)
[2022-05-09 04:27:34] Ep. 18 : Up. 990000 : Sen. 26,957,865 : Cost 2.91764450 : Time 13685.94s : 13087.30 words/s : gNorm 0.9154
[2022-05-09 04:27:34] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-09 04:27:38] Saving Adam parameters
[2022-05-09 04:27:42] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-09 04:27:53] [valid] Ep. 18 : Up. 990000 : perplexity : 12.363 : stalled 3 times (last best: 12.3065)
[2022-05-09 08:17:08] Ep. 18 : Up. 1000000 : Sen. 41,061,096 : Cost 2.90649009 : Time 13773.28s : 13078.09 words/s : gNorm 0.9240
[2022-05-09 08:17:08] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-09 08:17:12] Saving Adam parameters
[2022-05-09 08:17:15] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-09 08:17:28] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-05-09 08:17:30] [valid] Ep. 18 : Up. 1000000 : perplexity : 12.2919 : new best
[2022-05-09 12:06:37] Ep. 18 : Up. 1010000 : Sen. 55,202,455 : Cost 2.90346622 : Time 13768.92s : 13093.17 words/s : gNorm 0.8940
[2022-05-09 12:06:37] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-09 12:06:40] Saving Adam parameters
[2022-05-09 12:06:44] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-09 12:06:55] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-05-09 12:06:57] [valid] Ep. 18 : Up. 1010000 : perplexity : 12.2516 : new best
[2022-05-09 15:17:29] [marian] Marian v1.10.24; 12a1bfa 2021-10-11 16:59:52 +0100
[2022-05-09 15:17:29] [marian] Running on r18g06.bullx as process 133517 with command line:
[2022-05-09 15:17:29] [marian] /projappl/project_2001194/marian-dev/build/marian --task transformer-big --optimizer-delay 2 --early-stopping 10 --valid-freq 10000 --valid-sets /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/val/Tatoeba-dev-v2021-08-07.src.spm32k /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/val/Tatoeba-dev-v2021-08-07.trg.spm32k --valid-metrics perplexity --valid-mini-batch 16 --valid-max-length 100 --valid-log /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.valid1.log --beam-size 6 --normalize 1 --allow-unk --workspace 15000 --model /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz --train-sets /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/train/opusTCv20210807+bt.src.clean.spm32k.gz /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/train/opusTCv20210807+bt.trg.clean.spm32k.gz --vocabs /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.vocab.yml /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.vocab.yml --save-freq 10000 --disp-freq 10000 --log /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.train1.log --devices 0 1 --seed 1111 --tempdir /run/nvme/job_11668422/tmp --shuffle batches --sharding local --overwrite --keep-best
[2022-05-09 15:17:31] [config] after: 0e
[2022-05-09 15:17:31] [config] after-batches: 0
[2022-05-09 15:17:31] [config] after-epochs: 0
[2022-05-09 15:17:31] [config] all-caps-every: 0
[2022-05-09 15:17:31] [config] allow-unk: true
[2022-05-09 15:17:31] [config] authors: false
[2022-05-09 15:17:31] [config] beam-size: 6
[2022-05-09 15:17:31] [config] bert-class-symbol: "[CLS]"
[2022-05-09 15:17:31] [config] bert-mask-symbol: "[MASK]"
[2022-05-09 15:17:31] [config] bert-masking-fraction: 0.15
[2022-05-09 15:17:31] [config] bert-sep-symbol: "[SEP]"
[2022-05-09 15:17:31] [config] bert-train-type-embeddings: true
[2022-05-09 15:17:31] [config] bert-type-vocab-size: 2
[2022-05-09 15:17:31] [config] build-info: ""
[2022-05-09 15:17:31] [config] check-gradient-nan: false
[2022-05-09 15:17:31] [config] check-nan: false
[2022-05-09 15:17:31] [config] cite: false
[2022-05-09 15:17:31] [config] clip-norm: 0
[2022-05-09 15:17:31] [config] cost-scaling:
[2022-05-09 15:17:31] [config]   []
[2022-05-09 15:17:31] [config] cost-type: ce-mean-words
[2022-05-09 15:17:31] [config] cpu-threads: 0
[2022-05-09 15:17:31] [config] data-weighting: ""
[2022-05-09 15:17:31] [config] data-weighting-type: sentence
[2022-05-09 15:17:31] [config] dec-cell: gru
[2022-05-09 15:17:31] [config] dec-cell-base-depth: 2
[2022-05-09 15:17:31] [config] dec-cell-high-depth: 1
[2022-05-09 15:17:31] [config] dec-depth: 6
[2022-05-09 15:17:31] [config] devices:
[2022-05-09 15:17:31] [config]   - 0
[2022-05-09 15:17:31] [config]   - 1
[2022-05-09 15:17:31] [config] dim-emb: 1024
[2022-05-09 15:17:31] [config] dim-rnn: 1024
[2022-05-09 15:17:31] [config] dim-vocabs:
[2022-05-09 15:17:31] [config]   - 58791
[2022-05-09 15:17:31] [config]   - 58791
[2022-05-09 15:17:31] [config] disp-first: 0
[2022-05-09 15:17:31] [config] disp-freq: 10000
[2022-05-09 15:17:31] [config] disp-label-counts: true
[2022-05-09 15:17:31] [config] dropout-rnn: 0
[2022-05-09 15:17:31] [config] dropout-src: 0
[2022-05-09 15:17:31] [config] dropout-trg: 0
[2022-05-09 15:17:31] [config] dump-config: ""
[2022-05-09 15:17:31] [config] dynamic-gradient-scaling:
[2022-05-09 15:17:31] [config]   []
[2022-05-09 15:17:31] [config] early-stopping: 10
[2022-05-09 15:17:31] [config] early-stopping-on: first
[2022-05-09 15:17:31] [config] embedding-fix-src: false
[2022-05-09 15:17:31] [config] embedding-fix-trg: false
[2022-05-09 15:17:31] [config] embedding-normalization: false
[2022-05-09 15:17:31] [config] embedding-vectors:
[2022-05-09 15:17:31] [config]   []
[2022-05-09 15:17:31] [config] enc-cell: gru
[2022-05-09 15:17:31] [config] enc-cell-depth: 1
[2022-05-09 15:17:31] [config] enc-depth: 6
[2022-05-09 15:17:31] [config] enc-type: bidirectional
[2022-05-09 15:17:31] [config] english-title-case-every: 0
[2022-05-09 15:17:31] [config] exponential-smoothing: 0.0001
[2022-05-09 15:17:31] [config] factor-weight: 1
[2022-05-09 15:17:31] [config] factors-combine: sum
[2022-05-09 15:17:31] [config] factors-dim-emb: 0
[2022-05-09 15:17:31] [config] gradient-checkpointing: false
[2022-05-09 15:17:31] [config] gradient-norm-average-window: 100
[2022-05-09 15:17:31] [config] guided-alignment: none
[2022-05-09 15:17:31] [config] guided-alignment-cost: mse
[2022-05-09 15:17:31] [config] guided-alignment-weight: 0.1
[2022-05-09 15:17:31] [config] ignore-model-config: false
[2022-05-09 15:17:31] [config] input-types:
[2022-05-09 15:17:31] [config]   []
[2022-05-09 15:17:31] [config] interpolate-env-vars: false
[2022-05-09 15:17:31] [config] keep-best: true
[2022-05-09 15:17:31] [config] label-smoothing: 0.1
[2022-05-09 15:17:31] [config] layer-normalization: false
[2022-05-09 15:17:31] [config] learn-rate: 0.0002
[2022-05-09 15:17:31] [config] lemma-dependency: ""
[2022-05-09 15:17:31] [config] lemma-dim-emb: 0
[2022-05-09 15:17:31] [config] log: /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.train1.log
[2022-05-09 15:17:31] [config] log-level: info
[2022-05-09 15:17:31] [config] log-time-zone: ""
[2022-05-09 15:17:31] [config] logical-epoch:
[2022-05-09 15:17:31] [config]   - 1e
[2022-05-09 15:17:31] [config]   - 0
[2022-05-09 15:17:31] [config] lr-decay: 0
[2022-05-09 15:17:31] [config] lr-decay-freq: 50000
[2022-05-09 15:17:31] [config] lr-decay-inv-sqrt:
[2022-05-09 15:17:31] [config]   - 8000
[2022-05-09 15:17:31] [config] lr-decay-repeat-warmup: false
[2022-05-09 15:17:31] [config] lr-decay-reset-optimizer: false
[2022-05-09 15:17:31] [config] lr-decay-start:
[2022-05-09 15:17:31] [config]   - 10
[2022-05-09 15:17:31] [config]   - 1
[2022-05-09 15:17:31] [config] lr-decay-strategy: epoch+stalled
[2022-05-09 15:17:31] [config] lr-report: false
[2022-05-09 15:17:31] [config] lr-warmup: 8000
[2022-05-09 15:17:31] [config] lr-warmup-at-reload: false
[2022-05-09 15:17:31] [config] lr-warmup-cycle: false
[2022-05-09 15:17:31] [config] lr-warmup-start-rate: 0
[2022-05-09 15:17:31] [config] max-length: 100
[2022-05-09 15:17:31] [config] max-length-crop: false
[2022-05-09 15:17:31] [config] max-length-factor: 3
[2022-05-09 15:17:31] [config] maxi-batch: 1000
[2022-05-09 15:17:31] [config] maxi-batch-sort: trg
[2022-05-09 15:17:31] [config] mini-batch: 1000
[2022-05-09 15:17:31] [config] mini-batch-fit: true
[2022-05-09 15:17:31] [config] mini-batch-fit-step: 10
[2022-05-09 15:17:31] [config] mini-batch-round-up: true
[2022-05-09 15:17:31] [config] mini-batch-track-lr: false
[2022-05-09 15:17:31] [config] mini-batch-warmup: 0
[2022-05-09 15:17:31] [config] mini-batch-words: 0
[2022-05-09 15:17:31] [config] mini-batch-words-ref: 0
[2022-05-09 15:17:31] [config] model: /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-09 15:17:31] [config] multi-loss-type: sum
[2022-05-09 15:17:31] [config] n-best: false
[2022-05-09 15:17:31] [config] no-nccl: false
[2022-05-09 15:17:31] [config] no-reload: false
[2022-05-09 15:17:31] [config] no-restore-corpus: false
[2022-05-09 15:17:31] [config] normalize: 1
[2022-05-09 15:17:31] [config] normalize-gradient: false
[2022-05-09 15:17:31] [config] num-devices: 0
[2022-05-09 15:17:31] [config] optimizer: adam
[2022-05-09 15:17:31] [config] optimizer-delay: 2
[2022-05-09 15:17:31] [config] optimizer-params:
[2022-05-09 15:17:31] [config]   - 0.9
[2022-05-09 15:17:31] [config]   - 0.998
[2022-05-09 15:17:31] [config]   - 1e-09
[2022-05-09 15:17:31] [config] output-omit-bias: false
[2022-05-09 15:17:31] [config] overwrite: true
[2022-05-09 15:17:31] [config] precision:
[2022-05-09 15:17:31] [config]   - float32
[2022-05-09 15:17:31] [config]   - float32
[2022-05-09 15:17:31] [config] pretrained-model: ""
[2022-05-09 15:17:31] [config] quantize-biases: false
[2022-05-09 15:17:31] [config] quantize-bits: 0
[2022-05-09 15:17:31] [config] quantize-log-based: false
[2022-05-09 15:17:31] [config] quantize-optimization-steps: 0
[2022-05-09 15:17:31] [config] quiet: false
[2022-05-09 15:17:31] [config] quiet-translation: false
[2022-05-09 15:17:31] [config] relative-paths: false
[2022-05-09 15:17:31] [config] right-left: false
[2022-05-09 15:17:31] [config] save-freq: 10000
[2022-05-09 15:17:31] [config] seed: 1111
[2022-05-09 15:17:31] [config] sentencepiece-alphas:
[2022-05-09 15:17:31] [config]   []
[2022-05-09 15:17:31] [config] sentencepiece-max-lines: 2000000
[2022-05-09 15:17:31] [config] sentencepiece-options: ""
[2022-05-09 15:17:31] [config] sharding: local
[2022-05-09 15:17:31] [config] shuffle: batches
[2022-05-09 15:17:31] [config] shuffle-in-ram: false
[2022-05-09 15:17:31] [config] sigterm: save-and-exit
[2022-05-09 15:17:31] [config] skip: false
[2022-05-09 15:17:31] [config] sqlite: ""
[2022-05-09 15:17:31] [config] sqlite-drop: false
[2022-05-09 15:17:31] [config] sync-freq: 200u
[2022-05-09 15:17:31] [config] sync-sgd: true
[2022-05-09 15:17:31] [config] tempdir: /run/nvme/job_11668422/tmp
[2022-05-09 15:17:31] [config] tied-embeddings: false
[2022-05-09 15:17:31] [config] tied-embeddings-all: true
[2022-05-09 15:17:31] [config] tied-embeddings-src: false
[2022-05-09 15:17:31] [config] train-embedder-rank:
[2022-05-09 15:17:31] [config]   []
[2022-05-09 15:17:31] [config] train-sets:
[2022-05-09 15:17:31] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/train/opusTCv20210807+bt.src.clean.spm32k.gz
[2022-05-09 15:17:31] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/train/opusTCv20210807+bt.trg.clean.spm32k.gz
[2022-05-09 15:17:31] [config] transformer-aan-activation: swish
[2022-05-09 15:17:31] [config] transformer-aan-depth: 2
[2022-05-09 15:17:31] [config] transformer-aan-nogate: false
[2022-05-09 15:17:31] [config] transformer-decoder-autoreg: self-attention
[2022-05-09 15:17:31] [config] transformer-depth-scaling: false
[2022-05-09 15:17:31] [config] transformer-dim-aan: 2048
[2022-05-09 15:17:31] [config] transformer-dim-ffn: 4096
[2022-05-09 15:17:31] [config] transformer-dropout: 0.1
[2022-05-09 15:17:31] [config] transformer-dropout-attention: 0
[2022-05-09 15:17:31] [config] transformer-dropout-ffn: 0
[2022-05-09 15:17:31] [config] transformer-ffn-activation: relu
[2022-05-09 15:17:31] [config] transformer-ffn-depth: 2
[2022-05-09 15:17:31] [config] transformer-guided-alignment-layer: last
[2022-05-09 15:17:31] [config] transformer-heads: 16
[2022-05-09 15:17:31] [config] transformer-no-projection: false
[2022-05-09 15:17:31] [config] transformer-pool: false
[2022-05-09 15:17:31] [config] transformer-postprocess: dan
[2022-05-09 15:17:31] [config] transformer-postprocess-emb: d
[2022-05-09 15:17:31] [config] transformer-postprocess-top: ""
[2022-05-09 15:17:31] [config] transformer-preprocess: ""
[2022-05-09 15:17:31] [config] transformer-tied-layers:
[2022-05-09 15:17:31] [config]   []
[2022-05-09 15:17:31] [config] transformer-train-position-embeddings: false
[2022-05-09 15:17:31] [config] tsv: false
[2022-05-09 15:17:31] [config] tsv-fields: 0
[2022-05-09 15:17:31] [config] type: transformer
[2022-05-09 15:17:31] [config] ulr: false
[2022-05-09 15:17:31] [config] ulr-dim-emb: 0
[2022-05-09 15:17:31] [config] ulr-dropout: 0
[2022-05-09 15:17:31] [config] ulr-keys-vectors: ""
[2022-05-09 15:17:31] [config] ulr-query-vectors: ""
[2022-05-09 15:17:31] [config] ulr-softmax-temperature: 1
[2022-05-09 15:17:31] [config] ulr-trainable-transformation: false
[2022-05-09 15:17:31] [config] unlikelihood-loss: false
[2022-05-09 15:17:31] [config] valid-freq: 10000
[2022-05-09 15:17:31] [config] valid-log: /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.valid1.log
[2022-05-09 15:17:31] [config] valid-max-length: 100
[2022-05-09 15:17:31] [config] valid-metrics:
[2022-05-09 15:17:31] [config]   - perplexity
[2022-05-09 15:17:31] [config] valid-mini-batch: 16
[2022-05-09 15:17:31] [config] valid-reset-stalled: false
[2022-05-09 15:17:31] [config] valid-script-args:
[2022-05-09 15:17:31] [config]   []
[2022-05-09 15:17:31] [config] valid-script-path: ""
[2022-05-09 15:17:31] [config] valid-sets:
[2022-05-09 15:17:31] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/val/Tatoeba-dev-v2021-08-07.src.spm32k
[2022-05-09 15:17:31] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/val/Tatoeba-dev-v2021-08-07.trg.spm32k
[2022-05-09 15:17:31] [config] valid-translation-output: ""
[2022-05-09 15:17:31] [config] version: v1.10.24; 12a1bfa 2021-10-11 16:59:52 +0100
[2022-05-09 15:17:31] [config] vocabs:
[2022-05-09 15:17:31] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-05-09 15:17:31] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-05-09 15:17:31] [config] word-penalty: 0
[2022-05-09 15:17:31] [config] word-scores: false
[2022-05-09 15:17:31] [config] workspace: 15000
[2022-05-09 15:17:31] [config] Loaded model has been created with Marian v1.10.24; 12a1bfa 2021-10-11 16:59:52 +0100
[2022-05-09 15:17:31] Using synchronous SGD
[2022-05-09 15:17:31] [comm] Compiled without MPI support. Running as a single process on r18g06.bullx
[2022-05-09 15:17:31] Synced seed 1111
[2022-05-09 15:17:31] [data] Loading vocabulary from JSON/Yaml file /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-05-09 15:17:32] [data] Setting vocabulary size for input 0 to 58,791
[2022-05-09 15:17:32] [data] Loading vocabulary from JSON/Yaml file /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-05-09 15:17:32] [data] Setting vocabulary size for input 1 to 58,791
[2022-05-09 15:17:32] [batching] Collecting statistics for batch fitting with step size 10
[2022-05-09 15:17:33] [memory] Extending reserved space to 15104 MB (device gpu0)
[2022-05-09 15:17:35] [memory] Extending reserved space to 15104 MB (device gpu1)
[2022-05-09 15:17:35] [comm] Using NCCL 2.8.3 for GPU communication
[2022-05-09 15:17:35] [comm] Using global sharding
[2022-05-09 15:17:37] [comm] NCCLCommunicators constructed successfully
[2022-05-09 15:17:37] [training] Using 2 GPUs
[2022-05-09 15:17:37] [logits] Applying loss function for 1 factor(s)
[2022-05-09 15:17:37] [memory] Reserving 902 MB, device gpu0
[2022-05-09 15:17:40] [gpu] 16-bit TensorCores enabled for float32 matrix operations
[2022-05-09 15:17:40] [memory] Reserving 902 MB, device gpu0
[2022-05-09 15:18:10] [batching] Done. Typical MB size is 27,120 target words
[2022-05-09 15:18:10] [memory] Extending reserved space to 15104 MB (device gpu0)
[2022-05-09 15:18:10] [memory] Extending reserved space to 15104 MB (device gpu1)
[2022-05-09 15:18:10] [comm] Using NCCL 2.8.3 for GPU communication
[2022-05-09 15:18:10] [comm] Using global sharding
[2022-05-09 15:18:12] [comm] NCCLCommunicators constructed successfully
[2022-05-09 15:18:12] [training] Using 2 GPUs
[2022-05-09 15:18:12] Loading model from /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-09 15:18:15] Loading model from /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-09 15:18:22] Allocating memory for general optimizer shards
[2022-05-09 15:18:22] [memory] Reserving 451 MB, device gpu0
[2022-05-09 15:18:22] [memory] Reserving 451 MB, device gpu1
[2022-05-09 15:18:23] Loading Adam parameters
[2022-05-09 15:18:23] [memory] Reserving 902 MB, device gpu0
[2022-05-09 15:18:24] [memory] Reserving 902 MB, device gpu1
[2022-05-09 15:18:25] [memory] Reserving 902 MB, device gpu0
[2022-05-09 15:18:25] [memory] Reserving 902 MB, device gpu1
[2022-05-09 15:18:25] [training] Master parameters and optimizers restored from training checkpoint /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-09 15:18:25] [data] Restoring the corpus state to epoch 18, batch 1010000
[2022-05-09 15:30:27] Training started
[2022-05-09 15:30:27] [training] Batches are processed as 1 process(es) x 2 devices/process
[2022-05-09 15:30:28] [memory] Reserving 902 MB, device gpu0
[2022-05-09 15:30:28] [memory] Reserving 902 MB, device gpu1
[2022-05-09 15:30:31] Parameter type float32, optimization type float32, casting types false
[2022-05-09 19:20:04] Ep. 18 : Up. 1020000 : Sen. 69,378,335 : Cost 2.90121961 : Time 14513.55s : 12426.47 words/s : gNorm 0.8792
[2022-05-09 19:20:04] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-09 19:20:08] Saving Adam parameters
[2022-05-09 19:20:12] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-09 19:20:23] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-05-09 19:20:25] [valid] Ep. 18 : Up. 1020000 : perplexity : 12.2289 : new best
[2022-05-09 19:26:01] Seen 69,730,539 samples
[2022-05-09 19:26:01] Starting data epoch 19 in logical epoch 19
[2022-05-09 23:08:46] Ep. 19 : Up. 1030000 : Sen. 14,107,586 : Cost 2.91638470 : Time 13721.85s : 13032.09 words/s : gNorm 1.0715
[2022-05-09 23:08:46] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-09 23:08:50] Saving Adam parameters
[2022-05-09 23:08:53] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-09 23:09:04] [valid] Ep. 19 : Up. 1030000 : perplexity : 12.2824 : stalled 1 times (last best: 12.2289)
[2022-05-10 02:57:46] Ep. 19 : Up. 1040000 : Sen. 28,487,958 : Cost 2.91113186 : Time 13740.33s : 13047.73 words/s : gNorm 1.0080
[2022-05-10 02:57:46] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-10 02:57:50] Saving Adam parameters
[2022-05-10 02:57:54] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-10 02:58:06] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-05-10 02:58:09] [valid] Ep. 19 : Up. 1040000 : perplexity : 12.2015 : new best
[2022-05-10 06:47:48] Ep. 19 : Up. 1050000 : Sen. 42,596,537 : Cost 2.90122724 : Time 13801.94s : 13060.43 words/s : gNorm 0.9997
[2022-05-10 06:47:48] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-10 06:47:52] Saving Adam parameters
[2022-05-10 06:47:55] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-10 06:48:05] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-05-10 06:48:07] [valid] Ep. 19 : Up. 1050000 : perplexity : 12.1874 : new best
[2022-05-10 10:37:35] Ep. 19 : Up. 1060000 : Sen. 56,732,644 : Cost 2.89913797 : Time 13786.52s : 13066.35 words/s : gNorm 0.8947
[2022-05-10 10:37:36] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-10 10:37:40] Saving Adam parameters
[2022-05-10 10:37:43] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-10 10:37:56] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-05-10 10:37:58] [valid] Ep. 19 : Up. 1060000 : perplexity : 12.1325 : new best
[2022-05-10 14:08:22] Seen 69,730,539 samples
[2022-05-10 14:08:22] Starting data epoch 20 in logical epoch 20
[2022-05-10 14:27:38] Ep. 20 : Up. 1070000 : Sen. 1,201,000 : Cost 2.89732170 : Time 13802.37s : 13057.99 words/s : gNorm 0.9882
[2022-05-10 14:27:38] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-10 14:27:42] Saving Adam parameters
[2022-05-10 14:27:45] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-10 14:27:59] [valid] Ep. 20 : Up. 1070000 : perplexity : 12.1346 : stalled 1 times (last best: 12.1325)
[2022-05-10 18:15:46] Ep. 20 : Up. 1080000 : Sen. 15,641,457 : Cost 2.91151094 : Time 13687.90s : 13059.34 words/s : gNorm 0.8678
[2022-05-10 18:15:46] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-10 18:15:50] Saving Adam parameters
[2022-05-10 18:15:54] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-10 18:16:06] [valid] Ep. 20 : Up. 1080000 : perplexity : 12.2674 : stalled 2 times (last best: 12.1325)
[2022-05-10 22:22:08] Ep. 20 : Up. 1090000 : Sen. 30,000,000 : Cost 2.90648818 : Time 14781.35s : 12141.42 words/s : gNorm 0.9287
[2022-05-10 22:22:08] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-10 22:22:12] Saving Adam parameters
[2022-05-10 22:22:15] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-10 22:22:26] [valid] Ep. 20 : Up. 1090000 : perplexity : 12.1575 : stalled 3 times (last best: 12.1325)
[2022-05-11 02:12:43] Ep. 20 : Up. 1100000 : Sen. 44,108,911 : Cost 2.89672256 : Time 13835.50s : 13027.86 words/s : gNorm 0.9848
[2022-05-11 02:12:43] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-11 02:12:48] Saving Adam parameters
[2022-05-11 02:12:51] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-11 02:13:03] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-05-11 02:13:05] [valid] Ep. 20 : Up. 1100000 : perplexity : 12.0702 : new best
[2022-05-11 06:02:56] Ep. 20 : Up. 1110000 : Sen. 58,257,336 : Cost 2.89417076 : Time 13813.02s : 13051.57 words/s : gNorm 0.9794
[2022-05-11 06:02:56] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-11 06:03:00] Saving Adam parameters
[2022-05-11 06:03:03] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-11 06:03:15] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-05-11 06:03:18] [valid] Ep. 20 : Up. 1110000 : perplexity : 12.0272 : new best
[2022-05-11 09:09:20] Seen 69,730,539 samples
[2022-05-11 09:09:20] Starting data epoch 21 in logical epoch 21
[2022-05-11 09:53:10] Ep. 21 : Up. 1120000 : Sen. 2,776,841 : Cost 2.89599085 : Time 13813.62s : 13030.92 words/s : gNorm 0.9722
[2022-05-11 09:53:10] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-11 09:53:15] Saving Adam parameters
[2022-05-11 09:53:19] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-11 09:53:33] [valid] Ep. 21 : Up. 1120000 : perplexity : 12.079 : stalled 1 times (last best: 12.0272)
[2022-05-11 13:42:16] Ep. 21 : Up. 1130000 : Sen. 17,216,801 : Cost 2.90670776 : Time 13745.36s : 13005.70 words/s : gNorm 0.9679
[2022-05-11 13:42:16] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-11 13:42:19] Saving Adam parameters
[2022-05-11 13:42:22] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-11 13:42:34] [valid] Ep. 21 : Up. 1130000 : perplexity : 12.1564 : stalled 2 times (last best: 12.0272)
[2022-05-11 17:32:14] Ep. 21 : Up. 1140000 : Sen. 31,520,169 : Cost 2.90007782 : Time 13798.50s : 13011.98 words/s : gNorm 0.9728
[2022-05-11 17:32:14] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-11 17:32:24] Saving Adam parameters
[2022-05-11 17:32:29] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-11 17:32:43] [valid] Ep. 21 : Up. 1140000 : perplexity : 12.0665 : stalled 3 times (last best: 12.0272)
[2022-05-11 21:23:33] Ep. 21 : Up. 1150000 : Sen. 45,617,274 : Cost 2.89266944 : Time 13878.67s : 12987.95 words/s : gNorm 0.8895
[2022-05-11 21:23:33] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-11 21:23:45] Saving Adam parameters
[2022-05-11 21:23:49] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-11 21:24:14] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-05-11 21:24:19] [valid] Ep. 21 : Up. 1150000 : perplexity : 11.9876 : new best
[2022-05-12 01:15:54] Ep. 21 : Up. 1160000 : Sen. 59,794,611 : Cost 2.89042997 : Time 13940.37s : 12927.30 words/s : gNorm 1.0040
[2022-05-12 01:15:54] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-12 01:16:04] Saving Adam parameters
[2022-05-12 01:16:08] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-12 01:16:26] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-05-12 01:16:30] [valid] Ep. 21 : Up. 1160000 : perplexity : 11.9577 : new best
[2022-05-12 03:58:34] Seen 69,730,539 samples
[2022-05-12 03:58:34] Starting data epoch 22 in logical epoch 22
[2022-05-12 05:06:59] Ep. 22 : Up. 1170000 : Sen. 4,336,280 : Cost 2.89274120 : Time 13865.27s : 12960.68 words/s : gNorm 1.1211
[2022-05-12 05:06:59] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-12 05:07:03] Saving Adam parameters
[2022-05-12 05:07:06] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-12 05:07:22] [valid] Ep. 22 : Up. 1170000 : perplexity : 12.049 : stalled 1 times (last best: 11.9577)
[2022-05-12 08:56:26] Ep. 22 : Up. 1180000 : Sen. 18,787,317 : Cost 2.90331149 : Time 13766.69s : 13009.91 words/s : gNorm 0.9641
[2022-05-12 08:56:26] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-12 08:56:30] Saving Adam parameters
[2022-05-12 08:56:34] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-12 08:56:45] [valid] Ep. 22 : Up. 1180000 : perplexity : 12.0956 : stalled 2 times (last best: 11.9577)
[2022-05-12 12:46:51] Ep. 22 : Up. 1190000 : Sen. 33,039,596 : Cost 2.89470935 : Time 13824.70s : 12990.47 words/s : gNorm 1.0078
[2022-05-12 12:46:51] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-12 12:46:55] Saving Adam parameters
[2022-05-12 12:46:58] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-12 12:47:09] [valid] Ep. 22 : Up. 1190000 : perplexity : 12.1357 : stalled 3 times (last best: 11.9577)
[2022-05-12 15:18:00] [marian] Marian v1.10.24; 12a1bfa 2021-10-11 16:59:52 +0100
[2022-05-12 15:18:00] [marian] Running on r18g06.bullx as process 32864 with command line:
[2022-05-12 15:18:00] [marian] /projappl/project_2001194/marian-dev/build/marian --task transformer-big --optimizer-delay 2 --early-stopping 10 --valid-freq 10000 --valid-sets /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/val/Tatoeba-dev-v2021-08-07.src.spm32k /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/val/Tatoeba-dev-v2021-08-07.trg.spm32k --valid-metrics perplexity --valid-mini-batch 16 --valid-max-length 100 --valid-log /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.valid1.log --beam-size 6 --normalize 1 --allow-unk --workspace 15000 --model /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz --train-sets /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/train/opusTCv20210807+bt.src.clean.spm32k.gz /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/train/opusTCv20210807+bt.trg.clean.spm32k.gz --vocabs /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.vocab.yml /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.vocab.yml --save-freq 10000 --disp-freq 10000 --log /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.train1.log --devices 0 1 --seed 1111 --tempdir /run/nvme/job_11688053/tmp --shuffle batches --sharding local --overwrite --keep-best
[2022-05-12 15:18:02] [config] after: 0e
[2022-05-12 15:18:02] [config] after-batches: 0
[2022-05-12 15:18:02] [config] after-epochs: 0
[2022-05-12 15:18:02] [config] all-caps-every: 0
[2022-05-12 15:18:02] [config] allow-unk: true
[2022-05-12 15:18:02] [config] authors: false
[2022-05-12 15:18:02] [config] beam-size: 6
[2022-05-12 15:18:02] [config] bert-class-symbol: "[CLS]"
[2022-05-12 15:18:02] [config] bert-mask-symbol: "[MASK]"
[2022-05-12 15:18:02] [config] bert-masking-fraction: 0.15
[2022-05-12 15:18:02] [config] bert-sep-symbol: "[SEP]"
[2022-05-12 15:18:02] [config] bert-train-type-embeddings: true
[2022-05-12 15:18:02] [config] bert-type-vocab-size: 2
[2022-05-12 15:18:02] [config] build-info: ""
[2022-05-12 15:18:02] [config] check-gradient-nan: false
[2022-05-12 15:18:02] [config] check-nan: false
[2022-05-12 15:18:02] [config] cite: false
[2022-05-12 15:18:02] [config] clip-norm: 0
[2022-05-12 15:18:02] [config] cost-scaling:
[2022-05-12 15:18:02] [config]   []
[2022-05-12 15:18:02] [config] cost-type: ce-mean-words
[2022-05-12 15:18:02] [config] cpu-threads: 0
[2022-05-12 15:18:02] [config] data-weighting: ""
[2022-05-12 15:18:02] [config] data-weighting-type: sentence
[2022-05-12 15:18:02] [config] dec-cell: gru
[2022-05-12 15:18:02] [config] dec-cell-base-depth: 2
[2022-05-12 15:18:02] [config] dec-cell-high-depth: 1
[2022-05-12 15:18:02] [config] dec-depth: 6
[2022-05-12 15:18:02] [config] devices:
[2022-05-12 15:18:02] [config]   - 0
[2022-05-12 15:18:02] [config]   - 1
[2022-05-12 15:18:02] [config] dim-emb: 1024
[2022-05-12 15:18:02] [config] dim-rnn: 1024
[2022-05-12 15:18:02] [config] dim-vocabs:
[2022-05-12 15:18:02] [config]   - 58791
[2022-05-12 15:18:02] [config]   - 58791
[2022-05-12 15:18:02] [config] disp-first: 0
[2022-05-12 15:18:02] [config] disp-freq: 10000
[2022-05-12 15:18:02] [config] disp-label-counts: true
[2022-05-12 15:18:02] [config] dropout-rnn: 0
[2022-05-12 15:18:02] [config] dropout-src: 0
[2022-05-12 15:18:02] [config] dropout-trg: 0
[2022-05-12 15:18:02] [config] dump-config: ""
[2022-05-12 15:18:02] [config] dynamic-gradient-scaling:
[2022-05-12 15:18:02] [config]   []
[2022-05-12 15:18:02] [config] early-stopping: 10
[2022-05-12 15:18:02] [config] early-stopping-on: first
[2022-05-12 15:18:02] [config] embedding-fix-src: false
[2022-05-12 15:18:02] [config] embedding-fix-trg: false
[2022-05-12 15:18:02] [config] embedding-normalization: false
[2022-05-12 15:18:02] [config] embedding-vectors:
[2022-05-12 15:18:02] [config]   []
[2022-05-12 15:18:02] [config] enc-cell: gru
[2022-05-12 15:18:02] [config] enc-cell-depth: 1
[2022-05-12 15:18:02] [config] enc-depth: 6
[2022-05-12 15:18:02] [config] enc-type: bidirectional
[2022-05-12 15:18:02] [config] english-title-case-every: 0
[2022-05-12 15:18:02] [config] exponential-smoothing: 0.0001
[2022-05-12 15:18:02] [config] factor-weight: 1
[2022-05-12 15:18:02] [config] factors-combine: sum
[2022-05-12 15:18:02] [config] factors-dim-emb: 0
[2022-05-12 15:18:02] [config] gradient-checkpointing: false
[2022-05-12 15:18:02] [config] gradient-norm-average-window: 100
[2022-05-12 15:18:02] [config] guided-alignment: none
[2022-05-12 15:18:02] [config] guided-alignment-cost: mse
[2022-05-12 15:18:02] [config] guided-alignment-weight: 0.1
[2022-05-12 15:18:02] [config] ignore-model-config: false
[2022-05-12 15:18:02] [config] input-types:
[2022-05-12 15:18:02] [config]   []
[2022-05-12 15:18:02] [config] interpolate-env-vars: false
[2022-05-12 15:18:02] [config] keep-best: true
[2022-05-12 15:18:02] [config] label-smoothing: 0.1
[2022-05-12 15:18:02] [config] layer-normalization: false
[2022-05-12 15:18:02] [config] learn-rate: 0.0002
[2022-05-12 15:18:02] [config] lemma-dependency: ""
[2022-05-12 15:18:02] [config] lemma-dim-emb: 0
[2022-05-12 15:18:02] [config] log: /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.train1.log
[2022-05-12 15:18:02] [config] log-level: info
[2022-05-12 15:18:02] [config] log-time-zone: ""
[2022-05-12 15:18:02] [config] logical-epoch:
[2022-05-12 15:18:02] [config]   - 1e
[2022-05-12 15:18:02] [config]   - 0
[2022-05-12 15:18:02] [config] lr-decay: 0
[2022-05-12 15:18:02] [config] lr-decay-freq: 50000
[2022-05-12 15:18:02] [config] lr-decay-inv-sqrt:
[2022-05-12 15:18:02] [config]   - 8000
[2022-05-12 15:18:02] [config] lr-decay-repeat-warmup: false
[2022-05-12 15:18:02] [config] lr-decay-reset-optimizer: false
[2022-05-12 15:18:02] [config] lr-decay-start:
[2022-05-12 15:18:02] [config]   - 10
[2022-05-12 15:18:02] [config]   - 1
[2022-05-12 15:18:02] [config] lr-decay-strategy: epoch+stalled
[2022-05-12 15:18:02] [config] lr-report: false
[2022-05-12 15:18:02] [config] lr-warmup: 8000
[2022-05-12 15:18:02] [config] lr-warmup-at-reload: false
[2022-05-12 15:18:02] [config] lr-warmup-cycle: false
[2022-05-12 15:18:02] [config] lr-warmup-start-rate: 0
[2022-05-12 15:18:02] [config] max-length: 100
[2022-05-12 15:18:02] [config] max-length-crop: false
[2022-05-12 15:18:02] [config] max-length-factor: 3
[2022-05-12 15:18:02] [config] maxi-batch: 1000
[2022-05-12 15:18:02] [config] maxi-batch-sort: trg
[2022-05-12 15:18:02] [config] mini-batch: 1000
[2022-05-12 15:18:02] [config] mini-batch-fit: true
[2022-05-12 15:18:02] [config] mini-batch-fit-step: 10
[2022-05-12 15:18:02] [config] mini-batch-round-up: true
[2022-05-12 15:18:02] [config] mini-batch-track-lr: false
[2022-05-12 15:18:02] [config] mini-batch-warmup: 0
[2022-05-12 15:18:02] [config] mini-batch-words: 0
[2022-05-12 15:18:02] [config] mini-batch-words-ref: 0
[2022-05-12 15:18:02] [config] model: /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-12 15:18:02] [config] multi-loss-type: sum
[2022-05-12 15:18:02] [config] n-best: false
[2022-05-12 15:18:02] [config] no-nccl: false
[2022-05-12 15:18:02] [config] no-reload: false
[2022-05-12 15:18:02] [config] no-restore-corpus: false
[2022-05-12 15:18:02] [config] normalize: 1
[2022-05-12 15:18:02] [config] normalize-gradient: false
[2022-05-12 15:18:02] [config] num-devices: 0
[2022-05-12 15:18:02] [config] optimizer: adam
[2022-05-12 15:18:02] [config] optimizer-delay: 2
[2022-05-12 15:18:02] [config] optimizer-params:
[2022-05-12 15:18:02] [config]   - 0.9
[2022-05-12 15:18:02] [config]   - 0.998
[2022-05-12 15:18:02] [config]   - 1e-09
[2022-05-12 15:18:02] [config] output-omit-bias: false
[2022-05-12 15:18:02] [config] overwrite: true
[2022-05-12 15:18:02] [config] precision:
[2022-05-12 15:18:02] [config]   - float32
[2022-05-12 15:18:02] [config]   - float32
[2022-05-12 15:18:02] [config] pretrained-model: ""
[2022-05-12 15:18:02] [config] quantize-biases: false
[2022-05-12 15:18:02] [config] quantize-bits: 0
[2022-05-12 15:18:02] [config] quantize-log-based: false
[2022-05-12 15:18:02] [config] quantize-optimization-steps: 0
[2022-05-12 15:18:02] [config] quiet: false
[2022-05-12 15:18:02] [config] quiet-translation: false
[2022-05-12 15:18:02] [config] relative-paths: false
[2022-05-12 15:18:02] [config] right-left: false
[2022-05-12 15:18:02] [config] save-freq: 10000
[2022-05-12 15:18:02] [config] seed: 1111
[2022-05-12 15:18:02] [config] sentencepiece-alphas:
[2022-05-12 15:18:02] [config]   []
[2022-05-12 15:18:02] [config] sentencepiece-max-lines: 2000000
[2022-05-12 15:18:02] [config] sentencepiece-options: ""
[2022-05-12 15:18:02] [config] sharding: local
[2022-05-12 15:18:02] [config] shuffle: batches
[2022-05-12 15:18:02] [config] shuffle-in-ram: false
[2022-05-12 15:18:02] [config] sigterm: save-and-exit
[2022-05-12 15:18:02] [config] skip: false
[2022-05-12 15:18:02] [config] sqlite: ""
[2022-05-12 15:18:02] [config] sqlite-drop: false
[2022-05-12 15:18:02] [config] sync-freq: 200u
[2022-05-12 15:18:02] [config] sync-sgd: true
[2022-05-12 15:18:02] [config] tempdir: /run/nvme/job_11688053/tmp
[2022-05-12 15:18:02] [config] tied-embeddings: false
[2022-05-12 15:18:02] [config] tied-embeddings-all: true
[2022-05-12 15:18:02] [config] tied-embeddings-src: false
[2022-05-12 15:18:02] [config] train-embedder-rank:
[2022-05-12 15:18:02] [config]   []
[2022-05-12 15:18:02] [config] train-sets:
[2022-05-12 15:18:02] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/train/opusTCv20210807+bt.src.clean.spm32k.gz
[2022-05-12 15:18:02] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/train/opusTCv20210807+bt.trg.clean.spm32k.gz
[2022-05-12 15:18:02] [config] transformer-aan-activation: swish
[2022-05-12 15:18:02] [config] transformer-aan-depth: 2
[2022-05-12 15:18:02] [config] transformer-aan-nogate: false
[2022-05-12 15:18:02] [config] transformer-decoder-autoreg: self-attention
[2022-05-12 15:18:02] [config] transformer-depth-scaling: false
[2022-05-12 15:18:02] [config] transformer-dim-aan: 2048
[2022-05-12 15:18:02] [config] transformer-dim-ffn: 4096
[2022-05-12 15:18:02] [config] transformer-dropout: 0.1
[2022-05-12 15:18:02] [config] transformer-dropout-attention: 0
[2022-05-12 15:18:02] [config] transformer-dropout-ffn: 0
[2022-05-12 15:18:02] [config] transformer-ffn-activation: relu
[2022-05-12 15:18:02] [config] transformer-ffn-depth: 2
[2022-05-12 15:18:02] [config] transformer-guided-alignment-layer: last
[2022-05-12 15:18:02] [config] transformer-heads: 16
[2022-05-12 15:18:02] [config] transformer-no-projection: false
[2022-05-12 15:18:02] [config] transformer-pool: false
[2022-05-12 15:18:02] [config] transformer-postprocess: dan
[2022-05-12 15:18:02] [config] transformer-postprocess-emb: d
[2022-05-12 15:18:02] [config] transformer-postprocess-top: ""
[2022-05-12 15:18:02] [config] transformer-preprocess: ""
[2022-05-12 15:18:02] [config] transformer-tied-layers:
[2022-05-12 15:18:02] [config]   []
[2022-05-12 15:18:02] [config] transformer-train-position-embeddings: false
[2022-05-12 15:18:02] [config] tsv: false
[2022-05-12 15:18:02] [config] tsv-fields: 0
[2022-05-12 15:18:02] [config] type: transformer
[2022-05-12 15:18:02] [config] ulr: false
[2022-05-12 15:18:02] [config] ulr-dim-emb: 0
[2022-05-12 15:18:02] [config] ulr-dropout: 0
[2022-05-12 15:18:02] [config] ulr-keys-vectors: ""
[2022-05-12 15:18:02] [config] ulr-query-vectors: ""
[2022-05-12 15:18:02] [config] ulr-softmax-temperature: 1
[2022-05-12 15:18:02] [config] ulr-trainable-transformation: false
[2022-05-12 15:18:02] [config] unlikelihood-loss: false
[2022-05-12 15:18:02] [config] valid-freq: 10000
[2022-05-12 15:18:02] [config] valid-log: /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.valid1.log
[2022-05-12 15:18:02] [config] valid-max-length: 100
[2022-05-12 15:18:02] [config] valid-metrics:
[2022-05-12 15:18:02] [config]   - perplexity
[2022-05-12 15:18:02] [config] valid-mini-batch: 16
[2022-05-12 15:18:02] [config] valid-reset-stalled: false
[2022-05-12 15:18:02] [config] valid-script-args:
[2022-05-12 15:18:02] [config]   []
[2022-05-12 15:18:02] [config] valid-script-path: ""
[2022-05-12 15:18:02] [config] valid-sets:
[2022-05-12 15:18:02] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/val/Tatoeba-dev-v2021-08-07.src.spm32k
[2022-05-12 15:18:02] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/val/Tatoeba-dev-v2021-08-07.trg.spm32k
[2022-05-12 15:18:02] [config] valid-translation-output: ""
[2022-05-12 15:18:02] [config] version: v1.10.24; 12a1bfa 2021-10-11 16:59:52 +0100
[2022-05-12 15:18:02] [config] vocabs:
[2022-05-12 15:18:02] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-05-12 15:18:02] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-05-12 15:18:02] [config] word-penalty: 0
[2022-05-12 15:18:02] [config] word-scores: false
[2022-05-12 15:18:02] [config] workspace: 15000
[2022-05-12 15:18:02] [config] Loaded model has been created with Marian v1.10.24; 12a1bfa 2021-10-11 16:59:52 +0100
[2022-05-12 15:18:02] Using synchronous SGD
[2022-05-12 15:18:02] [comm] Compiled without MPI support. Running as a single process on r18g06.bullx
[2022-05-12 15:18:02] Synced seed 1111
[2022-05-12 15:18:02] [data] Loading vocabulary from JSON/Yaml file /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-05-12 15:18:03] [data] Setting vocabulary size for input 0 to 58,791
[2022-05-12 15:18:03] [data] Loading vocabulary from JSON/Yaml file /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-05-12 15:18:03] [data] Setting vocabulary size for input 1 to 58,791
[2022-05-12 15:18:03] [batching] Collecting statistics for batch fitting with step size 10
[2022-05-12 15:18:04] [memory] Extending reserved space to 15104 MB (device gpu0)
[2022-05-12 15:18:06] [memory] Extending reserved space to 15104 MB (device gpu1)
[2022-05-12 15:18:06] [comm] Using NCCL 2.8.3 for GPU communication
[2022-05-12 15:18:06] [comm] Using global sharding
[2022-05-12 15:18:07] [comm] NCCLCommunicators constructed successfully
[2022-05-12 15:18:07] [training] Using 2 GPUs
[2022-05-12 15:18:07] [logits] Applying loss function for 1 factor(s)
[2022-05-12 15:18:07] [memory] Reserving 902 MB, device gpu0
[2022-05-12 15:18:09] [gpu] 16-bit TensorCores enabled for float32 matrix operations
[2022-05-12 15:18:09] [memory] Reserving 902 MB, device gpu0
[2022-05-12 15:18:40] [batching] Done. Typical MB size is 27,120 target words
[2022-05-12 15:18:40] [memory] Extending reserved space to 15104 MB (device gpu0)
[2022-05-12 15:18:40] [memory] Extending reserved space to 15104 MB (device gpu1)
[2022-05-12 15:18:40] [comm] Using NCCL 2.8.3 for GPU communication
[2022-05-12 15:18:40] [comm] Using global sharding
[2022-05-12 15:18:41] [comm] NCCLCommunicators constructed successfully
[2022-05-12 15:18:41] [training] Using 2 GPUs
[2022-05-12 15:18:41] Loading model from /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-12 15:18:43] Loading model from /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-12 15:18:52] Allocating memory for general optimizer shards
[2022-05-12 15:18:52] [memory] Reserving 451 MB, device gpu0
[2022-05-12 15:18:52] [memory] Reserving 451 MB, device gpu1
[2022-05-12 15:18:52] Loading Adam parameters
[2022-05-12 15:18:53] [memory] Reserving 902 MB, device gpu0
[2022-05-12 15:18:54] [memory] Reserving 902 MB, device gpu1
[2022-05-12 15:18:55] [memory] Reserving 902 MB, device gpu0
[2022-05-12 15:18:55] [memory] Reserving 902 MB, device gpu1
[2022-05-12 15:18:55] [training] Master parameters and optimizers restored from training checkpoint /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-12 15:18:55] [data] Restoring the corpus state to epoch 22, batch 1190000
[2022-05-12 15:26:34] Training started
[2022-05-12 15:26:34] [training] Batches are processed as 1 process(es) x 2 devices/process
[2022-05-12 15:26:34] [memory] Reserving 902 MB, device gpu0
[2022-05-12 15:26:34] [memory] Reserving 902 MB, device gpu1
[2022-05-12 15:26:36] Parameter type float32, optimization type float32, casting types false
[2022-05-12 19:17:13] Ep. 22 : Up. 1200000 : Sen. 47,143,323 : Cost 2.88897657 : Time 14313.02s : 12594.96 words/s : gNorm 0.9286
[2022-05-12 19:17:13] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-12 19:17:18] Saving Adam parameters
[2022-05-12 19:17:22] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-12 19:17:35] [valid] Ep. 22 : Up. 1200000 : perplexity : 12.0775 : stalled 3 times (last best: 11.9577)
[2022-05-12 23:08:01] Ep. 22 : Up. 1210000 : Sen. 61,295,706 : Cost 2.88544869 : Time 13847.48s : 13018.83 words/s : gNorm 0.9700
[2022-05-12 23:08:01] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-12 23:08:08] Saving Adam parameters
[2022-05-12 23:08:12] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-12 23:08:22] [valid] Ep. 22 : Up. 1210000 : perplexity : 12.0152 : stalled 4 times (last best: 11.9577)
[2022-05-13 01:24:36] Seen 69,730,539 samples
[2022-05-13 01:24:36] Starting data epoch 23 in logical epoch 23
[2022-05-13 02:57:20] Ep. 23 : Up. 1220000 : Sen. 5,879,277 : Cost 2.89103508 : Time 13759.16s : 13055.27 words/s : gNorm 1.1069
[2022-05-13 02:57:20] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-13 02:57:23] Saving Adam parameters
[2022-05-13 02:57:27] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-13 02:57:38] [valid] Ep. 23 : Up. 1220000 : perplexity : 12.1038 : stalled 5 times (last best: 11.9577)
[2022-05-13 06:45:18] Ep. 23 : Up. 1230000 : Sen. 20,334,941 : Cost 2.89922023 : Time 13677.70s : 13070.35 words/s : gNorm 1.0000
[2022-05-13 06:45:18] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-13 06:45:22] Saving Adam parameters
[2022-05-13 06:45:25] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-13 06:45:36] [valid] Ep. 23 : Up. 1230000 : perplexity : 12.0693 : stalled 6 times (last best: 11.9577)
[2022-05-13 10:35:16] Ep. 23 : Up. 1240000 : Sen. 34,566,203 : Cost 2.89050746 : Time 13797.97s : 13038.91 words/s : gNorm 0.9741
[2022-05-13 10:35:16] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-13 10:35:19] Saving Adam parameters
[2022-05-13 10:35:23] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-13 10:35:38] [valid] Ep. 23 : Up. 1240000 : perplexity : 12.0491 : stalled 7 times (last best: 11.9577)
[2022-05-13 14:25:55] Ep. 23 : Up. 1250000 : Sen. 48,670,830 : Cost 2.88508415 : Time 13839.68s : 13029.52 words/s : gNorm 0.9789
[2022-05-13 14:25:55] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-13 14:25:59] Saving Adam parameters
[2022-05-13 14:26:03] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-13 14:26:15] [valid] Ep. 23 : Up. 1250000 : perplexity : 12.0114 : stalled 8 times (last best: 11.9577)
[2022-05-13 18:16:37] Ep. 23 : Up. 1260000 : Sen. 62,829,271 : Cost 2.88207841 : Time 13841.56s : 13014.34 words/s : gNorm 1.0035
[2022-05-13 18:16:37] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-13 18:16:41] Saving Adam parameters
[2022-05-13 18:16:44] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-13 18:16:55] [valid] Ep. 23 : Up. 1260000 : perplexity : 11.9735 : stalled 9 times (last best: 11.9577)
[2022-05-13 20:08:40] Seen 69,730,539 samples
[2022-05-13 20:08:40] Starting data epoch 24 in logical epoch 24
[2022-05-13 22:06:33] Ep. 24 : Up. 1270000 : Sen. 7,420,812 : Cost 2.88893771 : Time 13796.09s : 13012.87 words/s : gNorm 1.0514
[2022-05-13 22:06:33] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-13 22:06:41] Saving Adam parameters
[2022-05-13 22:06:45] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-05-13 22:06:56] [valid] Ep. 24 : Up. 1270000 : perplexity : 12.0946 : stalled 10 times (last best: 11.9577)
[2022-05-13 22:06:56] Training finished
[2022-05-13 22:06:56] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-05-13 22:06:59] Saving Adam parameters
[2022-05-13 22:07:03] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-zho/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
