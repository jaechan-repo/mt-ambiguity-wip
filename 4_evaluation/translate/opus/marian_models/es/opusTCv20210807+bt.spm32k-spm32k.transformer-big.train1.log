[2022-02-15 10:19:31] [marian] Marian v1.10.24; 12a1bfa 2021-10-11 16:59:52 +0100
[2022-02-15 10:19:31] [marian] Running on r02g07.bullx as process 132689 with command line:
[2022-02-15 10:19:31] [marian] /projappl/project_2001194/marian-dev/build/marian --task transformer-big --optimizer-delay 2 --no-restore-corpus --early-stopping 10 --valid-freq 10000 --valid-sets /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/val/Tatoeba-dev-v2021-08-07.src.spm32k /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/val/Tatoeba-dev-v2021-08-07.trg.spm32k --valid-metrics perplexity --valid-mini-batch 16 --valid-max-length 100 --valid-log /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.valid1.log --beam-size 6 --normalize 1 --allow-unk --workspace 15000 --model /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz --train-sets /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/train/opusTCv20210807+bt.src.clean.spm32k.gz /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/train/opusTCv20210807+bt.trg.clean.spm32k.gz --vocabs /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.vocab.yml /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.vocab.yml --save-freq 10000 --disp-freq 10000 --log /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.train1.log --devices 0 1 --seed 1111 --tempdir /run/nvme/job_10594741/tmp --shuffle batches --sharding local --overwrite --keep-best
[2022-02-15 10:19:31] [config] after: 0e
[2022-02-15 10:19:31] [config] after-batches: 0
[2022-02-15 10:19:31] [config] after-epochs: 0
[2022-02-15 10:19:31] [config] all-caps-every: 0
[2022-02-15 10:19:31] [config] allow-unk: true
[2022-02-15 10:19:31] [config] authors: false
[2022-02-15 10:19:31] [config] beam-size: 6
[2022-02-15 10:19:31] [config] bert-class-symbol: "[CLS]"
[2022-02-15 10:19:31] [config] bert-mask-symbol: "[MASK]"
[2022-02-15 10:19:31] [config] bert-masking-fraction: 0.15
[2022-02-15 10:19:31] [config] bert-sep-symbol: "[SEP]"
[2022-02-15 10:19:31] [config] bert-train-type-embeddings: true
[2022-02-15 10:19:31] [config] bert-type-vocab-size: 2
[2022-02-15 10:19:31] [config] build-info: ""
[2022-02-15 10:19:31] [config] check-gradient-nan: false
[2022-02-15 10:19:31] [config] check-nan: false
[2022-02-15 10:19:31] [config] cite: false
[2022-02-15 10:19:31] [config] clip-norm: 0
[2022-02-15 10:19:31] [config] cost-scaling:
[2022-02-15 10:19:31] [config]   []
[2022-02-15 10:19:31] [config] cost-type: ce-mean-words
[2022-02-15 10:19:31] [config] cpu-threads: 0
[2022-02-15 10:19:31] [config] data-weighting: ""
[2022-02-15 10:19:31] [config] data-weighting-type: sentence
[2022-02-15 10:19:31] [config] dec-cell: gru
[2022-02-15 10:19:31] [config] dec-cell-base-depth: 2
[2022-02-15 10:19:31] [config] dec-cell-high-depth: 1
[2022-02-15 10:19:31] [config] dec-depth: 6
[2022-02-15 10:19:31] [config] devices:
[2022-02-15 10:19:31] [config]   - 0
[2022-02-15 10:19:31] [config]   - 1
[2022-02-15 10:19:31] [config] dim-emb: 1024
[2022-02-15 10:19:31] [config] dim-rnn: 1024
[2022-02-15 10:19:31] [config] dim-vocabs:
[2022-02-15 10:19:31] [config]   - 0
[2022-02-15 10:19:31] [config]   - 0
[2022-02-15 10:19:31] [config] disp-first: 0
[2022-02-15 10:19:31] [config] disp-freq: 10000
[2022-02-15 10:19:31] [config] disp-label-counts: true
[2022-02-15 10:19:31] [config] dropout-rnn: 0
[2022-02-15 10:19:31] [config] dropout-src: 0
[2022-02-15 10:19:31] [config] dropout-trg: 0
[2022-02-15 10:19:31] [config] dump-config: ""
[2022-02-15 10:19:31] [config] dynamic-gradient-scaling:
[2022-02-15 10:19:31] [config]   []
[2022-02-15 10:19:31] [config] early-stopping: 10
[2022-02-15 10:19:31] [config] early-stopping-on: first
[2022-02-15 10:19:31] [config] embedding-fix-src: false
[2022-02-15 10:19:31] [config] embedding-fix-trg: false
[2022-02-15 10:19:31] [config] embedding-normalization: false
[2022-02-15 10:19:31] [config] embedding-vectors:
[2022-02-15 10:19:31] [config]   []
[2022-02-15 10:19:31] [config] enc-cell: gru
[2022-02-15 10:19:31] [config] enc-cell-depth: 1
[2022-02-15 10:19:31] [config] enc-depth: 6
[2022-02-15 10:19:31] [config] enc-type: bidirectional
[2022-02-15 10:19:31] [config] english-title-case-every: 0
[2022-02-15 10:19:31] [config] exponential-smoothing: 0.0001
[2022-02-15 10:19:31] [config] factor-weight: 1
[2022-02-15 10:19:31] [config] factors-combine: sum
[2022-02-15 10:19:31] [config] factors-dim-emb: 0
[2022-02-15 10:19:31] [config] gradient-checkpointing: false
[2022-02-15 10:19:31] [config] gradient-norm-average-window: 100
[2022-02-15 10:19:31] [config] guided-alignment: none
[2022-02-15 10:19:31] [config] guided-alignment-cost: mse
[2022-02-15 10:19:31] [config] guided-alignment-weight: 0.1
[2022-02-15 10:19:31] [config] ignore-model-config: false
[2022-02-15 10:19:31] [config] input-types:
[2022-02-15 10:19:31] [config]   []
[2022-02-15 10:19:31] [config] interpolate-env-vars: false
[2022-02-15 10:19:31] [config] keep-best: true
[2022-02-15 10:19:31] [config] label-smoothing: 0.1
[2022-02-15 10:19:31] [config] layer-normalization: false
[2022-02-15 10:19:31] [config] learn-rate: 0.0002
[2022-02-15 10:19:31] [config] lemma-dependency: ""
[2022-02-15 10:19:31] [config] lemma-dim-emb: 0
[2022-02-15 10:19:31] [config] log: /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.train1.log
[2022-02-15 10:19:31] [config] log-level: info
[2022-02-15 10:19:31] [config] log-time-zone: ""
[2022-02-15 10:19:31] [config] logical-epoch:
[2022-02-15 10:19:31] [config]   - 1e
[2022-02-15 10:19:31] [config]   - 0
[2022-02-15 10:19:31] [config] lr-decay: 0
[2022-02-15 10:19:31] [config] lr-decay-freq: 50000
[2022-02-15 10:19:31] [config] lr-decay-inv-sqrt:
[2022-02-15 10:19:31] [config]   - 8000
[2022-02-15 10:19:31] [config] lr-decay-repeat-warmup: false
[2022-02-15 10:19:31] [config] lr-decay-reset-optimizer: false
[2022-02-15 10:19:31] [config] lr-decay-start:
[2022-02-15 10:19:31] [config]   - 10
[2022-02-15 10:19:31] [config]   - 1
[2022-02-15 10:19:31] [config] lr-decay-strategy: epoch+stalled
[2022-02-15 10:19:31] [config] lr-report: false
[2022-02-15 10:19:31] [config] lr-warmup: 8000
[2022-02-15 10:19:31] [config] lr-warmup-at-reload: false
[2022-02-15 10:19:31] [config] lr-warmup-cycle: false
[2022-02-15 10:19:31] [config] lr-warmup-start-rate: 0
[2022-02-15 10:19:31] [config] max-length: 100
[2022-02-15 10:19:31] [config] max-length-crop: false
[2022-02-15 10:19:31] [config] max-length-factor: 3
[2022-02-15 10:19:31] [config] maxi-batch: 1000
[2022-02-15 10:19:31] [config] maxi-batch-sort: trg
[2022-02-15 10:19:31] [config] mini-batch: 1000
[2022-02-15 10:19:31] [config] mini-batch-fit: true
[2022-02-15 10:19:31] [config] mini-batch-fit-step: 10
[2022-02-15 10:19:31] [config] mini-batch-round-up: true
[2022-02-15 10:19:31] [config] mini-batch-track-lr: false
[2022-02-15 10:19:31] [config] mini-batch-warmup: 0
[2022-02-15 10:19:31] [config] mini-batch-words: 0
[2022-02-15 10:19:31] [config] mini-batch-words-ref: 0
[2022-02-15 10:19:31] [config] model: /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-15 10:19:31] [config] multi-loss-type: sum
[2022-02-15 10:19:31] [config] n-best: false
[2022-02-15 10:19:31] [config] no-nccl: false
[2022-02-15 10:19:31] [config] no-reload: false
[2022-02-15 10:19:31] [config] no-restore-corpus: true
[2022-02-15 10:19:31] [config] normalize: 1
[2022-02-15 10:19:31] [config] normalize-gradient: false
[2022-02-15 10:19:31] [config] num-devices: 0
[2022-02-15 10:19:31] [config] optimizer: adam
[2022-02-15 10:19:31] [config] optimizer-delay: 2
[2022-02-15 10:19:31] [config] optimizer-params:
[2022-02-15 10:19:31] [config]   - 0.9
[2022-02-15 10:19:31] [config]   - 0.998
[2022-02-15 10:19:31] [config]   - 1e-09
[2022-02-15 10:19:31] [config] output-omit-bias: false
[2022-02-15 10:19:31] [config] overwrite: true
[2022-02-15 10:19:31] [config] precision:
[2022-02-15 10:19:31] [config]   - float32
[2022-02-15 10:19:31] [config]   - float32
[2022-02-15 10:19:31] [config] pretrained-model: ""
[2022-02-15 10:19:31] [config] quantize-biases: false
[2022-02-15 10:19:31] [config] quantize-bits: 0
[2022-02-15 10:19:31] [config] quantize-log-based: false
[2022-02-15 10:19:31] [config] quantize-optimization-steps: 0
[2022-02-15 10:19:31] [config] quiet: false
[2022-02-15 10:19:31] [config] quiet-translation: false
[2022-02-15 10:19:31] [config] relative-paths: false
[2022-02-15 10:19:31] [config] right-left: false
[2022-02-15 10:19:31] [config] save-freq: 10000
[2022-02-15 10:19:31] [config] seed: 1111
[2022-02-15 10:19:31] [config] sentencepiece-alphas:
[2022-02-15 10:19:31] [config]   []
[2022-02-15 10:19:31] [config] sentencepiece-max-lines: 2000000
[2022-02-15 10:19:31] [config] sentencepiece-options: ""
[2022-02-15 10:19:31] [config] sharding: local
[2022-02-15 10:19:31] [config] shuffle: batches
[2022-02-15 10:19:31] [config] shuffle-in-ram: false
[2022-02-15 10:19:31] [config] sigterm: save-and-exit
[2022-02-15 10:19:31] [config] skip: false
[2022-02-15 10:19:31] [config] sqlite: ""
[2022-02-15 10:19:31] [config] sqlite-drop: false
[2022-02-15 10:19:31] [config] sync-freq: 200u
[2022-02-15 10:19:31] [config] sync-sgd: true
[2022-02-15 10:19:31] [config] tempdir: /run/nvme/job_10594741/tmp
[2022-02-15 10:19:31] [config] tied-embeddings: false
[2022-02-15 10:19:31] [config] tied-embeddings-all: true
[2022-02-15 10:19:31] [config] tied-embeddings-src: false
[2022-02-15 10:19:31] [config] train-embedder-rank:
[2022-02-15 10:19:31] [config]   []
[2022-02-15 10:19:31] [config] train-sets:
[2022-02-15 10:19:31] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/train/opusTCv20210807+bt.src.clean.spm32k.gz
[2022-02-15 10:19:31] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/train/opusTCv20210807+bt.trg.clean.spm32k.gz
[2022-02-15 10:19:31] [config] transformer-aan-activation: swish
[2022-02-15 10:19:31] [config] transformer-aan-depth: 2
[2022-02-15 10:19:31] [config] transformer-aan-nogate: false
[2022-02-15 10:19:31] [config] transformer-decoder-autoreg: self-attention
[2022-02-15 10:19:31] [config] transformer-depth-scaling: false
[2022-02-15 10:19:31] [config] transformer-dim-aan: 2048
[2022-02-15 10:19:31] [config] transformer-dim-ffn: 4096
[2022-02-15 10:19:31] [config] transformer-dropout: 0.1
[2022-02-15 10:19:31] [config] transformer-dropout-attention: 0
[2022-02-15 10:19:31] [config] transformer-dropout-ffn: 0
[2022-02-15 10:19:31] [config] transformer-ffn-activation: relu
[2022-02-15 10:19:31] [config] transformer-ffn-depth: 2
[2022-02-15 10:19:31] [config] transformer-guided-alignment-layer: last
[2022-02-15 10:19:31] [config] transformer-heads: 16
[2022-02-15 10:19:31] [config] transformer-no-projection: false
[2022-02-15 10:19:31] [config] transformer-pool: false
[2022-02-15 10:19:31] [config] transformer-postprocess: dan
[2022-02-15 10:19:31] [config] transformer-postprocess-emb: d
[2022-02-15 10:19:31] [config] transformer-postprocess-top: ""
[2022-02-15 10:19:31] [config] transformer-preprocess: ""
[2022-02-15 10:19:31] [config] transformer-tied-layers:
[2022-02-15 10:19:31] [config]   []
[2022-02-15 10:19:31] [config] transformer-train-position-embeddings: false
[2022-02-15 10:19:31] [config] tsv: false
[2022-02-15 10:19:31] [config] tsv-fields: 0
[2022-02-15 10:19:31] [config] type: transformer
[2022-02-15 10:19:31] [config] ulr: false
[2022-02-15 10:19:31] [config] ulr-dim-emb: 0
[2022-02-15 10:19:31] [config] ulr-dropout: 0
[2022-02-15 10:19:31] [config] ulr-keys-vectors: ""
[2022-02-15 10:19:31] [config] ulr-query-vectors: ""
[2022-02-15 10:19:31] [config] ulr-softmax-temperature: 1
[2022-02-15 10:19:31] [config] ulr-trainable-transformation: false
[2022-02-15 10:19:31] [config] unlikelihood-loss: false
[2022-02-15 10:19:31] [config] valid-freq: 10000
[2022-02-15 10:19:31] [config] valid-log: /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.valid1.log
[2022-02-15 10:19:31] [config] valid-max-length: 100
[2022-02-15 10:19:31] [config] valid-metrics:
[2022-02-15 10:19:31] [config]   - perplexity
[2022-02-15 10:19:31] [config] valid-mini-batch: 16
[2022-02-15 10:19:31] [config] valid-reset-stalled: false
[2022-02-15 10:19:31] [config] valid-script-args:
[2022-02-15 10:19:31] [config]   []
[2022-02-15 10:19:31] [config] valid-script-path: ""
[2022-02-15 10:19:31] [config] valid-sets:
[2022-02-15 10:19:31] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/val/Tatoeba-dev-v2021-08-07.src.spm32k
[2022-02-15 10:19:31] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/val/Tatoeba-dev-v2021-08-07.trg.spm32k
[2022-02-15 10:19:31] [config] valid-translation-output: ""
[2022-02-15 10:19:31] [config] vocabs:
[2022-02-15 10:19:31] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-02-15 10:19:31] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-02-15 10:19:31] [config] word-penalty: 0
[2022-02-15 10:19:31] [config] word-scores: false
[2022-02-15 10:19:31] [config] workspace: 15000
[2022-02-15 10:19:31] [config] Model is being created with Marian v1.10.24; 12a1bfa 2021-10-11 16:59:52 +0100
[2022-02-15 10:19:31] Using synchronous SGD
[2022-02-15 10:19:31] [comm] Compiled without MPI support. Running as a single process on r02g07.bullx
[2022-02-15 10:19:31] Synced seed 1111
[2022-02-15 10:19:31] [data] Loading vocabulary from JSON/Yaml file /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-02-15 10:19:32] [data] Setting vocabulary size for input 0 to 55,026
[2022-02-15 10:19:32] [data] Loading vocabulary from JSON/Yaml file /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-02-15 10:19:32] [data] Setting vocabulary size for input 1 to 55,026
[2022-02-15 10:19:32] [batching] Collecting statistics for batch fitting with step size 10
[2022-02-15 10:19:33] [memory] Extending reserved space to 15104 MB (device gpu0)
[2022-02-15 10:19:35] [memory] Extending reserved space to 15104 MB (device gpu1)
[2022-02-15 10:19:35] [comm] Using NCCL 2.8.3 for GPU communication
[2022-02-15 10:19:35] [comm] Using global sharding
[2022-02-15 10:19:35] [comm] NCCLCommunicators constructed successfully
[2022-02-15 10:19:35] [training] Using 2 GPUs
[2022-02-15 10:19:36] [logits] Applying loss function for 1 factor(s)
[2022-02-15 10:19:36] [memory] Reserving 887 MB, device gpu0
[2022-02-15 10:19:40] [gpu] 16-bit TensorCores enabled for float32 matrix operations
[2022-02-15 10:19:40] [memory] Reserving 887 MB, device gpu0
[2022-02-15 10:20:09] [batching] Done. Typical MB size is 27,468 target words
[2022-02-15 10:20:09] [memory] Extending reserved space to 15104 MB (device gpu0)
[2022-02-15 10:20:09] [memory] Extending reserved space to 15104 MB (device gpu1)
[2022-02-15 10:20:09] [comm] Using NCCL 2.8.3 for GPU communication
[2022-02-15 10:20:09] [comm] Using global sharding
[2022-02-15 10:20:10] [comm] NCCLCommunicators constructed successfully
[2022-02-15 10:20:10] [training] Using 2 GPUs
[2022-02-15 10:20:10] Training started
[2022-02-15 10:20:33] [training] Batches are processed as 1 process(es) x 2 devices/process
[2022-02-15 10:20:33] [memory] Reserving 887 MB, device gpu0
[2022-02-15 10:20:33] [memory] Reserving 887 MB, device gpu1
[2022-02-15 10:20:33] [memory] Reserving 887 MB, device gpu0
[2022-02-15 10:20:34] [memory] Reserving 887 MB, device gpu1
[2022-02-15 10:20:36] Parameter type float32, optimization type float32, casting types false
[2022-02-15 10:20:36] Allocating memory for general optimizer shards
[2022-02-15 10:20:36] [memory] Reserving 443 MB, device gpu0
[2022-02-15 10:20:36] [memory] Reserving 443 MB, device gpu1
[2022-02-15 10:20:36] Allocating memory for Adam-specific shards
[2022-02-15 10:20:36] [memory] Reserving 887 MB, device gpu1
[2022-02-15 10:20:36] [memory] Reserving 887 MB, device gpu0
[2022-02-15 14:35:44] Ep. 1 : Up. 10000 : Sen. 8,809,207 : Cost 5.02274275 : Time 15334.75s : 14809.34 words/s : gNorm 0.6307
[2022-02-15 14:35:44] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-15 14:35:49] Saving Adam parameters
[2022-02-15 14:35:53] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-15 14:36:04] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-15 14:36:06] [valid] Ep. 1 : Up. 10000 : perplexity : 3.49698 : new best
[2022-02-15 18:51:00] Ep. 1 : Up. 20000 : Sen. 17,604,389 : Cost 2.84500885 : Time 15316.16s : 14825.36 words/s : gNorm 0.4408
[2022-02-15 18:51:00] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-15 18:51:04] Saving Adam parameters
[2022-02-15 18:51:07] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-15 18:51:18] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-15 18:51:20] [valid] Ep. 1 : Up. 20000 : perplexity : 2.55979 : new best
[2022-02-15 23:06:06] Ep. 1 : Up. 30000 : Sen. 26,392,815 : Cost 2.66249204 : Time 15305.98s : 14829.72 words/s : gNorm 0.4174
[2022-02-15 23:06:06] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-15 23:06:10] Saving Adam parameters
[2022-02-15 23:06:13] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-15 23:06:28] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-15 23:06:30] [valid] Ep. 1 : Up. 30000 : perplexity : 2.38431 : new best
[2022-02-16 03:21:25] Ep. 1 : Up. 40000 : Sen. 35,198,052 : Cost 2.58715487 : Time 15319.23s : 14826.45 words/s : gNorm 0.4013
[2022-02-16 03:21:25] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-16 03:21:29] Saving Adam parameters
[2022-02-16 03:21:32] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-16 03:21:43] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-16 03:21:45] [valid] Ep. 1 : Up. 40000 : perplexity : 2.30569 : new best
[2022-02-16 07:36:25] Ep. 1 : Up. 50000 : Sen. 44,010,124 : Cost 2.54463458 : Time 15299.94s : 14843.63 words/s : gNorm 0.4052
[2022-02-16 07:36:25] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-16 07:36:32] Saving Adam parameters
[2022-02-16 07:36:39] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-16 07:36:57] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-16 07:37:00] [valid] Ep. 1 : Up. 50000 : perplexity : 2.25489 : new best
[2022-02-16 11:51:23] Ep. 1 : Up. 60000 : Sen. 52,814,651 : Cost 2.51546717 : Time 15297.24s : 14836.83 words/s : gNorm 0.4068
[2022-02-16 11:51:23] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-16 11:51:26] Saving Adam parameters
[2022-02-16 11:51:30] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-16 11:51:40] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-16 11:51:43] [valid] Ep. 1 : Up. 60000 : perplexity : 2.22202 : new best
[2022-02-16 16:06:30] Ep. 1 : Up. 70000 : Sen. 61,603,455 : Cost 2.49276805 : Time 15307.10s : 14841.56 words/s : gNorm 0.4038
[2022-02-16 16:06:30] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-16 16:06:34] Saving Adam parameters
[2022-02-16 16:06:37] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-16 16:06:47] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-16 16:06:49] [valid] Ep. 1 : Up. 70000 : perplexity : 2.19718 : new best
[2022-02-16 20:21:33] Ep. 1 : Up. 80000 : Sen. 70,427,669 : Cost 2.47609258 : Time 15303.24s : 14832.44 words/s : gNorm 0.4058
[2022-02-16 20:21:33] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-16 20:21:37] Saving Adam parameters
[2022-02-16 20:21:40] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-16 20:21:51] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-16 20:21:53] [valid] Ep. 1 : Up. 80000 : perplexity : 2.18057 : new best
[2022-02-17 00:36:40] Ep. 1 : Up. 90000 : Sen. 79,211,349 : Cost 2.46154141 : Time 15306.38s : 14843.63 words/s : gNorm 0.4111
[2022-02-17 00:36:40] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-17 00:36:43] Saving Adam parameters
[2022-02-17 00:36:46] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-17 00:36:57] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-17 00:36:59] [valid] Ep. 1 : Up. 90000 : perplexity : 2.16579 : new best
[2022-02-17 04:51:33] Ep. 1 : Up. 100000 : Sen. 88,016,176 : Cost 2.44998550 : Time 15293.28s : 14851.03 words/s : gNorm 0.4123
[2022-02-17 04:51:33] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-17 04:51:40] Saving Adam parameters
[2022-02-17 04:51:47] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-17 04:51:58] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-17 04:52:00] [valid] Ep. 1 : Up. 100000 : perplexity : 2.15308 : new best
[2022-02-17 09:06:47] Ep. 1 : Up. 110000 : Sen. 96,828,380 : Cost 2.44059992 : Time 15313.53s : 14831.66 words/s : gNorm 0.4348
[2022-02-17 09:06:47] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-17 09:06:50] Saving Adam parameters
[2022-02-17 09:06:53] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-17 09:07:04] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-17 09:07:06] [valid] Ep. 1 : Up. 110000 : perplexity : 2.14246 : new best
[2022-02-17 13:22:05] Ep. 1 : Up. 120000 : Sen. 105,605,925 : Cost 2.43057156 : Time 15318.11s : 14826.53 words/s : gNorm 0.4131
[2022-02-17 13:22:05] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-17 13:22:08] Saving Adam parameters
[2022-02-17 13:22:12] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-17 13:22:22] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-17 13:22:24] [valid] Ep. 1 : Up. 120000 : perplexity : 2.13322 : new best
[2022-02-17 17:37:19] Ep. 1 : Up. 130000 : Sen. 114,405,705 : Cost 2.42399716 : Time 15314.54s : 14820.68 words/s : gNorm 0.4227
[2022-02-17 17:37:19] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-17 17:37:23] Saving Adam parameters
[2022-02-17 17:37:26] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-17 17:37:38] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-17 17:37:40] [valid] Ep. 1 : Up. 130000 : perplexity : 2.1259 : new best
[2022-02-17 21:52:42] Ep. 1 : Up. 140000 : Sen. 123,216,854 : Cost 2.41724157 : Time 15322.83s : 14824.45 words/s : gNorm 0.4289
[2022-02-17 21:52:42] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-17 21:52:46] Saving Adam parameters
[2022-02-17 21:52:49] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-17 21:53:00] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-17 21:53:02] [valid] Ep. 1 : Up. 140000 : perplexity : 2.12021 : new best
[2022-02-18 02:08:29] Ep. 1 : Up. 150000 : Sen. 132,035,340 : Cost 2.41106033 : Time 15346.46s : 14806.68 words/s : gNorm 0.4229
[2022-02-18 02:08:29] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-18 02:08:32] Saving Adam parameters
[2022-02-18 02:08:36] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-18 02:08:46] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-18 02:08:49] [valid] Ep. 1 : Up. 150000 : perplexity : 2.11406 : new best
[2022-02-18 06:23:46] Ep. 1 : Up. 160000 : Sen. 140,844,597 : Cost 2.40597939 : Time 15316.93s : 14816.97 words/s : gNorm 0.4184
[2022-02-18 06:23:46] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-18 06:23:49] Saving Adam parameters
[2022-02-18 06:23:53] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-18 06:24:03] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-18 06:24:06] [valid] Ep. 1 : Up. 160000 : perplexity : 2.10989 : new best
[2022-02-18 15:38:34] [marian] Marian v1.10.24; 12a1bfa 2021-10-11 16:59:52 +0100
[2022-02-18 15:38:34] [marian] Running on r14g01.bullx as process 11416 with command line:
[2022-02-18 15:38:34] [marian] /projappl/project_2001194/marian-dev/build/marian --task transformer-big --optimizer-delay 2 --no-restore-corpus --early-stopping 10 --valid-freq 10000 --valid-sets /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/val/Tatoeba-dev-v2021-08-07.src.spm32k /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/val/Tatoeba-dev-v2021-08-07.trg.spm32k --valid-metrics perplexity --valid-mini-batch 16 --valid-max-length 100 --valid-log /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.valid1.log --beam-size 6 --normalize 1 --allow-unk --workspace 15000 --model /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz --train-sets /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/train/opusTCv20210807+bt.src.clean.spm32k.gz /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/train/opusTCv20210807+bt.trg.clean.spm32k.gz --vocabs /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.vocab.yml /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.vocab.yml --save-freq 10000 --disp-freq 10000 --log /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.train1.log --devices 0 1 --seed 1111 --tempdir /run/nvme/job_10610612/tmp --shuffle batches --sharding local --overwrite --keep-best
[2022-02-18 15:38:37] [config] after: 0e
[2022-02-18 15:38:37] [config] after-batches: 0
[2022-02-18 15:38:37] [config] after-epochs: 0
[2022-02-18 15:38:37] [config] all-caps-every: 0
[2022-02-18 15:38:37] [config] allow-unk: true
[2022-02-18 15:38:37] [config] authors: false
[2022-02-18 15:38:37] [config] beam-size: 6
[2022-02-18 15:38:37] [config] bert-class-symbol: "[CLS]"
[2022-02-18 15:38:37] [config] bert-mask-symbol: "[MASK]"
[2022-02-18 15:38:37] [config] bert-masking-fraction: 0.15
[2022-02-18 15:38:37] [config] bert-sep-symbol: "[SEP]"
[2022-02-18 15:38:37] [config] bert-train-type-embeddings: true
[2022-02-18 15:38:37] [config] bert-type-vocab-size: 2
[2022-02-18 15:38:37] [config] build-info: ""
[2022-02-18 15:38:37] [config] check-gradient-nan: false
[2022-02-18 15:38:37] [config] check-nan: false
[2022-02-18 15:38:37] [config] cite: false
[2022-02-18 15:38:37] [config] clip-norm: 0
[2022-02-18 15:38:37] [config] cost-scaling:
[2022-02-18 15:38:37] [config]   []
[2022-02-18 15:38:37] [config] cost-type: ce-mean-words
[2022-02-18 15:38:37] [config] cpu-threads: 0
[2022-02-18 15:38:37] [config] data-weighting: ""
[2022-02-18 15:38:37] [config] data-weighting-type: sentence
[2022-02-18 15:38:37] [config] dec-cell: gru
[2022-02-18 15:38:37] [config] dec-cell-base-depth: 2
[2022-02-18 15:38:37] [config] dec-cell-high-depth: 1
[2022-02-18 15:38:37] [config] dec-depth: 6
[2022-02-18 15:38:37] [config] devices:
[2022-02-18 15:38:37] [config]   - 0
[2022-02-18 15:38:37] [config]   - 1
[2022-02-18 15:38:37] [config] dim-emb: 1024
[2022-02-18 15:38:37] [config] dim-rnn: 1024
[2022-02-18 15:38:37] [config] dim-vocabs:
[2022-02-18 15:38:37] [config]   - 55026
[2022-02-18 15:38:37] [config]   - 55026
[2022-02-18 15:38:37] [config] disp-first: 0
[2022-02-18 15:38:37] [config] disp-freq: 10000
[2022-02-18 15:38:37] [config] disp-label-counts: true
[2022-02-18 15:38:37] [config] dropout-rnn: 0
[2022-02-18 15:38:37] [config] dropout-src: 0
[2022-02-18 15:38:37] [config] dropout-trg: 0
[2022-02-18 15:38:37] [config] dump-config: ""
[2022-02-18 15:38:37] [config] dynamic-gradient-scaling:
[2022-02-18 15:38:37] [config]   []
[2022-02-18 15:38:37] [config] early-stopping: 10
[2022-02-18 15:38:37] [config] early-stopping-on: first
[2022-02-18 15:38:37] [config] embedding-fix-src: false
[2022-02-18 15:38:37] [config] embedding-fix-trg: false
[2022-02-18 15:38:37] [config] embedding-normalization: false
[2022-02-18 15:38:37] [config] embedding-vectors:
[2022-02-18 15:38:37] [config]   []
[2022-02-18 15:38:37] [config] enc-cell: gru
[2022-02-18 15:38:37] [config] enc-cell-depth: 1
[2022-02-18 15:38:37] [config] enc-depth: 6
[2022-02-18 15:38:37] [config] enc-type: bidirectional
[2022-02-18 15:38:37] [config] english-title-case-every: 0
[2022-02-18 15:38:37] [config] exponential-smoothing: 0.0001
[2022-02-18 15:38:37] [config] factor-weight: 1
[2022-02-18 15:38:37] [config] factors-combine: sum
[2022-02-18 15:38:37] [config] factors-dim-emb: 0
[2022-02-18 15:38:37] [config] gradient-checkpointing: false
[2022-02-18 15:38:37] [config] gradient-norm-average-window: 100
[2022-02-18 15:38:37] [config] guided-alignment: none
[2022-02-18 15:38:37] [config] guided-alignment-cost: mse
[2022-02-18 15:38:37] [config] guided-alignment-weight: 0.1
[2022-02-18 15:38:37] [config] ignore-model-config: false
[2022-02-18 15:38:37] [config] input-types:
[2022-02-18 15:38:37] [config]   []
[2022-02-18 15:38:37] [config] interpolate-env-vars: false
[2022-02-18 15:38:37] [config] keep-best: true
[2022-02-18 15:38:37] [config] label-smoothing: 0.1
[2022-02-18 15:38:37] [config] layer-normalization: false
[2022-02-18 15:38:37] [config] learn-rate: 0.0002
[2022-02-18 15:38:37] [config] lemma-dependency: ""
[2022-02-18 15:38:37] [config] lemma-dim-emb: 0
[2022-02-18 15:38:37] [config] log: /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.train1.log
[2022-02-18 15:38:37] [config] log-level: info
[2022-02-18 15:38:37] [config] log-time-zone: ""
[2022-02-18 15:38:37] [config] logical-epoch:
[2022-02-18 15:38:37] [config]   - 1e
[2022-02-18 15:38:37] [config]   - 0
[2022-02-18 15:38:37] [config] lr-decay: 0
[2022-02-18 15:38:37] [config] lr-decay-freq: 50000
[2022-02-18 15:38:37] [config] lr-decay-inv-sqrt:
[2022-02-18 15:38:37] [config]   - 8000
[2022-02-18 15:38:37] [config] lr-decay-repeat-warmup: false
[2022-02-18 15:38:37] [config] lr-decay-reset-optimizer: false
[2022-02-18 15:38:37] [config] lr-decay-start:
[2022-02-18 15:38:37] [config]   - 10
[2022-02-18 15:38:37] [config]   - 1
[2022-02-18 15:38:37] [config] lr-decay-strategy: epoch+stalled
[2022-02-18 15:38:37] [config] lr-report: false
[2022-02-18 15:38:37] [config] lr-warmup: 8000
[2022-02-18 15:38:37] [config] lr-warmup-at-reload: false
[2022-02-18 15:38:37] [config] lr-warmup-cycle: false
[2022-02-18 15:38:37] [config] lr-warmup-start-rate: 0
[2022-02-18 15:38:37] [config] max-length: 100
[2022-02-18 15:38:37] [config] max-length-crop: false
[2022-02-18 15:38:37] [config] max-length-factor: 3
[2022-02-18 15:38:37] [config] maxi-batch: 1000
[2022-02-18 15:38:37] [config] maxi-batch-sort: trg
[2022-02-18 15:38:37] [config] mini-batch: 1000
[2022-02-18 15:38:37] [config] mini-batch-fit: true
[2022-02-18 15:38:37] [config] mini-batch-fit-step: 10
[2022-02-18 15:38:37] [config] mini-batch-round-up: true
[2022-02-18 15:38:37] [config] mini-batch-track-lr: false
[2022-02-18 15:38:37] [config] mini-batch-warmup: 0
[2022-02-18 15:38:37] [config] mini-batch-words: 0
[2022-02-18 15:38:37] [config] mini-batch-words-ref: 0
[2022-02-18 15:38:37] [config] model: /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-18 15:38:37] [config] multi-loss-type: sum
[2022-02-18 15:38:37] [config] n-best: false
[2022-02-18 15:38:37] [config] no-nccl: false
[2022-02-18 15:38:37] [config] no-reload: false
[2022-02-18 15:38:37] [config] no-restore-corpus: true
[2022-02-18 15:38:37] [config] normalize: 1
[2022-02-18 15:38:37] [config] normalize-gradient: false
[2022-02-18 15:38:37] [config] num-devices: 0
[2022-02-18 15:38:37] [config] optimizer: adam
[2022-02-18 15:38:37] [config] optimizer-delay: 2
[2022-02-18 15:38:37] [config] optimizer-params:
[2022-02-18 15:38:37] [config]   - 0.9
[2022-02-18 15:38:37] [config]   - 0.998
[2022-02-18 15:38:37] [config]   - 1e-09
[2022-02-18 15:38:37] [config] output-omit-bias: false
[2022-02-18 15:38:37] [config] overwrite: true
[2022-02-18 15:38:37] [config] precision:
[2022-02-18 15:38:37] [config]   - float32
[2022-02-18 15:38:37] [config]   - float32
[2022-02-18 15:38:37] [config] pretrained-model: ""
[2022-02-18 15:38:37] [config] quantize-biases: false
[2022-02-18 15:38:37] [config] quantize-bits: 0
[2022-02-18 15:38:37] [config] quantize-log-based: false
[2022-02-18 15:38:37] [config] quantize-optimization-steps: 0
[2022-02-18 15:38:37] [config] quiet: false
[2022-02-18 15:38:37] [config] quiet-translation: false
[2022-02-18 15:38:37] [config] relative-paths: false
[2022-02-18 15:38:37] [config] right-left: false
[2022-02-18 15:38:37] [config] save-freq: 10000
[2022-02-18 15:38:37] [config] seed: 1111
[2022-02-18 15:38:37] [config] sentencepiece-alphas:
[2022-02-18 15:38:37] [config]   []
[2022-02-18 15:38:37] [config] sentencepiece-max-lines: 2000000
[2022-02-18 15:38:37] [config] sentencepiece-options: ""
[2022-02-18 15:38:37] [config] sharding: local
[2022-02-18 15:38:37] [config] shuffle: batches
[2022-02-18 15:38:37] [config] shuffle-in-ram: false
[2022-02-18 15:38:37] [config] sigterm: save-and-exit
[2022-02-18 15:38:37] [config] skip: false
[2022-02-18 15:38:37] [config] sqlite: ""
[2022-02-18 15:38:37] [config] sqlite-drop: false
[2022-02-18 15:38:37] [config] sync-freq: 200u
[2022-02-18 15:38:37] [config] sync-sgd: true
[2022-02-18 15:38:37] [config] tempdir: /run/nvme/job_10610612/tmp
[2022-02-18 15:38:37] [config] tied-embeddings: false
[2022-02-18 15:38:37] [config] tied-embeddings-all: true
[2022-02-18 15:38:37] [config] tied-embeddings-src: false
[2022-02-18 15:38:37] [config] train-embedder-rank:
[2022-02-18 15:38:37] [config]   []
[2022-02-18 15:38:37] [config] train-sets:
[2022-02-18 15:38:37] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/train/opusTCv20210807+bt.src.clean.spm32k.gz
[2022-02-18 15:38:37] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/train/opusTCv20210807+bt.trg.clean.spm32k.gz
[2022-02-18 15:38:37] [config] transformer-aan-activation: swish
[2022-02-18 15:38:37] [config] transformer-aan-depth: 2
[2022-02-18 15:38:37] [config] transformer-aan-nogate: false
[2022-02-18 15:38:37] [config] transformer-decoder-autoreg: self-attention
[2022-02-18 15:38:37] [config] transformer-depth-scaling: false
[2022-02-18 15:38:37] [config] transformer-dim-aan: 2048
[2022-02-18 15:38:37] [config] transformer-dim-ffn: 4096
[2022-02-18 15:38:37] [config] transformer-dropout: 0.1
[2022-02-18 15:38:37] [config] transformer-dropout-attention: 0
[2022-02-18 15:38:37] [config] transformer-dropout-ffn: 0
[2022-02-18 15:38:37] [config] transformer-ffn-activation: relu
[2022-02-18 15:38:37] [config] transformer-ffn-depth: 2
[2022-02-18 15:38:37] [config] transformer-guided-alignment-layer: last
[2022-02-18 15:38:37] [config] transformer-heads: 16
[2022-02-18 15:38:37] [config] transformer-no-projection: false
[2022-02-18 15:38:37] [config] transformer-pool: false
[2022-02-18 15:38:37] [config] transformer-postprocess: dan
[2022-02-18 15:38:37] [config] transformer-postprocess-emb: d
[2022-02-18 15:38:37] [config] transformer-postprocess-top: ""
[2022-02-18 15:38:37] [config] transformer-preprocess: ""
[2022-02-18 15:38:37] [config] transformer-tied-layers:
[2022-02-18 15:38:37] [config]   []
[2022-02-18 15:38:37] [config] transformer-train-position-embeddings: false
[2022-02-18 15:38:37] [config] tsv: false
[2022-02-18 15:38:37] [config] tsv-fields: 0
[2022-02-18 15:38:37] [config] type: transformer
[2022-02-18 15:38:37] [config] ulr: false
[2022-02-18 15:38:37] [config] ulr-dim-emb: 0
[2022-02-18 15:38:37] [config] ulr-dropout: 0
[2022-02-18 15:38:37] [config] ulr-keys-vectors: ""
[2022-02-18 15:38:37] [config] ulr-query-vectors: ""
[2022-02-18 15:38:37] [config] ulr-softmax-temperature: 1
[2022-02-18 15:38:37] [config] ulr-trainable-transformation: false
[2022-02-18 15:38:37] [config] unlikelihood-loss: false
[2022-02-18 15:38:37] [config] valid-freq: 10000
[2022-02-18 15:38:37] [config] valid-log: /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.valid1.log
[2022-02-18 15:38:37] [config] valid-max-length: 100
[2022-02-18 15:38:37] [config] valid-metrics:
[2022-02-18 15:38:37] [config]   - perplexity
[2022-02-18 15:38:37] [config] valid-mini-batch: 16
[2022-02-18 15:38:37] [config] valid-reset-stalled: false
[2022-02-18 15:38:37] [config] valid-script-args:
[2022-02-18 15:38:37] [config]   []
[2022-02-18 15:38:37] [config] valid-script-path: ""
[2022-02-18 15:38:37] [config] valid-sets:
[2022-02-18 15:38:37] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/val/Tatoeba-dev-v2021-08-07.src.spm32k
[2022-02-18 15:38:37] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/val/Tatoeba-dev-v2021-08-07.trg.spm32k
[2022-02-18 15:38:37] [config] valid-translation-output: ""
[2022-02-18 15:38:37] [config] version: v1.10.24; 12a1bfa 2021-10-11 16:59:52 +0100
[2022-02-18 15:38:37] [config] vocabs:
[2022-02-18 15:38:37] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-02-18 15:38:37] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-02-18 15:38:37] [config] word-penalty: 0
[2022-02-18 15:38:37] [config] word-scores: false
[2022-02-18 15:38:37] [config] workspace: 15000
[2022-02-18 15:38:37] [config] Loaded model has been created with Marian v1.10.24; 12a1bfa 2021-10-11 16:59:52 +0100
[2022-02-18 15:38:37] Using synchronous SGD
[2022-02-18 15:38:37] [comm] Compiled without MPI support. Running as a single process on r14g01.bullx
[2022-02-18 15:38:37] Synced seed 1111
[2022-02-18 15:38:37] [data] Loading vocabulary from JSON/Yaml file /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-02-18 15:38:37] [data] Setting vocabulary size for input 0 to 55,026
[2022-02-18 15:38:37] [data] Loading vocabulary from JSON/Yaml file /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-02-18 15:38:37] [data] Setting vocabulary size for input 1 to 55,026
[2022-02-18 15:38:38] [batching] Collecting statistics for batch fitting with step size 10
[2022-02-18 15:38:38] [memory] Extending reserved space to 15104 MB (device gpu0)
[2022-02-18 15:38:41] [memory] Extending reserved space to 15104 MB (device gpu1)
[2022-02-18 15:38:41] [comm] Using NCCL 2.8.3 for GPU communication
[2022-02-18 15:38:41] [comm] Using global sharding
[2022-02-18 15:38:43] [comm] NCCLCommunicators constructed successfully
[2022-02-18 15:38:43] [training] Using 2 GPUs
[2022-02-18 15:38:43] [logits] Applying loss function for 1 factor(s)
[2022-02-18 15:38:43] [memory] Reserving 887 MB, device gpu0
[2022-02-18 15:38:47] [gpu] 16-bit TensorCores enabled for float32 matrix operations
[2022-02-18 15:38:47] [memory] Reserving 887 MB, device gpu0
[2022-02-18 15:39:15] [batching] Done. Typical MB size is 27,468 target words
[2022-02-18 15:39:15] [memory] Extending reserved space to 15104 MB (device gpu0)
[2022-02-18 15:39:15] [memory] Extending reserved space to 15104 MB (device gpu1)
[2022-02-18 15:39:15] [comm] Using NCCL 2.8.3 for GPU communication
[2022-02-18 15:39:15] [comm] Using global sharding
[2022-02-18 15:39:18] [comm] NCCLCommunicators constructed successfully
[2022-02-18 15:39:18] [training] Using 2 GPUs
[2022-02-18 15:39:18] Loading model from /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-18 15:39:20] Loading model from /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-18 15:39:29] Allocating memory for general optimizer shards
[2022-02-18 15:39:29] [memory] Reserving 443 MB, device gpu0
[2022-02-18 15:39:29] [memory] Reserving 443 MB, device gpu1
[2022-02-18 15:39:29] Loading Adam parameters
[2022-02-18 15:39:30] [memory] Reserving 887 MB, device gpu0
[2022-02-18 15:39:30] [memory] Reserving 887 MB, device gpu1
[2022-02-18 15:39:30] [memory] Reserving 887 MB, device gpu0
[2022-02-18 15:39:31] [memory] Reserving 887 MB, device gpu1
[2022-02-18 15:39:31] [training] Master parameters and optimizers restored from training checkpoint /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-18 15:39:31] Training started
[2022-02-18 15:39:54] [training] Batches are processed as 1 process(es) x 2 devices/process
[2022-02-18 15:39:54] [memory] Reserving 887 MB, device gpu0
[2022-02-18 15:39:54] [memory] Reserving 887 MB, device gpu1
[2022-02-18 15:39:57] Parameter type float32, optimization type float32, casting types false
[2022-02-18 19:53:49] Ep. 1 : Up. 170000 : Sen. 8,809,207 : Cost 2.40070343 : Time 15273.73s : 14868.50 words/s : gNorm 0.4300
[2022-02-18 19:53:49] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-18 19:53:53] Saving Adam parameters
[2022-02-18 19:53:56] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-18 19:54:06] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-18 19:54:09] [valid] Ep. 1 : Up. 170000 : perplexity : 2.10328 : new best
[2022-02-19 00:07:48] Ep. 1 : Up. 180000 : Sen. 17,604,389 : Cost 2.39401460 : Time 15238.76s : 14900.65 words/s : gNorm 0.4393
[2022-02-19 00:07:48] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-19 00:07:51] Saving Adam parameters
[2022-02-19 00:07:54] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-19 00:08:05] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-19 00:08:08] [valid] Ep. 1 : Up. 180000 : perplexity : 2.09943 : new best
[2022-02-19 04:21:45] Ep. 1 : Up. 190000 : Sen. 26,392,815 : Cost 2.39059377 : Time 15237.35s : 14896.52 words/s : gNorm 0.4357
[2022-02-19 04:21:45] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-19 04:21:48] Saving Adam parameters
[2022-02-19 04:21:52] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-19 04:22:02] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-19 04:22:04] [valid] Ep. 1 : Up. 190000 : perplexity : 2.09659 : new best
[2022-02-19 08:36:07] Ep. 1 : Up. 200000 : Sen. 35,198,052 : Cost 2.38520670 : Time 15261.29s : 14882.73 words/s : gNorm 0.4260
[2022-02-19 08:36:07] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-19 08:36:10] Saving Adam parameters
[2022-02-19 08:36:14] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-19 08:36:24] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-19 08:36:27] [valid] Ep. 1 : Up. 200000 : perplexity : 2.09278 : new best
[2022-02-19 12:50:21] Ep. 1 : Up. 210000 : Sen. 44,010,124 : Cost 2.38197231 : Time 15253.97s : 14888.37 words/s : gNorm 0.4546
[2022-02-19 12:50:21] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-19 12:50:28] Saving Adam parameters
[2022-02-19 12:50:34] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-19 12:50:53] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-19 12:50:57] [valid] Ep. 1 : Up. 210000 : perplexity : 2.08828 : new best
[2022-02-19 17:04:41] Ep. 1 : Up. 220000 : Sen. 52,814,651 : Cost 2.37867594 : Time 15260.52s : 14872.53 words/s : gNorm 0.4590
[2022-02-19 17:04:41] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-19 17:04:45] Saving Adam parameters
[2022-02-19 17:04:48] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-19 17:04:59] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-19 17:05:01] [valid] Ep. 1 : Up. 220000 : perplexity : 2.08346 : new best
[2022-02-19 21:19:16] Ep. 1 : Up. 230000 : Sen. 61,603,455 : Cost 2.37472773 : Time 15274.76s : 14872.98 words/s : gNorm 0.4559
[2022-02-19 21:19:16] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-19 21:19:20] Saving Adam parameters
[2022-02-19 21:19:23] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-19 21:19:34] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-19 21:19:36] [valid] Ep. 1 : Up. 230000 : perplexity : 2.08006 : new best
[2022-02-20 01:33:21] Ep. 1 : Up. 240000 : Sen. 70,427,669 : Cost 2.37211704 : Time 15244.53s : 14889.57 words/s : gNorm 0.4583
[2022-02-20 01:33:21] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-20 01:33:24] Saving Adam parameters
[2022-02-20 01:33:28] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-20 01:33:38] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-20 01:33:40] [valid] Ep. 1 : Up. 240000 : perplexity : 2.07831 : new best
[2022-02-20 05:47:22] Ep. 1 : Up. 250000 : Sen. 79,211,349 : Cost 2.36868048 : Time 15241.13s : 14907.18 words/s : gNorm 0.4638
[2022-02-20 05:47:22] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-20 05:47:25] Saving Adam parameters
[2022-02-20 05:47:29] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-20 05:47:39] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-20 05:47:42] [valid] Ep. 1 : Up. 250000 : perplexity : 2.07611 : new best
[2022-02-20 10:01:38] Ep. 1 : Up. 260000 : Sen. 88,016,176 : Cost 2.36620450 : Time 15256.26s : 14887.06 words/s : gNorm 0.4670
[2022-02-20 10:01:38] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-20 10:01:45] Saving Adam parameters
[2022-02-20 10:01:52] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-20 10:02:03] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-20 10:02:06] [valid] Ep. 1 : Up. 260000 : perplexity : 2.0724 : new best
[2022-02-20 14:16:44] Ep. 1 : Up. 270000 : Sen. 96,828,380 : Cost 2.36416888 : Time 15305.77s : 14839.17 words/s : gNorm 0.4864
[2022-02-20 14:16:44] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-20 14:16:48] Saving Adam parameters
[2022-02-20 14:16:51] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-20 14:17:02] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-20 14:17:04] [valid] Ep. 1 : Up. 270000 : perplexity : 2.07136 : new best
[2022-02-20 18:31:56] Ep. 1 : Up. 280000 : Sen. 105,605,925 : Cost 2.36056399 : Time 15311.96s : 14832.48 words/s : gNorm 0.4599
[2022-02-20 18:31:56] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-20 18:31:59] Saving Adam parameters
[2022-02-20 18:32:03] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-20 18:32:13] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-20 18:32:16] [valid] Ep. 1 : Up. 280000 : perplexity : 2.06703 : new best
[2022-02-20 22:47:02] Ep. 1 : Up. 290000 : Sen. 114,405,705 : Cost 2.35923696 : Time 15305.69s : 14829.25 words/s : gNorm 0.4792
[2022-02-20 22:47:02] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-20 22:47:06] Saving Adam parameters
[2022-02-20 22:47:09] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-20 22:47:20] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-20 22:47:22] [valid] Ep. 1 : Up. 290000 : perplexity : 2.06536 : new best
[2022-02-21 03:01:54] Ep. 1 : Up. 300000 : Sen. 123,216,854 : Cost 2.35732865 : Time 15291.71s : 14854.62 words/s : gNorm 0.4726
[2022-02-21 03:01:54] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-21 03:01:57] Saving Adam parameters
[2022-02-21 03:02:01] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-21 03:02:11] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-21 03:02:13] [valid] Ep. 1 : Up. 300000 : perplexity : 2.06383 : new best
[2022-02-21 07:16:53] Ep. 1 : Up. 310000 : Sen. 132,035,340 : Cost 2.35516262 : Time 15299.52s : 14852.11 words/s : gNorm 0.4715
[2022-02-21 07:16:53] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-21 07:16:57] Saving Adam parameters
[2022-02-21 07:17:00] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-21 07:17:11] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-21 07:17:13] [valid] Ep. 1 : Up. 310000 : perplexity : 2.0628 : new best
[2022-02-21 11:31:21] Ep. 1 : Up. 320000 : Sen. 140,844,597 : Cost 2.35368919 : Time 15267.82s : 14864.63 words/s : gNorm 0.4621
[2022-02-21 11:31:21] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-21 11:31:25] Saving Adam parameters
[2022-02-21 11:31:28] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-21 11:31:39] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-21 11:31:41] [valid] Ep. 1 : Up. 320000 : perplexity : 2.06134 : new best
[2022-02-21 16:42:45] [marian] Marian v1.10.24; 12a1bfa 2021-10-11 16:59:52 +0100
[2022-02-21 16:42:45] [marian] Running on r18g08.bullx as process 67540 with command line:
[2022-02-21 16:42:45] [marian] /projappl/project_2001194/marian-dev/build/marian --task transformer-big --optimizer-delay 2 --no-restore-corpus --early-stopping 10 --valid-freq 10000 --valid-sets /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/val/Tatoeba-dev-v2021-08-07.src.spm32k /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/val/Tatoeba-dev-v2021-08-07.trg.spm32k --valid-metrics perplexity --valid-mini-batch 16 --valid-max-length 100 --valid-log /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.valid1.log --beam-size 6 --normalize 1 --allow-unk --workspace 15000 --model /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz --train-sets /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/train/opusTCv20210807+bt.src.clean.spm32k.gz /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/train/opusTCv20210807+bt.trg.clean.spm32k.gz --vocabs /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.vocab.yml /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.vocab.yml --save-freq 10000 --disp-freq 10000 --log /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.train1.log --devices 0 1 --seed 1111 --tempdir /run/nvme/job_10668947/tmp --shuffle batches --sharding local --overwrite --keep-best
[2022-02-21 16:42:47] [config] after: 0e
[2022-02-21 16:42:47] [config] after-batches: 0
[2022-02-21 16:42:47] [config] after-epochs: 0
[2022-02-21 16:42:47] [config] all-caps-every: 0
[2022-02-21 16:42:47] [config] allow-unk: true
[2022-02-21 16:42:47] [config] authors: false
[2022-02-21 16:42:47] [config] beam-size: 6
[2022-02-21 16:42:47] [config] bert-class-symbol: "[CLS]"
[2022-02-21 16:42:47] [config] bert-mask-symbol: "[MASK]"
[2022-02-21 16:42:47] [config] bert-masking-fraction: 0.15
[2022-02-21 16:42:47] [config] bert-sep-symbol: "[SEP]"
[2022-02-21 16:42:47] [config] bert-train-type-embeddings: true
[2022-02-21 16:42:47] [config] bert-type-vocab-size: 2
[2022-02-21 16:42:47] [config] build-info: ""
[2022-02-21 16:42:47] [config] check-gradient-nan: false
[2022-02-21 16:42:47] [config] check-nan: false
[2022-02-21 16:42:47] [config] cite: false
[2022-02-21 16:42:47] [config] clip-norm: 0
[2022-02-21 16:42:47] [config] cost-scaling:
[2022-02-21 16:42:47] [config]   []
[2022-02-21 16:42:47] [config] cost-type: ce-mean-words
[2022-02-21 16:42:47] [config] cpu-threads: 0
[2022-02-21 16:42:47] [config] data-weighting: ""
[2022-02-21 16:42:47] [config] data-weighting-type: sentence
[2022-02-21 16:42:47] [config] dec-cell: gru
[2022-02-21 16:42:47] [config] dec-cell-base-depth: 2
[2022-02-21 16:42:47] [config] dec-cell-high-depth: 1
[2022-02-21 16:42:47] [config] dec-depth: 6
[2022-02-21 16:42:47] [config] devices:
[2022-02-21 16:42:47] [config]   - 0
[2022-02-21 16:42:47] [config]   - 1
[2022-02-21 16:42:47] [config] dim-emb: 1024
[2022-02-21 16:42:47] [config] dim-rnn: 1024
[2022-02-21 16:42:47] [config] dim-vocabs:
[2022-02-21 16:42:47] [config]   - 55026
[2022-02-21 16:42:47] [config]   - 55026
[2022-02-21 16:42:47] [config] disp-first: 0
[2022-02-21 16:42:47] [config] disp-freq: 10000
[2022-02-21 16:42:47] [config] disp-label-counts: true
[2022-02-21 16:42:47] [config] dropout-rnn: 0
[2022-02-21 16:42:47] [config] dropout-src: 0
[2022-02-21 16:42:47] [config] dropout-trg: 0
[2022-02-21 16:42:47] [config] dump-config: ""
[2022-02-21 16:42:47] [config] dynamic-gradient-scaling:
[2022-02-21 16:42:47] [config]   []
[2022-02-21 16:42:47] [config] early-stopping: 10
[2022-02-21 16:42:47] [config] early-stopping-on: first
[2022-02-21 16:42:47] [config] embedding-fix-src: false
[2022-02-21 16:42:47] [config] embedding-fix-trg: false
[2022-02-21 16:42:47] [config] embedding-normalization: false
[2022-02-21 16:42:47] [config] embedding-vectors:
[2022-02-21 16:42:47] [config]   []
[2022-02-21 16:42:47] [config] enc-cell: gru
[2022-02-21 16:42:47] [config] enc-cell-depth: 1
[2022-02-21 16:42:47] [config] enc-depth: 6
[2022-02-21 16:42:47] [config] enc-type: bidirectional
[2022-02-21 16:42:47] [config] english-title-case-every: 0
[2022-02-21 16:42:47] [config] exponential-smoothing: 0.0001
[2022-02-21 16:42:47] [config] factor-weight: 1
[2022-02-21 16:42:47] [config] factors-combine: sum
[2022-02-21 16:42:47] [config] factors-dim-emb: 0
[2022-02-21 16:42:47] [config] gradient-checkpointing: false
[2022-02-21 16:42:47] [config] gradient-norm-average-window: 100
[2022-02-21 16:42:47] [config] guided-alignment: none
[2022-02-21 16:42:47] [config] guided-alignment-cost: mse
[2022-02-21 16:42:47] [config] guided-alignment-weight: 0.1
[2022-02-21 16:42:47] [config] ignore-model-config: false
[2022-02-21 16:42:47] [config] input-types:
[2022-02-21 16:42:47] [config]   []
[2022-02-21 16:42:47] [config] interpolate-env-vars: false
[2022-02-21 16:42:47] [config] keep-best: true
[2022-02-21 16:42:47] [config] label-smoothing: 0.1
[2022-02-21 16:42:47] [config] layer-normalization: false
[2022-02-21 16:42:47] [config] learn-rate: 0.0002
[2022-02-21 16:42:47] [config] lemma-dependency: ""
[2022-02-21 16:42:47] [config] lemma-dim-emb: 0
[2022-02-21 16:42:47] [config] log: /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.train1.log
[2022-02-21 16:42:47] [config] log-level: info
[2022-02-21 16:42:47] [config] log-time-zone: ""
[2022-02-21 16:42:47] [config] logical-epoch:
[2022-02-21 16:42:47] [config]   - 1e
[2022-02-21 16:42:47] [config]   - 0
[2022-02-21 16:42:47] [config] lr-decay: 0
[2022-02-21 16:42:47] [config] lr-decay-freq: 50000
[2022-02-21 16:42:47] [config] lr-decay-inv-sqrt:
[2022-02-21 16:42:47] [config]   - 8000
[2022-02-21 16:42:47] [config] lr-decay-repeat-warmup: false
[2022-02-21 16:42:47] [config] lr-decay-reset-optimizer: false
[2022-02-21 16:42:47] [config] lr-decay-start:
[2022-02-21 16:42:47] [config]   - 10
[2022-02-21 16:42:47] [config]   - 1
[2022-02-21 16:42:47] [config] lr-decay-strategy: epoch+stalled
[2022-02-21 16:42:47] [config] lr-report: false
[2022-02-21 16:42:47] [config] lr-warmup: 8000
[2022-02-21 16:42:47] [config] lr-warmup-at-reload: false
[2022-02-21 16:42:47] [config] lr-warmup-cycle: false
[2022-02-21 16:42:47] [config] lr-warmup-start-rate: 0
[2022-02-21 16:42:47] [config] max-length: 100
[2022-02-21 16:42:47] [config] max-length-crop: false
[2022-02-21 16:42:47] [config] max-length-factor: 3
[2022-02-21 16:42:47] [config] maxi-batch: 1000
[2022-02-21 16:42:47] [config] maxi-batch-sort: trg
[2022-02-21 16:42:47] [config] mini-batch: 1000
[2022-02-21 16:42:47] [config] mini-batch-fit: true
[2022-02-21 16:42:47] [config] mini-batch-fit-step: 10
[2022-02-21 16:42:47] [config] mini-batch-round-up: true
[2022-02-21 16:42:47] [config] mini-batch-track-lr: false
[2022-02-21 16:42:47] [config] mini-batch-warmup: 0
[2022-02-21 16:42:47] [config] mini-batch-words: 0
[2022-02-21 16:42:47] [config] mini-batch-words-ref: 0
[2022-02-21 16:42:47] [config] model: /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-21 16:42:47] [config] multi-loss-type: sum
[2022-02-21 16:42:47] [config] n-best: false
[2022-02-21 16:42:47] [config] no-nccl: false
[2022-02-21 16:42:47] [config] no-reload: false
[2022-02-21 16:42:47] [config] no-restore-corpus: true
[2022-02-21 16:42:47] [config] normalize: 1
[2022-02-21 16:42:47] [config] normalize-gradient: false
[2022-02-21 16:42:47] [config] num-devices: 0
[2022-02-21 16:42:47] [config] optimizer: adam
[2022-02-21 16:42:47] [config] optimizer-delay: 2
[2022-02-21 16:42:47] [config] optimizer-params:
[2022-02-21 16:42:47] [config]   - 0.9
[2022-02-21 16:42:47] [config]   - 0.998
[2022-02-21 16:42:47] [config]   - 1e-09
[2022-02-21 16:42:47] [config] output-omit-bias: false
[2022-02-21 16:42:47] [config] overwrite: true
[2022-02-21 16:42:47] [config] precision:
[2022-02-21 16:42:47] [config]   - float32
[2022-02-21 16:42:47] [config]   - float32
[2022-02-21 16:42:47] [config] pretrained-model: ""
[2022-02-21 16:42:47] [config] quantize-biases: false
[2022-02-21 16:42:47] [config] quantize-bits: 0
[2022-02-21 16:42:47] [config] quantize-log-based: false
[2022-02-21 16:42:47] [config] quantize-optimization-steps: 0
[2022-02-21 16:42:47] [config] quiet: false
[2022-02-21 16:42:47] [config] quiet-translation: false
[2022-02-21 16:42:47] [config] relative-paths: false
[2022-02-21 16:42:47] [config] right-left: false
[2022-02-21 16:42:47] [config] save-freq: 10000
[2022-02-21 16:42:47] [config] seed: 1111
[2022-02-21 16:42:47] [config] sentencepiece-alphas:
[2022-02-21 16:42:47] [config]   []
[2022-02-21 16:42:47] [config] sentencepiece-max-lines: 2000000
[2022-02-21 16:42:47] [config] sentencepiece-options: ""
[2022-02-21 16:42:47] [config] sharding: local
[2022-02-21 16:42:47] [config] shuffle: batches
[2022-02-21 16:42:47] [config] shuffle-in-ram: false
[2022-02-21 16:42:47] [config] sigterm: save-and-exit
[2022-02-21 16:42:47] [config] skip: false
[2022-02-21 16:42:47] [config] sqlite: ""
[2022-02-21 16:42:47] [config] sqlite-drop: false
[2022-02-21 16:42:47] [config] sync-freq: 200u
[2022-02-21 16:42:47] [config] sync-sgd: true
[2022-02-21 16:42:47] [config] tempdir: /run/nvme/job_10668947/tmp
[2022-02-21 16:42:47] [config] tied-embeddings: false
[2022-02-21 16:42:47] [config] tied-embeddings-all: true
[2022-02-21 16:42:47] [config] tied-embeddings-src: false
[2022-02-21 16:42:47] [config] train-embedder-rank:
[2022-02-21 16:42:47] [config]   []
[2022-02-21 16:42:47] [config] train-sets:
[2022-02-21 16:42:47] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/train/opusTCv20210807+bt.src.clean.spm32k.gz
[2022-02-21 16:42:47] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/train/opusTCv20210807+bt.trg.clean.spm32k.gz
[2022-02-21 16:42:47] [config] transformer-aan-activation: swish
[2022-02-21 16:42:47] [config] transformer-aan-depth: 2
[2022-02-21 16:42:47] [config] transformer-aan-nogate: false
[2022-02-21 16:42:47] [config] transformer-decoder-autoreg: self-attention
[2022-02-21 16:42:47] [config] transformer-depth-scaling: false
[2022-02-21 16:42:47] [config] transformer-dim-aan: 2048
[2022-02-21 16:42:47] [config] transformer-dim-ffn: 4096
[2022-02-21 16:42:47] [config] transformer-dropout: 0.1
[2022-02-21 16:42:47] [config] transformer-dropout-attention: 0
[2022-02-21 16:42:47] [config] transformer-dropout-ffn: 0
[2022-02-21 16:42:47] [config] transformer-ffn-activation: relu
[2022-02-21 16:42:47] [config] transformer-ffn-depth: 2
[2022-02-21 16:42:47] [config] transformer-guided-alignment-layer: last
[2022-02-21 16:42:47] [config] transformer-heads: 16
[2022-02-21 16:42:47] [config] transformer-no-projection: false
[2022-02-21 16:42:47] [config] transformer-pool: false
[2022-02-21 16:42:47] [config] transformer-postprocess: dan
[2022-02-21 16:42:47] [config] transformer-postprocess-emb: d
[2022-02-21 16:42:47] [config] transformer-postprocess-top: ""
[2022-02-21 16:42:47] [config] transformer-preprocess: ""
[2022-02-21 16:42:47] [config] transformer-tied-layers:
[2022-02-21 16:42:47] [config]   []
[2022-02-21 16:42:47] [config] transformer-train-position-embeddings: false
[2022-02-21 16:42:47] [config] tsv: false
[2022-02-21 16:42:47] [config] tsv-fields: 0
[2022-02-21 16:42:47] [config] type: transformer
[2022-02-21 16:42:47] [config] ulr: false
[2022-02-21 16:42:47] [config] ulr-dim-emb: 0
[2022-02-21 16:42:47] [config] ulr-dropout: 0
[2022-02-21 16:42:47] [config] ulr-keys-vectors: ""
[2022-02-21 16:42:47] [config] ulr-query-vectors: ""
[2022-02-21 16:42:47] [config] ulr-softmax-temperature: 1
[2022-02-21 16:42:47] [config] ulr-trainable-transformation: false
[2022-02-21 16:42:47] [config] unlikelihood-loss: false
[2022-02-21 16:42:47] [config] valid-freq: 10000
[2022-02-21 16:42:47] [config] valid-log: /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.valid1.log
[2022-02-21 16:42:47] [config] valid-max-length: 100
[2022-02-21 16:42:47] [config] valid-metrics:
[2022-02-21 16:42:47] [config]   - perplexity
[2022-02-21 16:42:47] [config] valid-mini-batch: 16
[2022-02-21 16:42:47] [config] valid-reset-stalled: false
[2022-02-21 16:42:47] [config] valid-script-args:
[2022-02-21 16:42:47] [config]   []
[2022-02-21 16:42:47] [config] valid-script-path: ""
[2022-02-21 16:42:47] [config] valid-sets:
[2022-02-21 16:42:47] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/val/Tatoeba-dev-v2021-08-07.src.spm32k
[2022-02-21 16:42:47] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/val/Tatoeba-dev-v2021-08-07.trg.spm32k
[2022-02-21 16:42:47] [config] valid-translation-output: ""
[2022-02-21 16:42:47] [config] version: v1.10.24; 12a1bfa 2021-10-11 16:59:52 +0100
[2022-02-21 16:42:47] [config] vocabs:
[2022-02-21 16:42:47] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-02-21 16:42:47] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-02-21 16:42:47] [config] word-penalty: 0
[2022-02-21 16:42:47] [config] word-scores: false
[2022-02-21 16:42:47] [config] workspace: 15000
[2022-02-21 16:42:47] [config] Loaded model has been created with Marian v1.10.24; 12a1bfa 2021-10-11 16:59:52 +0100
[2022-02-21 16:42:47] Using synchronous SGD
[2022-02-21 16:42:47] [comm] Compiled without MPI support. Running as a single process on r18g08.bullx
[2022-02-21 16:42:47] Synced seed 1111
[2022-02-21 16:42:47] [data] Loading vocabulary from JSON/Yaml file /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-02-21 16:42:48] [data] Setting vocabulary size for input 0 to 55,026
[2022-02-21 16:42:48] [data] Loading vocabulary from JSON/Yaml file /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-02-21 16:42:48] [data] Setting vocabulary size for input 1 to 55,026
[2022-02-21 16:42:48] [batching] Collecting statistics for batch fitting with step size 10
[2022-02-21 16:42:49] [memory] Extending reserved space to 15104 MB (device gpu0)
[2022-02-21 16:42:51] [memory] Extending reserved space to 15104 MB (device gpu1)
[2022-02-21 16:42:51] [comm] Using NCCL 2.8.3 for GPU communication
[2022-02-21 16:42:51] [comm] Using global sharding
[2022-02-21 16:42:52] [comm] NCCLCommunicators constructed successfully
[2022-02-21 16:42:52] [training] Using 2 GPUs
[2022-02-21 16:42:52] [logits] Applying loss function for 1 factor(s)
[2022-02-21 16:42:52] [memory] Reserving 887 MB, device gpu0
[2022-02-21 16:42:55] [gpu] 16-bit TensorCores enabled for float32 matrix operations
[2022-02-21 16:42:55] [memory] Reserving 887 MB, device gpu0
[2022-02-21 16:43:23] [batching] Done. Typical MB size is 27,468 target words
[2022-02-21 16:43:23] [memory] Extending reserved space to 15104 MB (device gpu0)
[2022-02-21 16:43:23] [memory] Extending reserved space to 15104 MB (device gpu1)
[2022-02-21 16:43:23] [comm] Using NCCL 2.8.3 for GPU communication
[2022-02-21 16:43:23] [comm] Using global sharding
[2022-02-21 16:43:25] [comm] NCCLCommunicators constructed successfully
[2022-02-21 16:43:25] [training] Using 2 GPUs
[2022-02-21 16:43:25] Loading model from /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-21 16:43:27] Loading model from /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-21 16:43:35] Allocating memory for general optimizer shards
[2022-02-21 16:43:35] [memory] Reserving 443 MB, device gpu0
[2022-02-21 16:43:36] [memory] Reserving 443 MB, device gpu1
[2022-02-21 16:43:36] Loading Adam parameters
[2022-02-21 16:43:36] [memory] Reserving 887 MB, device gpu0
[2022-02-21 16:43:36] [memory] Reserving 887 MB, device gpu1
[2022-02-21 16:43:37] [memory] Reserving 887 MB, device gpu0
[2022-02-21 16:43:37] [memory] Reserving 887 MB, device gpu1
[2022-02-21 16:43:38] [training] Master parameters and optimizers restored from training checkpoint /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-21 16:43:38] Training started
[2022-02-21 16:44:01] [training] Batches are processed as 1 process(es) x 2 devices/process
[2022-02-21 16:44:02] [memory] Reserving 887 MB, device gpu0
[2022-02-21 16:44:02] [memory] Reserving 887 MB, device gpu1
[2022-02-21 16:44:04] Parameter type float32, optimization type float32, casting types false
[2022-02-21 20:59:43] Ep. 1 : Up. 330000 : Sen. 8,809,207 : Cost 2.34621310 : Time 15379.43s : 14766.31 words/s : gNorm 0.4726
[2022-02-21 20:59:43] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-21 20:59:48] Saving Adam parameters
[2022-02-21 20:59:52] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-21 21:00:09] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-21 21:00:12] [valid] Ep. 1 : Up. 330000 : perplexity : 2.05849 : new best
[2022-02-22 01:16:12] Ep. 1 : Up. 340000 : Sen. 17,604,389 : Cost 2.34277821 : Time 15388.91s : 14755.26 words/s : gNorm 0.4879
[2022-02-22 01:16:12] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-22 01:16:16] Saving Adam parameters
[2022-02-22 01:16:19] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-22 01:16:34] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-22 01:16:37] [valid] Ep. 1 : Up. 340000 : perplexity : 2.05708 : new best
[2022-02-22 05:31:59] Ep. 1 : Up. 350000 : Sen. 26,392,815 : Cost 2.34214163 : Time 15347.40s : 14789.69 words/s : gNorm 0.4821
[2022-02-22 05:31:59] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-22 05:32:04] Saving Adam parameters
[2022-02-22 05:32:08] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-22 05:32:18] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-22 05:32:20] [valid] Ep. 1 : Up. 350000 : perplexity : 2.05545 : new best
[2022-02-22 09:47:24] Ep. 1 : Up. 360000 : Sen. 35,198,052 : Cost 2.33937311 : Time 15325.05s : 14820.81 words/s : gNorm 0.4637
[2022-02-22 09:47:24] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-22 09:47:29] Saving Adam parameters
[2022-02-22 09:47:33] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-22 09:47:49] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-22 09:47:52] [valid] Ep. 1 : Up. 360000 : perplexity : 2.05503 : new best
[2022-02-22 14:02:47] Ep. 1 : Up. 370000 : Sen. 44,010,124 : Cost 2.33841109 : Time 15322.91s : 14821.38 words/s : gNorm 0.5011
[2022-02-22 14:02:47] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-22 14:02:56] Saving Adam parameters
[2022-02-22 14:03:02] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-22 14:03:24] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-22 14:03:28] [valid] Ep. 1 : Up. 370000 : perplexity : 2.05295 : new best
[2022-02-22 18:18:35] Ep. 1 : Up. 380000 : Sen. 52,814,651 : Cost 2.33715415 : Time 15347.87s : 14787.89 words/s : gNorm 0.5062
[2022-02-22 18:18:35] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-22 18:18:40] Saving Adam parameters
[2022-02-22 18:18:43] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-22 18:18:54] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-22 18:18:56] [valid] Ep. 1 : Up. 380000 : perplexity : 2.0512 : new best
[2022-02-22 22:34:31] Ep. 1 : Up. 390000 : Sen. 61,603,455 : Cost 2.33511519 : Time 15355.97s : 14794.33 words/s : gNorm 0.5047
[2022-02-22 22:34:31] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-22 22:34:38] Saving Adam parameters
[2022-02-22 22:34:41] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-22 22:34:52] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-22 22:34:56] [valid] Ep. 1 : Up. 390000 : perplexity : 2.04983 : new best
[2022-02-23 02:50:30] Ep. 1 : Up. 400000 : Sen. 70,427,669 : Cost 2.33417797 : Time 15358.32s : 14779.25 words/s : gNorm 0.5011
[2022-02-23 02:50:30] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-23 02:50:33] Saving Adam parameters
[2022-02-23 02:50:37] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-23 02:50:47] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-23 02:50:50] [valid] Ep. 1 : Up. 400000 : perplexity : 2.04937 : new best
[2022-02-23 07:06:24] Ep. 1 : Up. 410000 : Sen. 79,211,349 : Cost 2.33228898 : Time 15354.30s : 14797.31 words/s : gNorm 0.5075
[2022-02-23 07:06:24] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-23 07:06:28] Saving Adam parameters
[2022-02-23 07:06:31] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-23 07:06:42] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-23 07:06:44] [valid] Ep. 1 : Up. 410000 : perplexity : 2.04871 : new best
[2022-02-23 11:22:13] Ep. 1 : Up. 420000 : Sen. 88,016,176 : Cost 2.33128142 : Time 15348.85s : 14797.25 words/s : gNorm 0.5124
[2022-02-23 11:22:13] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-23 11:22:21] Saving Adam parameters
[2022-02-23 11:22:28] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-23 11:22:54] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-23 11:22:58] [valid] Ep. 1 : Up. 420000 : perplexity : 2.04682 : new best
[2022-02-23 15:38:36] Ep. 1 : Up. 430000 : Sen. 96,828,380 : Cost 2.33059788 : Time 15383.14s : 14764.54 words/s : gNorm 0.5322
[2022-02-23 15:38:36] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-23 15:38:41] Saving Adam parameters
[2022-02-23 15:38:45] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-23 15:38:56] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-23 15:38:59] [valid] Ep. 1 : Up. 430000 : perplexity : 2.04652 : new best
[2022-02-23 19:55:13] Ep. 1 : Up. 440000 : Sen. 105,605,925 : Cost 2.32827044 : Time 15396.78s : 14750.77 words/s : gNorm 0.4986
[2022-02-23 19:55:13] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-23 19:55:17] Saving Adam parameters
[2022-02-23 19:55:21] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-23 19:55:31] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-23 19:55:33] [valid] Ep. 1 : Up. 440000 : perplexity : 2.04415 : new best
[2022-02-24 00:17:12] Ep. 1 : Up. 450000 : Sen. 114,405,705 : Cost 2.32806754 : Time 15719.19s : 14439.16 words/s : gNorm 0.5203
[2022-02-24 00:17:12] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-24 00:17:19] Saving Adam parameters
[2022-02-24 00:17:22] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-24 00:17:33] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-24 00:17:35] [valid] Ep. 1 : Up. 450000 : perplexity : 2.04306 : new best
[2022-02-24 04:33:10] Ep. 1 : Up. 460000 : Sen. 123,216,854 : Cost 2.32726169 : Time 15358.05s : 14790.45 words/s : gNorm 0.5132
[2022-02-24 04:33:11] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-24 04:33:15] Saving Adam parameters
[2022-02-24 04:33:18] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-24 04:33:28] [valid] Ep. 1 : Up. 460000 : perplexity : 2.0434 : stalled 1 times (last best: 2.04306)
[2022-02-24 08:49:24] Ep. 1 : Up. 470000 : Sen. 132,035,340 : Cost 2.32612348 : Time 15373.08s : 14781.04 words/s : gNorm 0.5118
[2022-02-24 08:49:24] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-24 08:49:28] Saving Adam parameters
[2022-02-24 08:49:31] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-24 08:49:42] [valid] Ep. 1 : Up. 470000 : perplexity : 2.04367 : stalled 2 times (last best: 2.04306)
[2022-02-24 13:05:05] Ep. 1 : Up. 480000 : Sen. 140,844,597 : Cost 2.32561445 : Time 15341.19s : 14793.54 words/s : gNorm 0.5004
[2022-02-24 13:05:05] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-24 13:05:08] Saving Adam parameters
[2022-02-24 13:05:12] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-24 13:05:32] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-24 13:05:34] [valid] Ep. 1 : Up. 480000 : perplexity : 2.04227 : new best
[2022-02-24 17:29:04] [marian] Marian v1.10.24; 12a1bfa 2021-10-11 16:59:52 +0100
[2022-02-24 17:29:04] [marian] Running on r18g08.bullx as process 21019 with command line:
[2022-02-24 17:29:04] [marian] /projappl/project_2001194/marian-dev/build/marian --task transformer-big --optimizer-delay 2 --no-restore-corpus --early-stopping 10 --valid-freq 10000 --valid-sets /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/val/Tatoeba-dev-v2021-08-07.src.spm32k /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/val/Tatoeba-dev-v2021-08-07.trg.spm32k --valid-metrics perplexity --valid-mini-batch 16 --valid-max-length 100 --valid-log /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.valid1.log --beam-size 6 --normalize 1 --allow-unk --workspace 15000 --model /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz --train-sets /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/train/opusTCv20210807+bt.src.clean.spm32k.gz /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/train/opusTCv20210807+bt.trg.clean.spm32k.gz --vocabs /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.vocab.yml /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.vocab.yml --save-freq 10000 --disp-freq 10000 --log /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.train1.log --devices 0 1 --seed 1111 --tempdir /run/nvme/job_10712061/tmp --shuffle batches --sharding local --overwrite --keep-best
[2022-02-24 18:03:39] [config] after: 0e
[2022-02-24 18:03:39] [config] after-batches: 0
[2022-02-24 18:03:39] [config] after-epochs: 0
[2022-02-24 18:03:39] [config] all-caps-every: 0
[2022-02-24 18:03:39] [config] allow-unk: true
[2022-02-24 18:03:39] [config] authors: false
[2022-02-24 18:03:39] [config] beam-size: 6
[2022-02-24 18:03:39] [config] bert-class-symbol: "[CLS]"
[2022-02-24 18:03:39] [config] bert-mask-symbol: "[MASK]"
[2022-02-24 18:03:39] [config] bert-masking-fraction: 0.15
[2022-02-24 18:03:39] [config] bert-sep-symbol: "[SEP]"
[2022-02-24 18:03:39] [config] bert-train-type-embeddings: true
[2022-02-24 18:03:39] [config] bert-type-vocab-size: 2
[2022-02-24 18:03:39] [config] build-info: ""
[2022-02-24 18:03:39] [config] check-gradient-nan: false
[2022-02-24 18:03:39] [config] check-nan: false
[2022-02-24 18:03:39] [config] cite: false
[2022-02-24 18:03:39] [config] clip-norm: 0
[2022-02-24 18:03:39] [config] cost-scaling:
[2022-02-24 18:03:39] [config]   []
[2022-02-24 18:03:39] [config] cost-type: ce-mean-words
[2022-02-24 18:03:39] [config] cpu-threads: 0
[2022-02-24 18:03:39] [config] data-weighting: ""
[2022-02-24 18:03:39] [config] data-weighting-type: sentence
[2022-02-24 18:03:39] [config] dec-cell: gru
[2022-02-24 18:03:39] [config] dec-cell-base-depth: 2
[2022-02-24 18:03:39] [config] dec-cell-high-depth: 1
[2022-02-24 18:03:39] [config] dec-depth: 6
[2022-02-24 18:03:39] [config] devices:
[2022-02-24 18:03:39] [config]   - 0
[2022-02-24 18:03:39] [config]   - 1
[2022-02-24 18:03:39] [config] dim-emb: 1024
[2022-02-24 18:03:39] [config] dim-rnn: 1024
[2022-02-24 18:03:39] [config] dim-vocabs:
[2022-02-24 18:03:39] [config]   - 55026
[2022-02-24 18:03:39] [config]   - 55026
[2022-02-24 18:03:39] [config] disp-first: 0
[2022-02-24 18:03:39] [config] disp-freq: 10000
[2022-02-24 18:03:39] [config] disp-label-counts: true
[2022-02-24 18:03:39] [config] dropout-rnn: 0
[2022-02-24 18:03:39] [config] dropout-src: 0
[2022-02-24 18:03:39] [config] dropout-trg: 0
[2022-02-24 18:03:39] [config] dump-config: ""
[2022-02-24 18:03:39] [config] dynamic-gradient-scaling:
[2022-02-24 18:03:39] [config]   []
[2022-02-24 18:03:39] [config] early-stopping: 10
[2022-02-24 18:03:39] [config] early-stopping-on: first
[2022-02-24 18:03:39] [config] embedding-fix-src: false
[2022-02-24 18:03:39] [config] embedding-fix-trg: false
[2022-02-24 18:03:39] [config] embedding-normalization: false
[2022-02-24 18:03:39] [config] embedding-vectors:
[2022-02-24 18:03:39] [config]   []
[2022-02-24 18:03:39] [config] enc-cell: gru
[2022-02-24 18:03:39] [config] enc-cell-depth: 1
[2022-02-24 18:03:39] [config] enc-depth: 6
[2022-02-24 18:03:39] [config] enc-type: bidirectional
[2022-02-24 18:03:39] [config] english-title-case-every: 0
[2022-02-24 18:03:39] [config] exponential-smoothing: 0.0001
[2022-02-24 18:03:39] [config] factor-weight: 1
[2022-02-24 18:03:39] [config] factors-combine: sum
[2022-02-24 18:03:39] [config] factors-dim-emb: 0
[2022-02-24 18:03:39] [config] gradient-checkpointing: false
[2022-02-24 18:03:39] [config] gradient-norm-average-window: 100
[2022-02-24 18:03:39] [config] guided-alignment: none
[2022-02-24 18:03:39] [config] guided-alignment-cost: mse
[2022-02-24 18:03:39] [config] guided-alignment-weight: 0.1
[2022-02-24 18:03:39] [config] ignore-model-config: false
[2022-02-24 18:03:39] [config] input-types:
[2022-02-24 18:03:39] [config]   []
[2022-02-24 18:03:39] [config] interpolate-env-vars: false
[2022-02-24 18:03:39] [config] keep-best: true
[2022-02-24 18:03:39] [config] label-smoothing: 0.1
[2022-02-24 18:03:39] [config] layer-normalization: false
[2022-02-24 18:03:39] [config] learn-rate: 0.0002
[2022-02-24 18:03:39] [config] lemma-dependency: ""
[2022-02-24 18:03:39] [config] lemma-dim-emb: 0
[2022-02-24 18:03:39] [config] log: /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.train1.log
[2022-02-24 18:03:39] [config] log-level: info
[2022-02-24 18:03:39] [config] log-time-zone: ""
[2022-02-24 18:03:39] [config] logical-epoch:
[2022-02-24 18:03:39] [config]   - 1e
[2022-02-24 18:03:39] [config]   - 0
[2022-02-24 18:03:39] [config] lr-decay: 0
[2022-02-24 18:03:39] [config] lr-decay-freq: 50000
[2022-02-24 18:03:39] [config] lr-decay-inv-sqrt:
[2022-02-24 18:03:39] [config]   - 8000
[2022-02-24 18:03:39] [config] lr-decay-repeat-warmup: false
[2022-02-24 18:03:39] [config] lr-decay-reset-optimizer: false
[2022-02-24 18:03:39] [config] lr-decay-start:
[2022-02-24 18:03:39] [config]   - 10
[2022-02-24 18:03:39] [config]   - 1
[2022-02-24 18:03:39] [config] lr-decay-strategy: epoch+stalled
[2022-02-24 18:03:39] [config] lr-report: false
[2022-02-24 18:03:39] [config] lr-warmup: 8000
[2022-02-24 18:03:39] [config] lr-warmup-at-reload: false
[2022-02-24 18:03:39] [config] lr-warmup-cycle: false
[2022-02-24 18:03:39] [config] lr-warmup-start-rate: 0
[2022-02-24 18:03:39] [config] max-length: 100
[2022-02-24 18:03:39] [config] max-length-crop: false
[2022-02-24 18:03:39] [config] max-length-factor: 3
[2022-02-24 18:03:39] [config] maxi-batch: 1000
[2022-02-24 18:03:39] [config] maxi-batch-sort: trg
[2022-02-24 18:03:39] [config] mini-batch: 1000
[2022-02-24 18:03:39] [config] mini-batch-fit: true
[2022-02-24 18:03:39] [config] mini-batch-fit-step: 10
[2022-02-24 18:03:39] [config] mini-batch-round-up: true
[2022-02-24 18:03:39] [config] mini-batch-track-lr: false
[2022-02-24 18:03:39] [config] mini-batch-warmup: 0
[2022-02-24 18:03:39] [config] mini-batch-words: 0
[2022-02-24 18:03:39] [config] mini-batch-words-ref: 0
[2022-02-24 18:03:39] [config] model: /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-24 18:03:39] [config] multi-loss-type: sum
[2022-02-24 18:03:39] [config] n-best: false
[2022-02-24 18:03:39] [config] no-nccl: false
[2022-02-24 18:03:39] [config] no-reload: false
[2022-02-24 18:03:39] [config] no-restore-corpus: true
[2022-02-24 18:03:39] [config] normalize: 1
[2022-02-24 18:03:39] [config] normalize-gradient: false
[2022-02-24 18:03:39] [config] num-devices: 0
[2022-02-24 18:03:39] [config] optimizer: adam
[2022-02-24 18:03:39] [config] optimizer-delay: 2
[2022-02-24 18:03:39] [config] optimizer-params:
[2022-02-24 18:03:39] [config]   - 0.9
[2022-02-24 18:03:39] [config]   - 0.998
[2022-02-24 18:03:39] [config]   - 1e-09
[2022-02-24 18:03:39] [config] output-omit-bias: false
[2022-02-24 18:03:39] [config] overwrite: true
[2022-02-24 18:03:39] [config] precision:
[2022-02-24 18:03:39] [config]   - float32
[2022-02-24 18:03:39] [config]   - float32
[2022-02-24 18:03:39] [config] pretrained-model: ""
[2022-02-24 18:03:39] [config] quantize-biases: false
[2022-02-24 18:03:39] [config] quantize-bits: 0
[2022-02-24 18:03:39] [config] quantize-log-based: false
[2022-02-24 18:03:39] [config] quantize-optimization-steps: 0
[2022-02-24 18:03:39] [config] quiet: false
[2022-02-24 18:03:39] [config] quiet-translation: false
[2022-02-24 18:03:39] [config] relative-paths: false
[2022-02-24 18:03:39] [config] right-left: false
[2022-02-24 18:03:39] [config] save-freq: 10000
[2022-02-24 18:03:39] [config] seed: 1111
[2022-02-24 18:03:39] [config] sentencepiece-alphas:
[2022-02-24 18:03:39] [config]   []
[2022-02-24 18:03:39] [config] sentencepiece-max-lines: 2000000
[2022-02-24 18:03:39] [config] sentencepiece-options: ""
[2022-02-24 18:03:39] [config] sharding: local
[2022-02-24 18:03:39] [config] shuffle: batches
[2022-02-24 18:03:39] [config] shuffle-in-ram: false
[2022-02-24 18:03:39] [config] sigterm: save-and-exit
[2022-02-24 18:03:39] [config] skip: false
[2022-02-24 18:03:39] [config] sqlite: ""
[2022-02-24 18:03:39] [config] sqlite-drop: false
[2022-02-24 18:03:39] [config] sync-freq: 200u
[2022-02-24 18:03:39] [config] sync-sgd: true
[2022-02-24 18:03:39] [config] tempdir: /run/nvme/job_10712061/tmp
[2022-02-24 18:03:39] [config] tied-embeddings: false
[2022-02-24 18:03:39] [config] tied-embeddings-all: true
[2022-02-24 18:03:39] [config] tied-embeddings-src: false
[2022-02-24 18:03:39] [config] train-embedder-rank:
[2022-02-24 18:03:39] [config]   []
[2022-02-24 18:03:39] [config] train-sets:
[2022-02-24 18:03:39] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/train/opusTCv20210807+bt.src.clean.spm32k.gz
[2022-02-24 18:03:39] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/train/opusTCv20210807+bt.trg.clean.spm32k.gz
[2022-02-24 18:03:39] [config] transformer-aan-activation: swish
[2022-02-24 18:03:39] [config] transformer-aan-depth: 2
[2022-02-24 18:03:39] [config] transformer-aan-nogate: false
[2022-02-24 18:03:39] [config] transformer-decoder-autoreg: self-attention
[2022-02-24 18:03:39] [config] transformer-depth-scaling: false
[2022-02-24 18:03:39] [config] transformer-dim-aan: 2048
[2022-02-24 18:03:39] [config] transformer-dim-ffn: 4096
[2022-02-24 18:03:39] [config] transformer-dropout: 0.1
[2022-02-24 18:03:39] [config] transformer-dropout-attention: 0
[2022-02-24 18:03:39] [config] transformer-dropout-ffn: 0
[2022-02-24 18:03:39] [config] transformer-ffn-activation: relu
[2022-02-24 18:03:39] [config] transformer-ffn-depth: 2
[2022-02-24 18:03:39] [config] transformer-guided-alignment-layer: last
[2022-02-24 18:03:39] [config] transformer-heads: 16
[2022-02-24 18:03:39] [config] transformer-no-projection: false
[2022-02-24 18:03:39] [config] transformer-pool: false
[2022-02-24 18:03:39] [config] transformer-postprocess: dan
[2022-02-24 18:03:39] [config] transformer-postprocess-emb: d
[2022-02-24 18:03:39] [config] transformer-postprocess-top: ""
[2022-02-24 18:03:39] [config] transformer-preprocess: ""
[2022-02-24 18:03:39] [config] transformer-tied-layers:
[2022-02-24 18:03:39] [config]   []
[2022-02-24 18:03:39] [config] transformer-train-position-embeddings: false
[2022-02-24 18:03:39] [config] tsv: false
[2022-02-24 18:03:39] [config] tsv-fields: 0
[2022-02-24 18:03:39] [config] type: transformer
[2022-02-24 18:03:39] [config] ulr: false
[2022-02-24 18:03:39] [config] ulr-dim-emb: 0
[2022-02-24 18:03:39] [config] ulr-dropout: 0
[2022-02-24 18:03:39] [config] ulr-keys-vectors: ""
[2022-02-24 18:03:39] [config] ulr-query-vectors: ""
[2022-02-24 18:03:39] [config] ulr-softmax-temperature: 1
[2022-02-24 18:03:39] [config] ulr-trainable-transformation: false
[2022-02-24 18:03:39] [config] unlikelihood-loss: false
[2022-02-24 18:03:39] [config] valid-freq: 10000
[2022-02-24 18:03:39] [config] valid-log: /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.valid1.log
[2022-02-24 18:03:39] [config] valid-max-length: 100
[2022-02-24 18:03:39] [config] valid-metrics:
[2022-02-24 18:03:39] [config]   - perplexity
[2022-02-24 18:03:39] [config] valid-mini-batch: 16
[2022-02-24 18:03:39] [config] valid-reset-stalled: false
[2022-02-24 18:03:39] [config] valid-script-args:
[2022-02-24 18:03:39] [config]   []
[2022-02-24 18:03:39] [config] valid-script-path: ""
[2022-02-24 18:03:39] [config] valid-sets:
[2022-02-24 18:03:39] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/val/Tatoeba-dev-v2021-08-07.src.spm32k
[2022-02-24 18:03:39] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/val/Tatoeba-dev-v2021-08-07.trg.spm32k
[2022-02-24 18:03:39] [config] valid-translation-output: ""
[2022-02-24 18:03:39] [config] version: v1.10.24; 12a1bfa 2021-10-11 16:59:52 +0100
[2022-02-24 18:03:39] [config] vocabs:
[2022-02-24 18:03:39] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-02-24 18:03:39] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-02-24 18:03:39] [config] word-penalty: 0
[2022-02-24 18:03:39] [config] word-scores: false
[2022-02-24 18:03:39] [config] workspace: 15000
[2022-02-24 18:03:39] [config] Loaded model has been created with Marian v1.10.24; 12a1bfa 2021-10-11 16:59:52 +0100
[2022-02-24 18:03:39] Using synchronous SGD
[2022-02-24 18:03:39] [comm] Compiled without MPI support. Running as a single process on r18g08.bullx
[2022-02-24 18:03:39] Synced seed 1111
[2022-02-24 18:03:39] [data] Loading vocabulary from JSON/Yaml file /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-02-24 18:03:39] [data] Setting vocabulary size for input 0 to 55,026
[2022-02-24 18:03:39] [data] Loading vocabulary from JSON/Yaml file /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-02-24 18:03:39] [data] Setting vocabulary size for input 1 to 55,026
[2022-02-24 18:03:39] [batching] Collecting statistics for batch fitting with step size 10
[2022-02-24 18:03:40] [memory] Extending reserved space to 15104 MB (device gpu0)
[2022-02-24 18:03:43] [memory] Extending reserved space to 15104 MB (device gpu1)
[2022-02-24 18:03:43] [comm] Using NCCL 2.8.3 for GPU communication
[2022-02-24 18:03:43] [comm] Using global sharding
[2022-02-24 18:03:46] [comm] NCCLCommunicators constructed successfully
[2022-02-24 18:03:46] [training] Using 2 GPUs
[2022-02-24 18:03:46] [logits] Applying loss function for 1 factor(s)
[2022-02-24 18:03:46] [memory] Reserving 887 MB, device gpu0
[2022-02-24 18:03:47] [gpu] 16-bit TensorCores enabled for float32 matrix operations
[2022-02-24 18:03:47] [memory] Reserving 887 MB, device gpu0
[2022-02-24 18:04:16] [batching] Done. Typical MB size is 27,468 target words
[2022-02-24 18:04:16] [memory] Extending reserved space to 15104 MB (device gpu0)
[2022-02-24 18:04:16] [memory] Extending reserved space to 15104 MB (device gpu1)
[2022-02-24 18:04:16] [comm] Using NCCL 2.8.3 for GPU communication
[2022-02-24 18:04:16] [comm] Using global sharding
[2022-02-24 18:04:17] [comm] NCCLCommunicators constructed successfully
[2022-02-24 18:04:17] [training] Using 2 GPUs
[2022-02-24 18:04:17] Loading model from /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-24 18:07:42] Loading model from /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-24 18:07:51] Allocating memory for general optimizer shards
[2022-02-24 18:07:51] [memory] Reserving 443 MB, device gpu0
[2022-02-24 18:07:51] [memory] Reserving 443 MB, device gpu1
[2022-02-24 18:07:51] Loading Adam parameters
[2022-02-24 18:07:51] [memory] Reserving 887 MB, device gpu0
[2022-02-24 18:07:52] [memory] Reserving 887 MB, device gpu1
[2022-02-24 18:07:52] [memory] Reserving 887 MB, device gpu0
[2022-02-24 18:07:52] [memory] Reserving 887 MB, device gpu1
[2022-02-24 18:07:53] [training] Master parameters and optimizers restored from training checkpoint /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-24 18:07:53] Training started
[2022-02-24 18:08:15] [training] Batches are processed as 1 process(es) x 2 devices/process
[2022-02-24 18:08:16] [memory] Reserving 887 MB, device gpu0
[2022-02-24 18:08:16] [memory] Reserving 887 MB, device gpu1
[2022-02-24 18:08:18] Parameter type float32, optimization type float32, casting types false
[2022-02-24 22:23:45] Ep. 1 : Up. 490000 : Sen. 8,809,207 : Cost 2.32111311 : Time 15569.08s : 14586.44 words/s : gNorm 0.5123
[2022-02-24 22:23:45] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-24 22:23:49] Saving Adam parameters
[2022-02-24 22:23:53] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-24 22:24:03] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-24 22:24:05] [valid] Ep. 1 : Up. 490000 : perplexity : 2.04041 : new best
[2022-02-25 02:39:14] Ep. 1 : Up. 500000 : Sen. 17,604,389 : Cost 2.31866550 : Time 15329.46s : 14812.49 words/s : gNorm 0.5297
[2022-02-25 02:39:14] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-25 02:39:18] Saving Adam parameters
[2022-02-25 02:39:21] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-25 02:39:32] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-25 02:39:34] [valid] Ep. 1 : Up. 500000 : perplexity : 2.03924 : new best
[2022-02-25 06:54:49] Ep. 1 : Up. 510000 : Sen. 26,392,815 : Cost 2.31883240 : Time 15334.32s : 14802.31 words/s : gNorm 0.5200
[2022-02-25 06:54:49] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-25 06:54:54] Saving Adam parameters
[2022-02-25 06:54:57] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-25 06:55:17] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-25 06:55:19] [valid] Ep. 1 : Up. 510000 : perplexity : 2.0388 : new best
[2022-02-25 11:10:49] Ep. 1 : Up. 520000 : Sen. 35,198,052 : Cost 2.31685519 : Time 15360.47s : 14786.64 words/s : gNorm 0.4979
[2022-02-25 11:10:49] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-25 11:10:53] Saving Adam parameters
[2022-02-25 11:10:56] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-25 11:11:07] [valid] Ep. 1 : Up. 520000 : perplexity : 2.03901 : stalled 1 times (last best: 2.0388)
[2022-02-25 15:26:11] Ep. 1 : Up. 530000 : Sen. 44,010,124 : Cost 2.31652403 : Time 15321.45s : 14822.79 words/s : gNorm 0.5402
[2022-02-25 15:26:11] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-25 15:26:18] Saving Adam parameters
[2022-02-25 15:26:25] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-25 15:26:52] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-25 15:26:54] [valid] Ep. 1 : Up. 530000 : perplexity : 2.03781 : new best
[2022-02-25 19:41:49] Ep. 1 : Up. 540000 : Sen. 52,814,651 : Cost 2.31592965 : Time 15338.38s : 14797.04 words/s : gNorm 0.5443
[2022-02-25 19:41:49] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-25 19:41:54] Saving Adam parameters
[2022-02-25 19:41:57] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-25 19:42:09] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-25 19:42:12] [valid] Ep. 1 : Up. 540000 : perplexity : 2.03726 : new best
[2022-02-25 23:57:39] Ep. 1 : Up. 550000 : Sen. 61,603,455 : Cost 2.31449914 : Time 15349.83s : 14800.24 words/s : gNorm 0.5453
[2022-02-25 23:57:39] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-25 23:57:43] Saving Adam parameters
[2022-02-25 23:57:46] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-25 23:57:56] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-25 23:57:59] [valid] Ep. 1 : Up. 550000 : perplexity : 2.03607 : new best
[2022-02-26 04:13:17] Ep. 1 : Up. 560000 : Sen. 70,427,669 : Cost 2.31408763 : Time 15337.20s : 14799.60 words/s : gNorm 0.5364
[2022-02-26 04:13:17] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-26 04:13:20] Saving Adam parameters
[2022-02-26 04:13:24] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-26 04:13:35] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-26 04:13:37] [valid] Ep. 1 : Up. 560000 : perplexity : 2.03607 : new best
[2022-02-26 08:28:52] Ep. 1 : Up. 570000 : Sen. 79,211,349 : Cost 2.31272769 : Time 15335.28s : 14815.66 words/s : gNorm 0.5441
[2022-02-26 08:28:52] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-26 08:28:56] Saving Adam parameters
[2022-02-26 08:28:59] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-26 08:29:10] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-26 08:29:12] [valid] Ep. 1 : Up. 570000 : perplexity : 2.03599 : new best
[2022-02-26 12:44:07] Ep. 1 : Up. 580000 : Sen. 88,016,176 : Cost 2.31221962 : Time 15315.22s : 14829.75 words/s : gNorm 0.5490
[2022-02-26 12:44:08] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-26 12:44:15] Saving Adam parameters
[2022-02-26 12:44:21] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-26 12:44:41] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-26 12:44:46] [valid] Ep. 1 : Up. 580000 : perplexity : 2.03424 : new best
[2022-02-26 16:59:45] Ep. 1 : Up. 590000 : Sen. 96,828,380 : Cost 2.31198359 : Time 15337.25s : 14808.71 words/s : gNorm 0.5701
[2022-02-26 16:59:45] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-26 16:59:48] Saving Adam parameters
[2022-02-26 16:59:51] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-26 17:00:02] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-26 17:00:05] [valid] Ep. 1 : Up. 590000 : perplexity : 2.0337 : new best
[2022-02-26 21:15:14] Ep. 1 : Up. 600000 : Sen. 105,605,925 : Cost 2.31011224 : Time 15328.82s : 14816.17 words/s : gNorm 0.5282
[2022-02-26 21:15:14] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-26 21:15:17] Saving Adam parameters
[2022-02-26 21:15:21] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-26 21:15:36] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-26 21:15:39] [valid] Ep. 1 : Up. 600000 : perplexity : 2.03182 : new best
[2022-02-27 01:30:46] Ep. 1 : Up. 610000 : Sen. 114,405,705 : Cost 2.31032109 : Time 15332.03s : 14803.78 words/s : gNorm 0.5548
[2022-02-27 01:30:46] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-27 01:30:49] Saving Adam parameters
[2022-02-27 01:30:52] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-27 01:31:03] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-27 01:31:08] [valid] Ep. 1 : Up. 610000 : perplexity : 2.03161 : new best
[2022-02-27 05:46:30] Ep. 1 : Up. 620000 : Sen. 123,216,854 : Cost 2.30993152 : Time 15344.39s : 14803.62 words/s : gNorm 0.5478
[2022-02-27 05:46:30] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-27 05:46:35] Saving Adam parameters
[2022-02-27 05:46:38] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-27 05:46:49] [valid] Ep. 1 : Up. 620000 : perplexity : 2.03167 : stalled 1 times (last best: 2.03161)
[2022-02-27 10:02:32] Ep. 1 : Up. 630000 : Sen. 132,035,340 : Cost 2.30915427 : Time 15361.21s : 14792.47 words/s : gNorm 0.5458
[2022-02-27 10:02:32] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-27 10:02:36] Saving Adam parameters
[2022-02-27 10:02:39] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-27 10:03:06] [valid] Ep. 1 : Up. 630000 : perplexity : 2.03329 : stalled 2 times (last best: 2.03161)
[2022-02-27 14:18:01] Ep. 1 : Up. 640000 : Sen. 140,844,597 : Cost 2.30900574 : Time 15329.43s : 14804.89 words/s : gNorm 0.5330
[2022-02-27 14:18:01] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-27 14:18:05] Saving Adam parameters
[2022-02-27 14:18:08] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-27 14:18:20] [valid] Ep. 1 : Up. 640000 : perplexity : 2.03197 : stalled 3 times (last best: 2.03161)
[2022-02-27 16:49:49] [marian] Marian v1.10.24; 12a1bfa 2021-10-11 16:59:52 +0100
[2022-02-27 16:49:49] [marian] Running on r18g01.bullx as process 100702 with command line:
[2022-02-27 16:49:49] [marian] /projappl/project_2001194/marian-dev/build/marian --task transformer-big --optimizer-delay 2 --no-restore-corpus --early-stopping 10 --valid-freq 10000 --valid-sets /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/val/Tatoeba-dev-v2021-08-07.src.spm32k /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/val/Tatoeba-dev-v2021-08-07.trg.spm32k --valid-metrics perplexity --valid-mini-batch 16 --valid-max-length 100 --valid-log /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.valid1.log --beam-size 6 --normalize 1 --allow-unk --workspace 15000 --model /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz --train-sets /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/train/opusTCv20210807+bt.src.clean.spm32k.gz /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/train/opusTCv20210807+bt.trg.clean.spm32k.gz --vocabs /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.vocab.yml /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.vocab.yml --save-freq 10000 --disp-freq 10000 --log /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.train1.log --devices 0 1 --seed 1111 --tempdir /run/nvme/job_10769183/tmp --shuffle batches --sharding local --overwrite --keep-best
[2022-02-27 16:49:50] [config] after: 0e
[2022-02-27 16:49:50] [config] after-batches: 0
[2022-02-27 16:49:50] [config] after-epochs: 0
[2022-02-27 16:49:50] [config] all-caps-every: 0
[2022-02-27 16:49:50] [config] allow-unk: true
[2022-02-27 16:49:50] [config] authors: false
[2022-02-27 16:49:50] [config] beam-size: 6
[2022-02-27 16:49:50] [config] bert-class-symbol: "[CLS]"
[2022-02-27 16:49:50] [config] bert-mask-symbol: "[MASK]"
[2022-02-27 16:49:50] [config] bert-masking-fraction: 0.15
[2022-02-27 16:49:50] [config] bert-sep-symbol: "[SEP]"
[2022-02-27 16:49:50] [config] bert-train-type-embeddings: true
[2022-02-27 16:49:50] [config] bert-type-vocab-size: 2
[2022-02-27 16:49:50] [config] build-info: ""
[2022-02-27 16:49:50] [config] check-gradient-nan: false
[2022-02-27 16:49:50] [config] check-nan: false
[2022-02-27 16:49:50] [config] cite: false
[2022-02-27 16:49:50] [config] clip-norm: 0
[2022-02-27 16:49:50] [config] cost-scaling:
[2022-02-27 16:49:50] [config]   []
[2022-02-27 16:49:50] [config] cost-type: ce-mean-words
[2022-02-27 16:49:50] [config] cpu-threads: 0
[2022-02-27 16:49:50] [config] data-weighting: ""
[2022-02-27 16:49:50] [config] data-weighting-type: sentence
[2022-02-27 16:49:50] [config] dec-cell: gru
[2022-02-27 16:49:50] [config] dec-cell-base-depth: 2
[2022-02-27 16:49:50] [config] dec-cell-high-depth: 1
[2022-02-27 16:49:50] [config] dec-depth: 6
[2022-02-27 16:49:50] [config] devices:
[2022-02-27 16:49:50] [config]   - 0
[2022-02-27 16:49:50] [config]   - 1
[2022-02-27 16:49:50] [config] dim-emb: 1024
[2022-02-27 16:49:50] [config] dim-rnn: 1024
[2022-02-27 16:49:50] [config] dim-vocabs:
[2022-02-27 16:49:50] [config]   - 55026
[2022-02-27 16:49:50] [config]   - 55026
[2022-02-27 16:49:50] [config] disp-first: 0
[2022-02-27 16:49:50] [config] disp-freq: 10000
[2022-02-27 16:49:50] [config] disp-label-counts: true
[2022-02-27 16:49:50] [config] dropout-rnn: 0
[2022-02-27 16:49:50] [config] dropout-src: 0
[2022-02-27 16:49:50] [config] dropout-trg: 0
[2022-02-27 16:49:50] [config] dump-config: ""
[2022-02-27 16:49:50] [config] dynamic-gradient-scaling:
[2022-02-27 16:49:50] [config]   []
[2022-02-27 16:49:50] [config] early-stopping: 10
[2022-02-27 16:49:50] [config] early-stopping-on: first
[2022-02-27 16:49:50] [config] embedding-fix-src: false
[2022-02-27 16:49:50] [config] embedding-fix-trg: false
[2022-02-27 16:49:50] [config] embedding-normalization: false
[2022-02-27 16:49:50] [config] embedding-vectors:
[2022-02-27 16:49:50] [config]   []
[2022-02-27 16:49:50] [config] enc-cell: gru
[2022-02-27 16:49:50] [config] enc-cell-depth: 1
[2022-02-27 16:49:50] [config] enc-depth: 6
[2022-02-27 16:49:50] [config] enc-type: bidirectional
[2022-02-27 16:49:50] [config] english-title-case-every: 0
[2022-02-27 16:49:50] [config] exponential-smoothing: 0.0001
[2022-02-27 16:49:50] [config] factor-weight: 1
[2022-02-27 16:49:50] [config] factors-combine: sum
[2022-02-27 16:49:50] [config] factors-dim-emb: 0
[2022-02-27 16:49:50] [config] gradient-checkpointing: false
[2022-02-27 16:49:50] [config] gradient-norm-average-window: 100
[2022-02-27 16:49:50] [config] guided-alignment: none
[2022-02-27 16:49:50] [config] guided-alignment-cost: mse
[2022-02-27 16:49:50] [config] guided-alignment-weight: 0.1
[2022-02-27 16:49:50] [config] ignore-model-config: false
[2022-02-27 16:49:50] [config] input-types:
[2022-02-27 16:49:50] [config]   []
[2022-02-27 16:49:50] [config] interpolate-env-vars: false
[2022-02-27 16:49:50] [config] keep-best: true
[2022-02-27 16:49:50] [config] label-smoothing: 0.1
[2022-02-27 16:49:50] [config] layer-normalization: false
[2022-02-27 16:49:50] [config] learn-rate: 0.0002
[2022-02-27 16:49:50] [config] lemma-dependency: ""
[2022-02-27 16:49:50] [config] lemma-dim-emb: 0
[2022-02-27 16:49:50] [config] log: /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.train1.log
[2022-02-27 16:49:50] [config] log-level: info
[2022-02-27 16:49:50] [config] log-time-zone: ""
[2022-02-27 16:49:50] [config] logical-epoch:
[2022-02-27 16:49:50] [config]   - 1e
[2022-02-27 16:49:50] [config]   - 0
[2022-02-27 16:49:50] [config] lr-decay: 0
[2022-02-27 16:49:50] [config] lr-decay-freq: 50000
[2022-02-27 16:49:50] [config] lr-decay-inv-sqrt:
[2022-02-27 16:49:50] [config]   - 8000
[2022-02-27 16:49:50] [config] lr-decay-repeat-warmup: false
[2022-02-27 16:49:50] [config] lr-decay-reset-optimizer: false
[2022-02-27 16:49:50] [config] lr-decay-start:
[2022-02-27 16:49:50] [config]   - 10
[2022-02-27 16:49:50] [config]   - 1
[2022-02-27 16:49:50] [config] lr-decay-strategy: epoch+stalled
[2022-02-27 16:49:50] [config] lr-report: false
[2022-02-27 16:49:50] [config] lr-warmup: 8000
[2022-02-27 16:49:50] [config] lr-warmup-at-reload: false
[2022-02-27 16:49:50] [config] lr-warmup-cycle: false
[2022-02-27 16:49:50] [config] lr-warmup-start-rate: 0
[2022-02-27 16:49:50] [config] max-length: 100
[2022-02-27 16:49:50] [config] max-length-crop: false
[2022-02-27 16:49:50] [config] max-length-factor: 3
[2022-02-27 16:49:50] [config] maxi-batch: 1000
[2022-02-27 16:49:50] [config] maxi-batch-sort: trg
[2022-02-27 16:49:50] [config] mini-batch: 1000
[2022-02-27 16:49:50] [config] mini-batch-fit: true
[2022-02-27 16:49:50] [config] mini-batch-fit-step: 10
[2022-02-27 16:49:50] [config] mini-batch-round-up: true
[2022-02-27 16:49:50] [config] mini-batch-track-lr: false
[2022-02-27 16:49:50] [config] mini-batch-warmup: 0
[2022-02-27 16:49:50] [config] mini-batch-words: 0
[2022-02-27 16:49:50] [config] mini-batch-words-ref: 0
[2022-02-27 16:49:50] [config] model: /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-27 16:49:50] [config] multi-loss-type: sum
[2022-02-27 16:49:50] [config] n-best: false
[2022-02-27 16:49:50] [config] no-nccl: false
[2022-02-27 16:49:50] [config] no-reload: false
[2022-02-27 16:49:50] [config] no-restore-corpus: true
[2022-02-27 16:49:50] [config] normalize: 1
[2022-02-27 16:49:50] [config] normalize-gradient: false
[2022-02-27 16:49:50] [config] num-devices: 0
[2022-02-27 16:49:50] [config] optimizer: adam
[2022-02-27 16:49:50] [config] optimizer-delay: 2
[2022-02-27 16:49:50] [config] optimizer-params:
[2022-02-27 16:49:50] [config]   - 0.9
[2022-02-27 16:49:50] [config]   - 0.998
[2022-02-27 16:49:50] [config]   - 1e-09
[2022-02-27 16:49:50] [config] output-omit-bias: false
[2022-02-27 16:49:50] [config] overwrite: true
[2022-02-27 16:49:50] [config] precision:
[2022-02-27 16:49:50] [config]   - float32
[2022-02-27 16:49:50] [config]   - float32
[2022-02-27 16:49:50] [config] pretrained-model: ""
[2022-02-27 16:49:50] [config] quantize-biases: false
[2022-02-27 16:49:50] [config] quantize-bits: 0
[2022-02-27 16:49:50] [config] quantize-log-based: false
[2022-02-27 16:49:50] [config] quantize-optimization-steps: 0
[2022-02-27 16:49:50] [config] quiet: false
[2022-02-27 16:49:50] [config] quiet-translation: false
[2022-02-27 16:49:50] [config] relative-paths: false
[2022-02-27 16:49:50] [config] right-left: false
[2022-02-27 16:49:50] [config] save-freq: 10000
[2022-02-27 16:49:50] [config] seed: 1111
[2022-02-27 16:49:50] [config] sentencepiece-alphas:
[2022-02-27 16:49:50] [config]   []
[2022-02-27 16:49:50] [config] sentencepiece-max-lines: 2000000
[2022-02-27 16:49:50] [config] sentencepiece-options: ""
[2022-02-27 16:49:50] [config] sharding: local
[2022-02-27 16:49:50] [config] shuffle: batches
[2022-02-27 16:49:50] [config] shuffle-in-ram: false
[2022-02-27 16:49:50] [config] sigterm: save-and-exit
[2022-02-27 16:49:50] [config] skip: false
[2022-02-27 16:49:50] [config] sqlite: ""
[2022-02-27 16:49:50] [config] sqlite-drop: false
[2022-02-27 16:49:50] [config] sync-freq: 200u
[2022-02-27 16:49:50] [config] sync-sgd: true
[2022-02-27 16:49:50] [config] tempdir: /run/nvme/job_10769183/tmp
[2022-02-27 16:49:50] [config] tied-embeddings: false
[2022-02-27 16:49:50] [config] tied-embeddings-all: true
[2022-02-27 16:49:50] [config] tied-embeddings-src: false
[2022-02-27 16:49:50] [config] train-embedder-rank:
[2022-02-27 16:49:50] [config]   []
[2022-02-27 16:49:50] [config] train-sets:
[2022-02-27 16:49:50] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/train/opusTCv20210807+bt.src.clean.spm32k.gz
[2022-02-27 16:49:50] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/train/opusTCv20210807+bt.trg.clean.spm32k.gz
[2022-02-27 16:49:50] [config] transformer-aan-activation: swish
[2022-02-27 16:49:50] [config] transformer-aan-depth: 2
[2022-02-27 16:49:50] [config] transformer-aan-nogate: false
[2022-02-27 16:49:50] [config] transformer-decoder-autoreg: self-attention
[2022-02-27 16:49:50] [config] transformer-depth-scaling: false
[2022-02-27 16:49:50] [config] transformer-dim-aan: 2048
[2022-02-27 16:49:50] [config] transformer-dim-ffn: 4096
[2022-02-27 16:49:50] [config] transformer-dropout: 0.1
[2022-02-27 16:49:50] [config] transformer-dropout-attention: 0
[2022-02-27 16:49:50] [config] transformer-dropout-ffn: 0
[2022-02-27 16:49:50] [config] transformer-ffn-activation: relu
[2022-02-27 16:49:50] [config] transformer-ffn-depth: 2
[2022-02-27 16:49:50] [config] transformer-guided-alignment-layer: last
[2022-02-27 16:49:50] [config] transformer-heads: 16
[2022-02-27 16:49:50] [config] transformer-no-projection: false
[2022-02-27 16:49:50] [config] transformer-pool: false
[2022-02-27 16:49:50] [config] transformer-postprocess: dan
[2022-02-27 16:49:50] [config] transformer-postprocess-emb: d
[2022-02-27 16:49:50] [config] transformer-postprocess-top: ""
[2022-02-27 16:49:50] [config] transformer-preprocess: ""
[2022-02-27 16:49:50] [config] transformer-tied-layers:
[2022-02-27 16:49:50] [config]   []
[2022-02-27 16:49:50] [config] transformer-train-position-embeddings: false
[2022-02-27 16:49:50] [config] tsv: false
[2022-02-27 16:49:50] [config] tsv-fields: 0
[2022-02-27 16:49:50] [config] type: transformer
[2022-02-27 16:49:50] [config] ulr: false
[2022-02-27 16:49:50] [config] ulr-dim-emb: 0
[2022-02-27 16:49:50] [config] ulr-dropout: 0
[2022-02-27 16:49:50] [config] ulr-keys-vectors: ""
[2022-02-27 16:49:50] [config] ulr-query-vectors: ""
[2022-02-27 16:49:50] [config] ulr-softmax-temperature: 1
[2022-02-27 16:49:50] [config] ulr-trainable-transformation: false
[2022-02-27 16:49:50] [config] unlikelihood-loss: false
[2022-02-27 16:49:50] [config] valid-freq: 10000
[2022-02-27 16:49:50] [config] valid-log: /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.valid1.log
[2022-02-27 16:49:50] [config] valid-max-length: 100
[2022-02-27 16:49:50] [config] valid-metrics:
[2022-02-27 16:49:50] [config]   - perplexity
[2022-02-27 16:49:50] [config] valid-mini-batch: 16
[2022-02-27 16:49:50] [config] valid-reset-stalled: false
[2022-02-27 16:49:50] [config] valid-script-args:
[2022-02-27 16:49:50] [config]   []
[2022-02-27 16:49:50] [config] valid-script-path: ""
[2022-02-27 16:49:50] [config] valid-sets:
[2022-02-27 16:49:50] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/val/Tatoeba-dev-v2021-08-07.src.spm32k
[2022-02-27 16:49:50] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/val/Tatoeba-dev-v2021-08-07.trg.spm32k
[2022-02-27 16:49:50] [config] valid-translation-output: ""
[2022-02-27 16:49:50] [config] version: v1.10.24; 12a1bfa 2021-10-11 16:59:52 +0100
[2022-02-27 16:49:50] [config] vocabs:
[2022-02-27 16:49:50] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-02-27 16:49:50] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-02-27 16:49:50] [config] word-penalty: 0
[2022-02-27 16:49:50] [config] word-scores: false
[2022-02-27 16:49:50] [config] workspace: 15000
[2022-02-27 16:49:50] [config] Loaded model has been created with Marian v1.10.24; 12a1bfa 2021-10-11 16:59:52 +0100
[2022-02-27 16:49:50] Using synchronous SGD
[2022-02-27 16:49:50] [comm] Compiled without MPI support. Running as a single process on r18g01.bullx
[2022-02-27 16:49:50] Synced seed 1111
[2022-02-27 16:49:50] [data] Loading vocabulary from JSON/Yaml file /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-02-27 16:49:51] [data] Setting vocabulary size for input 0 to 55,026
[2022-02-27 16:49:51] [data] Loading vocabulary from JSON/Yaml file /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-02-27 16:49:51] [data] Setting vocabulary size for input 1 to 55,026
[2022-02-27 16:49:51] [batching] Collecting statistics for batch fitting with step size 10
[2022-02-27 16:49:52] [memory] Extending reserved space to 15104 MB (device gpu0)
[2022-02-27 16:49:54] [memory] Extending reserved space to 15104 MB (device gpu1)
[2022-02-27 16:49:54] [comm] Using NCCL 2.8.3 for GPU communication
[2022-02-27 16:49:54] [comm] Using global sharding
[2022-02-27 16:49:55] [comm] NCCLCommunicators constructed successfully
[2022-02-27 16:49:55] [training] Using 2 GPUs
[2022-02-27 16:49:55] [logits] Applying loss function for 1 factor(s)
[2022-02-27 16:49:55] [memory] Reserving 887 MB, device gpu0
[2022-02-27 16:49:58] [gpu] 16-bit TensorCores enabled for float32 matrix operations
[2022-02-27 16:49:58] [memory] Reserving 887 MB, device gpu0
[2022-02-27 16:50:26] [batching] Done. Typical MB size is 27,468 target words
[2022-02-27 16:50:26] [memory] Extending reserved space to 15104 MB (device gpu0)
[2022-02-27 16:50:26] [memory] Extending reserved space to 15104 MB (device gpu1)
[2022-02-27 16:50:27] [comm] Using NCCL 2.8.3 for GPU communication
[2022-02-27 16:50:27] [comm] Using global sharding
[2022-02-27 16:50:27] [comm] NCCLCommunicators constructed successfully
[2022-02-27 16:50:27] [training] Using 2 GPUs
[2022-02-27 16:50:27] Loading model from /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-27 16:50:29] Loading model from /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-27 16:50:36] Allocating memory for general optimizer shards
[2022-02-27 16:50:36] [memory] Reserving 443 MB, device gpu0
[2022-02-27 16:50:36] [memory] Reserving 443 MB, device gpu1
[2022-02-27 16:50:37] Loading Adam parameters
[2022-02-27 16:50:37] [memory] Reserving 887 MB, device gpu0
[2022-02-27 16:50:38] [memory] Reserving 887 MB, device gpu1
[2022-02-27 16:50:38] [memory] Reserving 887 MB, device gpu0
[2022-02-27 16:50:38] [memory] Reserving 887 MB, device gpu1
[2022-02-27 16:50:39] [training] Master parameters and optimizers restored from training checkpoint /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-27 16:50:39] Training started
[2022-02-27 16:51:01] [training] Batches are processed as 1 process(es) x 2 devices/process
[2022-02-27 16:51:02] [memory] Reserving 887 MB, device gpu0
[2022-02-27 16:51:02] [memory] Reserving 887 MB, device gpu1
[2022-02-27 16:51:04] Parameter type float32, optimization type float32, casting types false
[2022-02-27 21:06:50] Ep. 1 : Up. 650000 : Sen. 8,809,207 : Cost 2.30568361 : Time 15383.84s : 14762.07 words/s : gNorm 0.5433
[2022-02-27 21:06:50] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-27 21:06:54] Saving Adam parameters
[2022-02-27 21:06:58] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-27 21:07:08] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-27 21:07:10] [valid] Ep. 1 : Up. 650000 : perplexity : 2.03064 : new best
[2022-02-28 01:22:52] Ep. 1 : Up. 660000 : Sen. 17,604,389 : Cost 2.30369878 : Time 15361.65s : 14781.45 words/s : gNorm 0.5628
[2022-02-28 01:22:52] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-28 01:22:58] Saving Adam parameters
[2022-02-28 01:23:01] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-28 01:23:15] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-28 01:23:18] [valid] Ep. 1 : Up. 660000 : perplexity : 2.03029 : new best
[2022-02-28 05:39:08] Ep. 1 : Up. 670000 : Sen. 26,392,815 : Cost 2.30418921 : Time 15375.96s : 14762.23 words/s : gNorm 0.5509
[2022-02-28 05:39:08] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-28 05:39:12] Saving Adam parameters
[2022-02-28 05:39:15] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-28 05:39:28] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-28 05:39:31] [valid] Ep. 1 : Up. 670000 : perplexity : 2.02962 : new best
[2022-02-28 09:55:34] Ep. 1 : Up. 680000 : Sen. 35,198,052 : Cost 2.30257630 : Time 15386.16s : 14761.95 words/s : gNorm 0.5249
[2022-02-28 09:55:35] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-28 09:55:39] Saving Adam parameters
[2022-02-28 09:55:42] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-28 09:56:00] [valid] Ep. 1 : Up. 680000 : perplexity : 2.03039 : stalled 1 times (last best: 2.02962)
[2022-02-28 14:11:49] Ep. 1 : Up. 690000 : Sen. 44,010,124 : Cost 2.30255508 : Time 15374.43s : 14771.71 words/s : gNorm 0.5716
[2022-02-28 14:11:49] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-28 14:11:56] Saving Adam parameters
[2022-02-28 14:12:03] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-28 14:12:23] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-28 14:12:27] [valid] Ep. 1 : Up. 690000 : perplexity : 2.02961 : new best
[2022-02-28 18:27:58] Ep. 1 : Up. 700000 : Sen. 52,814,651 : Cost 2.30224109 : Time 15368.99s : 14767.56 words/s : gNorm 0.5753
[2022-02-28 18:27:58] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-28 18:28:03] Saving Adam parameters
[2022-02-28 18:28:06] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-28 18:28:17] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-28 18:28:19] [valid] Ep. 1 : Up. 700000 : perplexity : 2.02891 : new best
[2022-02-28 22:44:18] Ep. 1 : Up. 710000 : Sen. 61,603,455 : Cost 2.30110073 : Time 15379.91s : 14771.30 words/s : gNorm 0.5790
[2022-02-28 22:44:18] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-02-28 22:44:23] Saving Adam parameters
[2022-02-28 22:44:26] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-02-28 22:44:46] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-02-28 22:44:48] [valid] Ep. 1 : Up. 710000 : perplexity : 2.02805 : new best
[2022-03-01 03:00:38] Ep. 1 : Up. 720000 : Sen. 70,427,669 : Cost 2.30093741 : Time 15379.97s : 14758.44 words/s : gNorm 0.5656
[2022-03-01 03:00:38] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-01 03:00:42] Saving Adam parameters
[2022-03-01 03:00:46] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-01 03:01:00] [valid] Ep. 1 : Up. 720000 : perplexity : 2.02834 : stalled 1 times (last best: 2.02805)
[2022-03-01 07:16:52] Ep. 1 : Up. 730000 : Sen. 79,211,349 : Cost 2.29982138 : Time 15374.06s : 14778.28 words/s : gNorm 0.5736
[2022-03-01 07:16:52] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-01 07:16:56] Saving Adam parameters
[2022-03-01 07:16:59] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-01 07:17:09] [valid] Ep. 1 : Up. 730000 : perplexity : 2.02826 : stalled 2 times (last best: 2.02805)
[2022-03-01 11:33:00] Ep. 1 : Up. 740000 : Sen. 88,016,176 : Cost 2.29955530 : Time 15368.04s : 14778.78 words/s : gNorm 0.5793
[2022-03-01 11:33:01] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-01 11:33:10] Saving Adam parameters
[2022-03-01 11:33:15] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-01 11:33:27] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-03-01 11:33:32] [valid] Ep. 1 : Up. 740000 : perplexity : 2.02645 : new best
[2022-03-01 15:49:36] Ep. 1 : Up. 750000 : Sen. 96,828,380 : Cost 2.29953361 : Time 15394.87s : 14753.29 words/s : gNorm 0.5989
[2022-03-01 15:49:36] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-01 15:49:43] Saving Adam parameters
[2022-03-01 15:49:46] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-01 15:49:57] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-03-01 15:50:05] [valid] Ep. 1 : Up. 750000 : perplexity : 2.02634 : new best
[2022-03-01 20:06:12] Ep. 1 : Up. 760000 : Sen. 105,605,925 : Cost 2.29789710 : Time 15396.71s : 14750.84 words/s : gNorm 0.5532
[2022-03-01 20:06:13] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-01 20:06:18] Saving Adam parameters
[2022-03-01 20:06:21] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-01 20:06:34] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-03-01 20:06:38] [valid] Ep. 1 : Up. 760000 : perplexity : 2.02471 : new best
[2022-03-02 00:22:37] Ep. 1 : Up. 770000 : Sen. 114,405,705 : Cost 2.29833198 : Time 15384.83s : 14752.97 words/s : gNorm 0.5833
[2022-03-02 00:22:37] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-02 00:22:43] Saving Adam parameters
[2022-03-02 00:22:47] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-02 00:22:59] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-03-02 00:23:03] [valid] Ep. 1 : Up. 770000 : perplexity : 2.02471 : new best
[2022-03-02 04:40:20] Ep. 1 : Up. 780000 : Sen. 123,216,854 : Cost 2.29812598 : Time 15462.60s : 14690.45 words/s : gNorm 0.5757
[2022-03-02 04:40:20] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-02 04:40:25] Saving Adam parameters
[2022-03-02 04:40:28] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-02 04:40:43] [valid] Ep. 1 : Up. 780000 : perplexity : 2.02491 : stalled 1 times (last best: 2.02471)
[2022-03-02 08:58:02] Ep. 1 : Up. 790000 : Sen. 132,035,340 : Cost 2.29754186 : Time 15461.73s : 14696.29 words/s : gNorm 0.5725
[2022-03-02 08:58:02] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-02 08:58:10] Saving Adam parameters
[2022-03-02 08:58:13] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-02 08:58:41] [valid] Ep. 1 : Up. 790000 : perplexity : 2.02682 : stalled 2 times (last best: 2.02471)
[2022-03-02 13:15:33] Ep. 1 : Up. 800000 : Sen. 140,844,597 : Cost 2.29757786 : Time 15451.54s : 14687.89 words/s : gNorm 0.5612
[2022-03-02 13:15:34] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-02 13:15:40] Saving Adam parameters
[2022-03-02 13:15:43] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-02 13:16:04] [valid] Ep. 1 : Up. 800000 : perplexity : 2.02578 : stalled 3 times (last best: 2.02471)
[2022-03-02 23:52:39] [marian] Marian v1.10.24; 12a1bfa 2021-10-11 16:59:52 +0100
[2022-03-02 23:52:39] [marian] Running on r17g06.bullx as process 59270 with command line:
[2022-03-02 23:52:39] [marian] /projappl/project_2001194/marian-dev/build/marian --task transformer-big --optimizer-delay 2 --no-restore-corpus --early-stopping 10 --valid-freq 10000 --valid-sets /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/val/Tatoeba-dev-v2021-08-07.src.spm32k /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/val/Tatoeba-dev-v2021-08-07.trg.spm32k --valid-metrics perplexity --valid-mini-batch 16 --valid-max-length 100 --valid-log /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.valid1.log --beam-size 6 --normalize 1 --allow-unk --workspace 15000 --model /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz --train-sets /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/train/opusTCv20210807+bt.src.clean.spm32k.gz /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/train/opusTCv20210807+bt.trg.clean.spm32k.gz --vocabs /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.vocab.yml /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.vocab.yml --save-freq 10000 --disp-freq 10000 --log /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.train1.log --devices 0 1 --seed 1111 --tempdir /run/nvme/job_10791484/tmp --shuffle batches --sharding local --overwrite --keep-best
[2022-03-02 23:52:41] [config] after: 0e
[2022-03-02 23:52:41] [config] after-batches: 0
[2022-03-02 23:52:41] [config] after-epochs: 0
[2022-03-02 23:52:41] [config] all-caps-every: 0
[2022-03-02 23:52:41] [config] allow-unk: true
[2022-03-02 23:52:41] [config] authors: false
[2022-03-02 23:52:41] [config] beam-size: 6
[2022-03-02 23:52:41] [config] bert-class-symbol: "[CLS]"
[2022-03-02 23:52:41] [config] bert-mask-symbol: "[MASK]"
[2022-03-02 23:52:41] [config] bert-masking-fraction: 0.15
[2022-03-02 23:52:41] [config] bert-sep-symbol: "[SEP]"
[2022-03-02 23:52:41] [config] bert-train-type-embeddings: true
[2022-03-02 23:52:41] [config] bert-type-vocab-size: 2
[2022-03-02 23:52:41] [config] build-info: ""
[2022-03-02 23:52:41] [config] check-gradient-nan: false
[2022-03-02 23:52:41] [config] check-nan: false
[2022-03-02 23:52:41] [config] cite: false
[2022-03-02 23:52:41] [config] clip-norm: 0
[2022-03-02 23:52:41] [config] cost-scaling:
[2022-03-02 23:52:41] [config]   []
[2022-03-02 23:52:41] [config] cost-type: ce-mean-words
[2022-03-02 23:52:41] [config] cpu-threads: 0
[2022-03-02 23:52:41] [config] data-weighting: ""
[2022-03-02 23:52:41] [config] data-weighting-type: sentence
[2022-03-02 23:52:41] [config] dec-cell: gru
[2022-03-02 23:52:41] [config] dec-cell-base-depth: 2
[2022-03-02 23:52:41] [config] dec-cell-high-depth: 1
[2022-03-02 23:52:41] [config] dec-depth: 6
[2022-03-02 23:52:41] [config] devices:
[2022-03-02 23:52:41] [config]   - 0
[2022-03-02 23:52:41] [config]   - 1
[2022-03-02 23:52:41] [config] dim-emb: 1024
[2022-03-02 23:52:41] [config] dim-rnn: 1024
[2022-03-02 23:52:41] [config] dim-vocabs:
[2022-03-02 23:52:41] [config]   - 55026
[2022-03-02 23:52:41] [config]   - 55026
[2022-03-02 23:52:41] [config] disp-first: 0
[2022-03-02 23:52:41] [config] disp-freq: 10000
[2022-03-02 23:52:41] [config] disp-label-counts: true
[2022-03-02 23:52:41] [config] dropout-rnn: 0
[2022-03-02 23:52:41] [config] dropout-src: 0
[2022-03-02 23:52:41] [config] dropout-trg: 0
[2022-03-02 23:52:41] [config] dump-config: ""
[2022-03-02 23:52:41] [config] dynamic-gradient-scaling:
[2022-03-02 23:52:41] [config]   []
[2022-03-02 23:52:41] [config] early-stopping: 10
[2022-03-02 23:52:41] [config] early-stopping-on: first
[2022-03-02 23:52:41] [config] embedding-fix-src: false
[2022-03-02 23:52:41] [config] embedding-fix-trg: false
[2022-03-02 23:52:41] [config] embedding-normalization: false
[2022-03-02 23:52:41] [config] embedding-vectors:
[2022-03-02 23:52:41] [config]   []
[2022-03-02 23:52:41] [config] enc-cell: gru
[2022-03-02 23:52:41] [config] enc-cell-depth: 1
[2022-03-02 23:52:41] [config] enc-depth: 6
[2022-03-02 23:52:41] [config] enc-type: bidirectional
[2022-03-02 23:52:41] [config] english-title-case-every: 0
[2022-03-02 23:52:41] [config] exponential-smoothing: 0.0001
[2022-03-02 23:52:41] [config] factor-weight: 1
[2022-03-02 23:52:41] [config] factors-combine: sum
[2022-03-02 23:52:41] [config] factors-dim-emb: 0
[2022-03-02 23:52:41] [config] gradient-checkpointing: false
[2022-03-02 23:52:41] [config] gradient-norm-average-window: 100
[2022-03-02 23:52:41] [config] guided-alignment: none
[2022-03-02 23:52:41] [config] guided-alignment-cost: mse
[2022-03-02 23:52:41] [config] guided-alignment-weight: 0.1
[2022-03-02 23:52:41] [config] ignore-model-config: false
[2022-03-02 23:52:41] [config] input-types:
[2022-03-02 23:52:41] [config]   []
[2022-03-02 23:52:41] [config] interpolate-env-vars: false
[2022-03-02 23:52:41] [config] keep-best: true
[2022-03-02 23:52:41] [config] label-smoothing: 0.1
[2022-03-02 23:52:41] [config] layer-normalization: false
[2022-03-02 23:52:41] [config] learn-rate: 0.0002
[2022-03-02 23:52:41] [config] lemma-dependency: ""
[2022-03-02 23:52:41] [config] lemma-dim-emb: 0
[2022-03-02 23:52:41] [config] log: /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.train1.log
[2022-03-02 23:52:41] [config] log-level: info
[2022-03-02 23:52:41] [config] log-time-zone: ""
[2022-03-02 23:52:41] [config] logical-epoch:
[2022-03-02 23:52:41] [config]   - 1e
[2022-03-02 23:52:41] [config]   - 0
[2022-03-02 23:52:41] [config] lr-decay: 0
[2022-03-02 23:52:41] [config] lr-decay-freq: 50000
[2022-03-02 23:52:41] [config] lr-decay-inv-sqrt:
[2022-03-02 23:52:41] [config]   - 8000
[2022-03-02 23:52:41] [config] lr-decay-repeat-warmup: false
[2022-03-02 23:52:41] [config] lr-decay-reset-optimizer: false
[2022-03-02 23:52:41] [config] lr-decay-start:
[2022-03-02 23:52:41] [config]   - 10
[2022-03-02 23:52:41] [config]   - 1
[2022-03-02 23:52:41] [config] lr-decay-strategy: epoch+stalled
[2022-03-02 23:52:41] [config] lr-report: false
[2022-03-02 23:52:41] [config] lr-warmup: 8000
[2022-03-02 23:52:41] [config] lr-warmup-at-reload: false
[2022-03-02 23:52:41] [config] lr-warmup-cycle: false
[2022-03-02 23:52:41] [config] lr-warmup-start-rate: 0
[2022-03-02 23:52:41] [config] max-length: 100
[2022-03-02 23:52:41] [config] max-length-crop: false
[2022-03-02 23:52:41] [config] max-length-factor: 3
[2022-03-02 23:52:41] [config] maxi-batch: 1000
[2022-03-02 23:52:41] [config] maxi-batch-sort: trg
[2022-03-02 23:52:41] [config] mini-batch: 1000
[2022-03-02 23:52:41] [config] mini-batch-fit: true
[2022-03-02 23:52:41] [config] mini-batch-fit-step: 10
[2022-03-02 23:52:41] [config] mini-batch-round-up: true
[2022-03-02 23:52:41] [config] mini-batch-track-lr: false
[2022-03-02 23:52:41] [config] mini-batch-warmup: 0
[2022-03-02 23:52:41] [config] mini-batch-words: 0
[2022-03-02 23:52:41] [config] mini-batch-words-ref: 0
[2022-03-02 23:52:41] [config] model: /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-02 23:52:41] [config] multi-loss-type: sum
[2022-03-02 23:52:41] [config] n-best: false
[2022-03-02 23:52:41] [config] no-nccl: false
[2022-03-02 23:52:41] [config] no-reload: false
[2022-03-02 23:52:41] [config] no-restore-corpus: true
[2022-03-02 23:52:41] [config] normalize: 1
[2022-03-02 23:52:41] [config] normalize-gradient: false
[2022-03-02 23:52:41] [config] num-devices: 0
[2022-03-02 23:52:41] [config] optimizer: adam
[2022-03-02 23:52:41] [config] optimizer-delay: 2
[2022-03-02 23:52:41] [config] optimizer-params:
[2022-03-02 23:52:41] [config]   - 0.9
[2022-03-02 23:52:41] [config]   - 0.998
[2022-03-02 23:52:41] [config]   - 1e-09
[2022-03-02 23:52:41] [config] output-omit-bias: false
[2022-03-02 23:52:41] [config] overwrite: true
[2022-03-02 23:52:41] [config] precision:
[2022-03-02 23:52:41] [config]   - float32
[2022-03-02 23:52:41] [config]   - float32
[2022-03-02 23:52:41] [config] pretrained-model: ""
[2022-03-02 23:52:41] [config] quantize-biases: false
[2022-03-02 23:52:41] [config] quantize-bits: 0
[2022-03-02 23:52:41] [config] quantize-log-based: false
[2022-03-02 23:52:41] [config] quantize-optimization-steps: 0
[2022-03-02 23:52:41] [config] quiet: false
[2022-03-02 23:52:41] [config] quiet-translation: false
[2022-03-02 23:52:41] [config] relative-paths: false
[2022-03-02 23:52:41] [config] right-left: false
[2022-03-02 23:52:41] [config] save-freq: 10000
[2022-03-02 23:52:41] [config] seed: 1111
[2022-03-02 23:52:41] [config] sentencepiece-alphas:
[2022-03-02 23:52:41] [config]   []
[2022-03-02 23:52:41] [config] sentencepiece-max-lines: 2000000
[2022-03-02 23:52:41] [config] sentencepiece-options: ""
[2022-03-02 23:52:41] [config] sharding: local
[2022-03-02 23:52:41] [config] shuffle: batches
[2022-03-02 23:52:41] [config] shuffle-in-ram: false
[2022-03-02 23:52:41] [config] sigterm: save-and-exit
[2022-03-02 23:52:41] [config] skip: false
[2022-03-02 23:52:41] [config] sqlite: ""
[2022-03-02 23:52:41] [config] sqlite-drop: false
[2022-03-02 23:52:41] [config] sync-freq: 200u
[2022-03-02 23:52:41] [config] sync-sgd: true
[2022-03-02 23:52:41] [config] tempdir: /run/nvme/job_10791484/tmp
[2022-03-02 23:52:41] [config] tied-embeddings: false
[2022-03-02 23:52:41] [config] tied-embeddings-all: true
[2022-03-02 23:52:41] [config] tied-embeddings-src: false
[2022-03-02 23:52:41] [config] train-embedder-rank:
[2022-03-02 23:52:41] [config]   []
[2022-03-02 23:52:41] [config] train-sets:
[2022-03-02 23:52:41] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/train/opusTCv20210807+bt.src.clean.spm32k.gz
[2022-03-02 23:52:41] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/train/opusTCv20210807+bt.trg.clean.spm32k.gz
[2022-03-02 23:52:41] [config] transformer-aan-activation: swish
[2022-03-02 23:52:41] [config] transformer-aan-depth: 2
[2022-03-02 23:52:41] [config] transformer-aan-nogate: false
[2022-03-02 23:52:41] [config] transformer-decoder-autoreg: self-attention
[2022-03-02 23:52:41] [config] transformer-depth-scaling: false
[2022-03-02 23:52:41] [config] transformer-dim-aan: 2048
[2022-03-02 23:52:41] [config] transformer-dim-ffn: 4096
[2022-03-02 23:52:41] [config] transformer-dropout: 0.1
[2022-03-02 23:52:41] [config] transformer-dropout-attention: 0
[2022-03-02 23:52:41] [config] transformer-dropout-ffn: 0
[2022-03-02 23:52:41] [config] transformer-ffn-activation: relu
[2022-03-02 23:52:41] [config] transformer-ffn-depth: 2
[2022-03-02 23:52:41] [config] transformer-guided-alignment-layer: last
[2022-03-02 23:52:41] [config] transformer-heads: 16
[2022-03-02 23:52:41] [config] transformer-no-projection: false
[2022-03-02 23:52:41] [config] transformer-pool: false
[2022-03-02 23:52:41] [config] transformer-postprocess: dan
[2022-03-02 23:52:41] [config] transformer-postprocess-emb: d
[2022-03-02 23:52:41] [config] transformer-postprocess-top: ""
[2022-03-02 23:52:41] [config] transformer-preprocess: ""
[2022-03-02 23:52:41] [config] transformer-tied-layers:
[2022-03-02 23:52:41] [config]   []
[2022-03-02 23:52:41] [config] transformer-train-position-embeddings: false
[2022-03-02 23:52:41] [config] tsv: false
[2022-03-02 23:52:41] [config] tsv-fields: 0
[2022-03-02 23:52:41] [config] type: transformer
[2022-03-02 23:52:41] [config] ulr: false
[2022-03-02 23:52:41] [config] ulr-dim-emb: 0
[2022-03-02 23:52:41] [config] ulr-dropout: 0
[2022-03-02 23:52:41] [config] ulr-keys-vectors: ""
[2022-03-02 23:52:41] [config] ulr-query-vectors: ""
[2022-03-02 23:52:41] [config] ulr-softmax-temperature: 1
[2022-03-02 23:52:41] [config] ulr-trainable-transformation: false
[2022-03-02 23:52:41] [config] unlikelihood-loss: false
[2022-03-02 23:52:41] [config] valid-freq: 10000
[2022-03-02 23:52:41] [config] valid-log: /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.valid1.log
[2022-03-02 23:52:41] [config] valid-max-length: 100
[2022-03-02 23:52:41] [config] valid-metrics:
[2022-03-02 23:52:41] [config]   - perplexity
[2022-03-02 23:52:41] [config] valid-mini-batch: 16
[2022-03-02 23:52:41] [config] valid-reset-stalled: false
[2022-03-02 23:52:41] [config] valid-script-args:
[2022-03-02 23:52:41] [config]   []
[2022-03-02 23:52:41] [config] valid-script-path: ""
[2022-03-02 23:52:41] [config] valid-sets:
[2022-03-02 23:52:41] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/val/Tatoeba-dev-v2021-08-07.src.spm32k
[2022-03-02 23:52:41] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/val/Tatoeba-dev-v2021-08-07.trg.spm32k
[2022-03-02 23:52:41] [config] valid-translation-output: ""
[2022-03-02 23:52:41] [config] version: v1.10.24; 12a1bfa 2021-10-11 16:59:52 +0100
[2022-03-02 23:52:41] [config] vocabs:
[2022-03-02 23:52:41] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-03-02 23:52:41] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-03-02 23:52:41] [config] word-penalty: 0
[2022-03-02 23:52:41] [config] word-scores: false
[2022-03-02 23:52:41] [config] workspace: 15000
[2022-03-02 23:52:41] [config] Loaded model has been created with Marian v1.10.24; 12a1bfa 2021-10-11 16:59:52 +0100
[2022-03-02 23:52:41] Using synchronous SGD
[2022-03-02 23:52:41] [comm] Compiled without MPI support. Running as a single process on r17g06.bullx
[2022-03-02 23:52:41] Synced seed 1111
[2022-03-02 23:52:41] [data] Loading vocabulary from JSON/Yaml file /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-03-02 23:52:41] [data] Setting vocabulary size for input 0 to 55,026
[2022-03-02 23:52:41] [data] Loading vocabulary from JSON/Yaml file /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-03-02 23:52:42] [data] Setting vocabulary size for input 1 to 55,026
[2022-03-02 23:52:42] [batching] Collecting statistics for batch fitting with step size 10
[2022-03-02 23:52:43] [memory] Extending reserved space to 15104 MB (device gpu0)
[2022-03-02 23:52:46] [memory] Extending reserved space to 15104 MB (device gpu1)
[2022-03-02 23:52:46] [comm] Using NCCL 2.8.3 for GPU communication
[2022-03-02 23:52:46] [comm] Using global sharding
[2022-03-02 23:52:47] [comm] NCCLCommunicators constructed successfully
[2022-03-02 23:52:47] [training] Using 2 GPUs
[2022-03-02 23:52:47] [logits] Applying loss function for 1 factor(s)
[2022-03-02 23:52:47] [memory] Reserving 887 MB, device gpu0
[2022-03-02 23:52:51] [gpu] 16-bit TensorCores enabled for float32 matrix operations
[2022-03-02 23:52:51] [memory] Reserving 887 MB, device gpu0
[2022-03-02 23:53:19] [batching] Done. Typical MB size is 27,468 target words
[2022-03-02 23:53:19] [memory] Extending reserved space to 15104 MB (device gpu0)
[2022-03-02 23:53:19] [memory] Extending reserved space to 15104 MB (device gpu1)
[2022-03-02 23:53:19] [comm] Using NCCL 2.8.3 for GPU communication
[2022-03-02 23:53:19] [comm] Using global sharding
[2022-03-02 23:53:20] [comm] NCCLCommunicators constructed successfully
[2022-03-02 23:53:20] [training] Using 2 GPUs
[2022-03-02 23:53:20] Loading model from /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-02 23:53:22] Loading model from /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-02 23:53:32] Allocating memory for general optimizer shards
[2022-03-02 23:53:32] [memory] Reserving 443 MB, device gpu0
[2022-03-02 23:53:32] [memory] Reserving 443 MB, device gpu1
[2022-03-02 23:53:32] Loading Adam parameters
[2022-03-02 23:53:33] [memory] Reserving 887 MB, device gpu0
[2022-03-02 23:53:33] [memory] Reserving 887 MB, device gpu1
[2022-03-02 23:53:33] [memory] Reserving 887 MB, device gpu0
[2022-03-02 23:53:33] [memory] Reserving 887 MB, device gpu1
[2022-03-02 23:53:34] [training] Master parameters and optimizers restored from training checkpoint /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-02 23:53:34] Training started
[2022-03-02 23:53:57] [training] Batches are processed as 1 process(es) x 2 devices/process
[2022-03-02 23:53:57] [memory] Reserving 887 MB, device gpu0
[2022-03-02 23:53:57] [memory] Reserving 887 MB, device gpu1
[2022-03-02 23:54:00] Parameter type float32, optimization type float32, casting types false
[2022-03-03 04:08:53] Ep. 1 : Up. 810000 : Sen. 8,809,207 : Cost 2.29487848 : Time 15334.52s : 14809.56 words/s : gNorm 0.5705
[2022-03-03 04:08:53] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-03 04:08:57] Saving Adam parameters
[2022-03-03 04:09:02] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-03 04:09:13] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-03-03 04:09:15] [valid] Ep. 1 : Up. 810000 : perplexity : 2.0246 : new best
[2022-03-03 08:24:17] Ep. 1 : Up. 820000 : Sen. 17,604,389 : Cost 2.29312539 : Time 15323.90s : 14817.87 words/s : gNorm 0.5911
[2022-03-03 08:24:17] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-03 08:24:26] Saving Adam parameters
[2022-03-03 08:24:29] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-03 08:24:41] [valid] Ep. 1 : Up. 820000 : perplexity : 2.0251 : stalled 1 times (last best: 2.0246)
[2022-03-03 12:39:23] Ep. 1 : Up. 830000 : Sen. 26,392,815 : Cost 2.29379511 : Time 15305.90s : 14829.80 words/s : gNorm 0.5776
[2022-03-03 12:39:23] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-03 12:39:27] Saving Adam parameters
[2022-03-03 12:39:30] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-03 12:39:41] [valid] Ep. 1 : Up. 830000 : perplexity : 2.02464 : stalled 2 times (last best: 2.0246)
[2022-03-03 16:54:32] Ep. 1 : Up. 840000 : Sen. 35,198,052 : Cost 2.29239798 : Time 15308.46s : 14836.87 words/s : gNorm 0.5477
[2022-03-03 16:54:32] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-03 16:54:36] Saving Adam parameters
[2022-03-03 16:54:39] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-03 16:54:50] [valid] Ep. 1 : Up. 840000 : perplexity : 2.02475 : stalled 3 times (last best: 2.0246)
[2022-03-03 21:09:22] Ep. 1 : Up. 850000 : Sen. 44,010,124 : Cost 2.29253626 : Time 15289.50s : 14853.77 words/s : gNorm 0.5977
[2022-03-03 21:09:22] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-03 21:09:29] Saving Adam parameters
[2022-03-03 21:09:36] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-03 21:09:54] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-03-03 21:09:56] [valid] Ep. 1 : Up. 850000 : perplexity : 2.02423 : new best
[2022-03-04 01:24:17] Ep. 1 : Up. 860000 : Sen. 52,814,651 : Cost 2.29238009 : Time 15295.41s : 14838.60 words/s : gNorm 0.6018
[2022-03-04 01:24:18] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-04 01:24:21] Saving Adam parameters
[2022-03-04 01:24:25] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-04 01:24:37] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-03-04 01:24:39] [valid] Ep. 1 : Up. 860000 : perplexity : 2.02266 : new best
[2022-03-04 05:39:43] Ep. 1 : Up. 870000 : Sen. 61,603,455 : Cost 2.29140544 : Time 15325.25s : 14823.98 words/s : gNorm 0.6048
[2022-03-04 05:39:43] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-04 05:39:46] Saving Adam parameters
[2022-03-04 05:39:50] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-04 05:40:00] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-03-04 05:40:02] [valid] Ep. 1 : Up. 870000 : perplexity : 2.02241 : new best
[2022-03-04 09:54:59] Ep. 1 : Up. 880000 : Sen. 70,427,669 : Cost 2.29139614 : Time 15315.87s : 14820.21 words/s : gNorm 0.5895
[2022-03-04 09:54:59] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-04 09:55:03] Saving Adam parameters
[2022-03-04 09:55:06] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-04 09:55:17] [valid] Ep. 1 : Up. 880000 : perplexity : 2.02279 : stalled 1 times (last best: 2.02241)
[2022-03-04 14:10:19] Ep. 1 : Up. 890000 : Sen. 79,211,349 : Cost 2.29040360 : Time 15319.92s : 14830.51 words/s : gNorm 0.5978
[2022-03-04 14:10:19] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-04 14:10:22] Saving Adam parameters
[2022-03-04 14:10:26] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-04 14:10:36] [valid] Ep. 1 : Up. 890000 : perplexity : 2.02273 : stalled 2 times (last best: 2.02241)
[2022-03-04 18:25:13] Ep. 1 : Up. 900000 : Sen. 88,016,176 : Cost 2.29027796 : Time 15293.80s : 14850.52 words/s : gNorm 0.6027
[2022-03-04 18:25:13] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-04 18:25:19] Saving Adam parameters
[2022-03-04 18:25:26] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-04 18:25:38] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-03-04 18:25:40] [valid] Ep. 1 : Up. 900000 : perplexity : 2.02183 : new best
[2022-03-04 22:40:42] Ep. 1 : Up. 910000 : Sen. 96,828,380 : Cost 2.29039359 : Time 15329.17s : 14816.52 words/s : gNorm 0.6239
[2022-03-04 22:40:42] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-04 22:40:45] Saving Adam parameters
[2022-03-04 22:40:49] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-04 22:41:00] [valid] Ep. 1 : Up. 910000 : perplexity : 2.02194 : stalled 1 times (last best: 2.02183)
[2022-03-05 02:56:11] Ep. 1 : Up. 920000 : Sen. 105,605,925 : Cost 2.28890371 : Time 15329.48s : 14815.53 words/s : gNorm 0.5764
[2022-03-05 02:56:11] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-05 02:56:15] Saving Adam parameters
[2022-03-05 02:56:18] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-05 02:56:29] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-03-05 02:56:31] [valid] Ep. 1 : Up. 920000 : perplexity : 2.02062 : new best
[2022-03-05 07:11:47] Ep. 1 : Up. 930000 : Sen. 114,405,705 : Cost 2.28945208 : Time 15335.37s : 14800.55 words/s : gNorm 0.6075
[2022-03-05 07:11:47] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-05 07:11:50] Saving Adam parameters
[2022-03-05 07:11:54] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-05 07:12:05] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-03-05 07:12:07] [valid] Ep. 1 : Up. 930000 : perplexity : 2.02005 : new best
[2022-03-05 11:27:16] Ep. 1 : Up. 940000 : Sen. 123,216,854 : Cost 2.28937101 : Time 15329.39s : 14818.11 words/s : gNorm 0.6001
[2022-03-05 11:27:16] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-05 11:27:20] Saving Adam parameters
[2022-03-05 11:27:24] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-05 11:27:34] [valid] Ep. 1 : Up. 940000 : perplexity : 2.02044 : stalled 1 times (last best: 2.02005)
[2022-03-05 15:42:49] Ep. 1 : Up. 950000 : Sen. 132,035,340 : Cost 2.28889132 : Time 15332.94s : 14819.74 words/s : gNorm 0.5959
[2022-03-05 15:42:49] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-05 15:42:53] Saving Adam parameters
[2022-03-05 15:42:57] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-05 15:43:07] [valid] Ep. 1 : Up. 950000 : perplexity : 2.02214 : stalled 2 times (last best: 2.02005)
[2022-03-05 19:57:28] Ep. 1 : Up. 960000 : Sen. 140,844,597 : Cost 2.28904414 : Time 15278.14s : 14854.59 words/s : gNorm 0.5839
[2022-03-05 19:57:28] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-05 19:57:31] Saving Adam parameters
[2022-03-05 19:57:35] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-05 19:57:45] [valid] Ep. 1 : Up. 960000 : perplexity : 2.02096 : stalled 3 times (last best: 2.02005)
[2022-03-06 22:04:58] [marian] Marian v1.10.24; 12a1bfa 2021-10-11 16:59:52 +0100
[2022-03-06 22:04:58] [marian] Running on r15g06.bullx as process 91902 with command line:
[2022-03-06 22:04:58] [marian] /projappl/project_2001194/marian-dev/build/marian --task transformer-big --optimizer-delay 2 --no-restore-corpus --early-stopping 10 --valid-freq 10000 --valid-sets /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/val/Tatoeba-dev-v2021-08-07.src.spm32k /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/val/Tatoeba-dev-v2021-08-07.trg.spm32k --valid-metrics perplexity --valid-mini-batch 16 --valid-max-length 100 --valid-log /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.valid1.log --beam-size 6 --normalize 1 --allow-unk --workspace 15000 --model /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz --train-sets /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/train/opusTCv20210807+bt.src.clean.spm32k.gz /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/train/opusTCv20210807+bt.trg.clean.spm32k.gz --vocabs /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.vocab.yml /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.vocab.yml --save-freq 10000 --disp-freq 10000 --log /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.train1.log --devices 0 1 --seed 1111 --tempdir /run/nvme/job_10839843/tmp --shuffle batches --sharding local --overwrite --keep-best
[2022-03-06 22:05:01] [config] after: 0e
[2022-03-06 22:05:01] [config] after-batches: 0
[2022-03-06 22:05:01] [config] after-epochs: 0
[2022-03-06 22:05:01] [config] all-caps-every: 0
[2022-03-06 22:05:01] [config] allow-unk: true
[2022-03-06 22:05:01] [config] authors: false
[2022-03-06 22:05:01] [config] beam-size: 6
[2022-03-06 22:05:01] [config] bert-class-symbol: "[CLS]"
[2022-03-06 22:05:01] [config] bert-mask-symbol: "[MASK]"
[2022-03-06 22:05:01] [config] bert-masking-fraction: 0.15
[2022-03-06 22:05:01] [config] bert-sep-symbol: "[SEP]"
[2022-03-06 22:05:01] [config] bert-train-type-embeddings: true
[2022-03-06 22:05:01] [config] bert-type-vocab-size: 2
[2022-03-06 22:05:01] [config] build-info: ""
[2022-03-06 22:05:01] [config] check-gradient-nan: false
[2022-03-06 22:05:01] [config] check-nan: false
[2022-03-06 22:05:01] [config] cite: false
[2022-03-06 22:05:01] [config] clip-norm: 0
[2022-03-06 22:05:01] [config] cost-scaling:
[2022-03-06 22:05:01] [config]   []
[2022-03-06 22:05:01] [config] cost-type: ce-mean-words
[2022-03-06 22:05:01] [config] cpu-threads: 0
[2022-03-06 22:05:01] [config] data-weighting: ""
[2022-03-06 22:05:01] [config] data-weighting-type: sentence
[2022-03-06 22:05:01] [config] dec-cell: gru
[2022-03-06 22:05:01] [config] dec-cell-base-depth: 2
[2022-03-06 22:05:01] [config] dec-cell-high-depth: 1
[2022-03-06 22:05:01] [config] dec-depth: 6
[2022-03-06 22:05:01] [config] devices:
[2022-03-06 22:05:01] [config]   - 0
[2022-03-06 22:05:01] [config]   - 1
[2022-03-06 22:05:01] [config] dim-emb: 1024
[2022-03-06 22:05:01] [config] dim-rnn: 1024
[2022-03-06 22:05:01] [config] dim-vocabs:
[2022-03-06 22:05:01] [config]   - 55026
[2022-03-06 22:05:01] [config]   - 55026
[2022-03-06 22:05:01] [config] disp-first: 0
[2022-03-06 22:05:01] [config] disp-freq: 10000
[2022-03-06 22:05:01] [config] disp-label-counts: true
[2022-03-06 22:05:01] [config] dropout-rnn: 0
[2022-03-06 22:05:01] [config] dropout-src: 0
[2022-03-06 22:05:01] [config] dropout-trg: 0
[2022-03-06 22:05:01] [config] dump-config: ""
[2022-03-06 22:05:01] [config] dynamic-gradient-scaling:
[2022-03-06 22:05:01] [config]   []
[2022-03-06 22:05:01] [config] early-stopping: 10
[2022-03-06 22:05:01] [config] early-stopping-on: first
[2022-03-06 22:05:01] [config] embedding-fix-src: false
[2022-03-06 22:05:01] [config] embedding-fix-trg: false
[2022-03-06 22:05:01] [config] embedding-normalization: false
[2022-03-06 22:05:01] [config] embedding-vectors:
[2022-03-06 22:05:01] [config]   []
[2022-03-06 22:05:01] [config] enc-cell: gru
[2022-03-06 22:05:01] [config] enc-cell-depth: 1
[2022-03-06 22:05:01] [config] enc-depth: 6
[2022-03-06 22:05:01] [config] enc-type: bidirectional
[2022-03-06 22:05:01] [config] english-title-case-every: 0
[2022-03-06 22:05:01] [config] exponential-smoothing: 0.0001
[2022-03-06 22:05:01] [config] factor-weight: 1
[2022-03-06 22:05:01] [config] factors-combine: sum
[2022-03-06 22:05:01] [config] factors-dim-emb: 0
[2022-03-06 22:05:01] [config] gradient-checkpointing: false
[2022-03-06 22:05:01] [config] gradient-norm-average-window: 100
[2022-03-06 22:05:01] [config] guided-alignment: none
[2022-03-06 22:05:01] [config] guided-alignment-cost: mse
[2022-03-06 22:05:01] [config] guided-alignment-weight: 0.1
[2022-03-06 22:05:01] [config] ignore-model-config: false
[2022-03-06 22:05:01] [config] input-types:
[2022-03-06 22:05:01] [config]   []
[2022-03-06 22:05:01] [config] interpolate-env-vars: false
[2022-03-06 22:05:01] [config] keep-best: true
[2022-03-06 22:05:01] [config] label-smoothing: 0.1
[2022-03-06 22:05:01] [config] layer-normalization: false
[2022-03-06 22:05:01] [config] learn-rate: 0.0002
[2022-03-06 22:05:01] [config] lemma-dependency: ""
[2022-03-06 22:05:01] [config] lemma-dim-emb: 0
[2022-03-06 22:05:01] [config] log: /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.train1.log
[2022-03-06 22:05:01] [config] log-level: info
[2022-03-06 22:05:01] [config] log-time-zone: ""
[2022-03-06 22:05:01] [config] logical-epoch:
[2022-03-06 22:05:01] [config]   - 1e
[2022-03-06 22:05:01] [config]   - 0
[2022-03-06 22:05:01] [config] lr-decay: 0
[2022-03-06 22:05:01] [config] lr-decay-freq: 50000
[2022-03-06 22:05:01] [config] lr-decay-inv-sqrt:
[2022-03-06 22:05:01] [config]   - 8000
[2022-03-06 22:05:01] [config] lr-decay-repeat-warmup: false
[2022-03-06 22:05:01] [config] lr-decay-reset-optimizer: false
[2022-03-06 22:05:01] [config] lr-decay-start:
[2022-03-06 22:05:01] [config]   - 10
[2022-03-06 22:05:01] [config]   - 1
[2022-03-06 22:05:01] [config] lr-decay-strategy: epoch+stalled
[2022-03-06 22:05:01] [config] lr-report: false
[2022-03-06 22:05:01] [config] lr-warmup: 8000
[2022-03-06 22:05:01] [config] lr-warmup-at-reload: false
[2022-03-06 22:05:01] [config] lr-warmup-cycle: false
[2022-03-06 22:05:01] [config] lr-warmup-start-rate: 0
[2022-03-06 22:05:01] [config] max-length: 100
[2022-03-06 22:05:01] [config] max-length-crop: false
[2022-03-06 22:05:01] [config] max-length-factor: 3
[2022-03-06 22:05:01] [config] maxi-batch: 1000
[2022-03-06 22:05:01] [config] maxi-batch-sort: trg
[2022-03-06 22:05:01] [config] mini-batch: 1000
[2022-03-06 22:05:01] [config] mini-batch-fit: true
[2022-03-06 22:05:01] [config] mini-batch-fit-step: 10
[2022-03-06 22:05:01] [config] mini-batch-round-up: true
[2022-03-06 22:05:01] [config] mini-batch-track-lr: false
[2022-03-06 22:05:01] [config] mini-batch-warmup: 0
[2022-03-06 22:05:01] [config] mini-batch-words: 0
[2022-03-06 22:05:01] [config] mini-batch-words-ref: 0
[2022-03-06 22:05:01] [config] model: /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-06 22:05:01] [config] multi-loss-type: sum
[2022-03-06 22:05:01] [config] n-best: false
[2022-03-06 22:05:01] [config] no-nccl: false
[2022-03-06 22:05:01] [config] no-reload: false
[2022-03-06 22:05:01] [config] no-restore-corpus: true
[2022-03-06 22:05:01] [config] normalize: 1
[2022-03-06 22:05:01] [config] normalize-gradient: false
[2022-03-06 22:05:01] [config] num-devices: 0
[2022-03-06 22:05:01] [config] optimizer: adam
[2022-03-06 22:05:01] [config] optimizer-delay: 2
[2022-03-06 22:05:01] [config] optimizer-params:
[2022-03-06 22:05:01] [config]   - 0.9
[2022-03-06 22:05:01] [config]   - 0.998
[2022-03-06 22:05:01] [config]   - 1e-09
[2022-03-06 22:05:01] [config] output-omit-bias: false
[2022-03-06 22:05:01] [config] overwrite: true
[2022-03-06 22:05:01] [config] precision:
[2022-03-06 22:05:01] [config]   - float32
[2022-03-06 22:05:01] [config]   - float32
[2022-03-06 22:05:01] [config] pretrained-model: ""
[2022-03-06 22:05:01] [config] quantize-biases: false
[2022-03-06 22:05:01] [config] quantize-bits: 0
[2022-03-06 22:05:01] [config] quantize-log-based: false
[2022-03-06 22:05:01] [config] quantize-optimization-steps: 0
[2022-03-06 22:05:01] [config] quiet: false
[2022-03-06 22:05:01] [config] quiet-translation: false
[2022-03-06 22:05:01] [config] relative-paths: false
[2022-03-06 22:05:01] [config] right-left: false
[2022-03-06 22:05:01] [config] save-freq: 10000
[2022-03-06 22:05:01] [config] seed: 1111
[2022-03-06 22:05:01] [config] sentencepiece-alphas:
[2022-03-06 22:05:01] [config]   []
[2022-03-06 22:05:01] [config] sentencepiece-max-lines: 2000000
[2022-03-06 22:05:01] [config] sentencepiece-options: ""
[2022-03-06 22:05:01] [config] sharding: local
[2022-03-06 22:05:01] [config] shuffle: batches
[2022-03-06 22:05:01] [config] shuffle-in-ram: false
[2022-03-06 22:05:01] [config] sigterm: save-and-exit
[2022-03-06 22:05:01] [config] skip: false
[2022-03-06 22:05:01] [config] sqlite: ""
[2022-03-06 22:05:01] [config] sqlite-drop: false
[2022-03-06 22:05:01] [config] sync-freq: 200u
[2022-03-06 22:05:01] [config] sync-sgd: true
[2022-03-06 22:05:01] [config] tempdir: /run/nvme/job_10839843/tmp
[2022-03-06 22:05:01] [config] tied-embeddings: false
[2022-03-06 22:05:01] [config] tied-embeddings-all: true
[2022-03-06 22:05:01] [config] tied-embeddings-src: false
[2022-03-06 22:05:01] [config] train-embedder-rank:
[2022-03-06 22:05:01] [config]   []
[2022-03-06 22:05:01] [config] train-sets:
[2022-03-06 22:05:01] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/train/opusTCv20210807+bt.src.clean.spm32k.gz
[2022-03-06 22:05:01] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/train/opusTCv20210807+bt.trg.clean.spm32k.gz
[2022-03-06 22:05:01] [config] transformer-aan-activation: swish
[2022-03-06 22:05:01] [config] transformer-aan-depth: 2
[2022-03-06 22:05:01] [config] transformer-aan-nogate: false
[2022-03-06 22:05:01] [config] transformer-decoder-autoreg: self-attention
[2022-03-06 22:05:01] [config] transformer-depth-scaling: false
[2022-03-06 22:05:01] [config] transformer-dim-aan: 2048
[2022-03-06 22:05:01] [config] transformer-dim-ffn: 4096
[2022-03-06 22:05:01] [config] transformer-dropout: 0.1
[2022-03-06 22:05:01] [config] transformer-dropout-attention: 0
[2022-03-06 22:05:01] [config] transformer-dropout-ffn: 0
[2022-03-06 22:05:01] [config] transformer-ffn-activation: relu
[2022-03-06 22:05:01] [config] transformer-ffn-depth: 2
[2022-03-06 22:05:01] [config] transformer-guided-alignment-layer: last
[2022-03-06 22:05:01] [config] transformer-heads: 16
[2022-03-06 22:05:01] [config] transformer-no-projection: false
[2022-03-06 22:05:01] [config] transformer-pool: false
[2022-03-06 22:05:01] [config] transformer-postprocess: dan
[2022-03-06 22:05:01] [config] transformer-postprocess-emb: d
[2022-03-06 22:05:01] [config] transformer-postprocess-top: ""
[2022-03-06 22:05:01] [config] transformer-preprocess: ""
[2022-03-06 22:05:01] [config] transformer-tied-layers:
[2022-03-06 22:05:01] [config]   []
[2022-03-06 22:05:01] [config] transformer-train-position-embeddings: false
[2022-03-06 22:05:01] [config] tsv: false
[2022-03-06 22:05:01] [config] tsv-fields: 0
[2022-03-06 22:05:01] [config] type: transformer
[2022-03-06 22:05:01] [config] ulr: false
[2022-03-06 22:05:01] [config] ulr-dim-emb: 0
[2022-03-06 22:05:01] [config] ulr-dropout: 0
[2022-03-06 22:05:01] [config] ulr-keys-vectors: ""
[2022-03-06 22:05:01] [config] ulr-query-vectors: ""
[2022-03-06 22:05:01] [config] ulr-softmax-temperature: 1
[2022-03-06 22:05:01] [config] ulr-trainable-transformation: false
[2022-03-06 22:05:01] [config] unlikelihood-loss: false
[2022-03-06 22:05:01] [config] valid-freq: 10000
[2022-03-06 22:05:01] [config] valid-log: /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.valid1.log
[2022-03-06 22:05:01] [config] valid-max-length: 100
[2022-03-06 22:05:01] [config] valid-metrics:
[2022-03-06 22:05:01] [config]   - perplexity
[2022-03-06 22:05:01] [config] valid-mini-batch: 16
[2022-03-06 22:05:01] [config] valid-reset-stalled: false
[2022-03-06 22:05:01] [config] valid-script-args:
[2022-03-06 22:05:01] [config]   []
[2022-03-06 22:05:01] [config] valid-script-path: ""
[2022-03-06 22:05:01] [config] valid-sets:
[2022-03-06 22:05:01] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/val/Tatoeba-dev-v2021-08-07.src.spm32k
[2022-03-06 22:05:01] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/val/Tatoeba-dev-v2021-08-07.trg.spm32k
[2022-03-06 22:05:01] [config] valid-translation-output: ""
[2022-03-06 22:05:01] [config] version: v1.10.24; 12a1bfa 2021-10-11 16:59:52 +0100
[2022-03-06 22:05:01] [config] vocabs:
[2022-03-06 22:05:01] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-03-06 22:05:01] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-03-06 22:05:01] [config] word-penalty: 0
[2022-03-06 22:05:01] [config] word-scores: false
[2022-03-06 22:05:01] [config] workspace: 15000
[2022-03-06 22:05:01] [config] Loaded model has been created with Marian v1.10.24; 12a1bfa 2021-10-11 16:59:52 +0100
[2022-03-06 22:05:01] Using synchronous SGD
[2022-03-06 22:05:01] [comm] Compiled without MPI support. Running as a single process on r15g06.bullx
[2022-03-06 22:05:01] Synced seed 1111
[2022-03-06 22:05:01] [data] Loading vocabulary from JSON/Yaml file /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-03-06 22:05:01] [data] Setting vocabulary size for input 0 to 55,026
[2022-03-06 22:05:01] [data] Loading vocabulary from JSON/Yaml file /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-03-06 22:05:01] [data] Setting vocabulary size for input 1 to 55,026
[2022-03-06 22:05:02] [batching] Collecting statistics for batch fitting with step size 10
[2022-03-06 22:05:02] [memory] Extending reserved space to 15104 MB (device gpu0)
[2022-03-06 22:05:02] [memory] Extending reserved space to 15104 MB (device gpu1)
[2022-03-06 22:05:03] [comm] Using NCCL 2.8.3 for GPU communication
[2022-03-06 22:05:03] [comm] Using global sharding
[2022-03-06 22:05:03] [comm] NCCLCommunicators constructed successfully
[2022-03-06 22:05:03] [training] Using 2 GPUs
[2022-03-06 22:05:03] [logits] Applying loss function for 1 factor(s)
[2022-03-06 22:05:03] [memory] Reserving 887 MB, device gpu0
[2022-03-06 22:05:03] [gpu] 16-bit TensorCores enabled for float32 matrix operations
[2022-03-06 22:05:04] [memory] Reserving 887 MB, device gpu0
[2022-03-06 22:05:32] [batching] Done. Typical MB size is 27,468 target words
[2022-03-06 22:05:32] [memory] Extending reserved space to 15104 MB (device gpu0)
[2022-03-06 22:05:32] [memory] Extending reserved space to 15104 MB (device gpu1)
[2022-03-06 22:05:32] [comm] Using NCCL 2.8.3 for GPU communication
[2022-03-06 22:05:32] [comm] Using global sharding
[2022-03-06 22:05:33] [comm] NCCLCommunicators constructed successfully
[2022-03-06 22:05:33] [training] Using 2 GPUs
[2022-03-06 22:05:33] Loading model from /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-06 22:05:35] Loading model from /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-06 22:05:44] Allocating memory for general optimizer shards
[2022-03-06 22:05:44] [memory] Reserving 443 MB, device gpu0
[2022-03-06 22:05:44] [memory] Reserving 443 MB, device gpu1
[2022-03-06 22:05:44] Loading Adam parameters
[2022-03-06 22:05:45] [memory] Reserving 887 MB, device gpu0
[2022-03-06 22:05:45] [memory] Reserving 887 MB, device gpu1
[2022-03-06 22:05:45] [memory] Reserving 887 MB, device gpu0
[2022-03-06 22:05:46] [memory] Reserving 887 MB, device gpu1
[2022-03-06 22:05:46] [training] Master parameters and optimizers restored from training checkpoint /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-06 22:05:46] Training started
[2022-03-06 22:06:10] [training] Batches are processed as 1 process(es) x 2 devices/process
[2022-03-06 22:06:10] [memory] Reserving 887 MB, device gpu0
[2022-03-06 22:06:10] [memory] Reserving 887 MB, device gpu1
[2022-03-06 22:06:12] Parameter type float32, optimization type float32, casting types false
[2022-03-07 02:20:37] Ep. 1 : Up. 970000 : Sen. 8,809,207 : Cost 2.28671765 : Time 15304.82s : 14838.30 words/s : gNorm 0.5931
[2022-03-07 02:20:37] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-07 02:20:40] Saving Adam parameters
[2022-03-07 02:20:44] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-07 02:20:54] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-03-07 02:20:56] [valid] Ep. 1 : Up. 970000 : perplexity : 2.01999 : new best
[2022-03-07 06:35:02] Ep. 1 : Up. 980000 : Sen. 17,604,389 : Cost 2.28511834 : Time 15265.23s : 14874.82 words/s : gNorm 0.6149
[2022-03-07 06:35:02] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-07 06:35:05] Saving Adam parameters
[2022-03-07 06:35:08] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-07 06:35:19] [valid] Ep. 1 : Up. 980000 : perplexity : 2.02041 : stalled 1 times (last best: 2.01999)
[2022-03-07 10:49:38] Ep. 1 : Up. 990000 : Sen. 26,392,815 : Cost 2.28590131 : Time 15276.43s : 14858.40 words/s : gNorm 0.6004
[2022-03-07 10:49:38] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-07 10:49:42] Saving Adam parameters
[2022-03-07 10:49:45] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-07 10:49:56] [valid] Ep. 1 : Up. 990000 : perplexity : 2.02007 : stalled 2 times (last best: 2.01999)
[2022-03-07 15:04:31] Ep. 1 : Up. 1000000 : Sen. 35,198,052 : Cost 2.28463030 : Time 15290.86s : 14853.95 words/s : gNorm 0.5670
[2022-03-07 15:04:31] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-07 15:04:42] Saving Adam parameters
[2022-03-07 15:04:45] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-07 15:05:02] [valid] Ep. 1 : Up. 1000000 : perplexity : 2.02089 : stalled 3 times (last best: 2.01999)
[2022-03-07 19:19:26] Ep. 1 : Up. 1010000 : Sen. 44,010,124 : Cost 2.28485155 : Time 15294.96s : 14848.46 words/s : gNorm 0.6194
[2022-03-07 19:19:27] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-07 19:19:34] Saving Adam parameters
[2022-03-07 19:19:41] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-07 19:19:59] [valid] Ep. 1 : Up. 1010000 : perplexity : 2.02028 : stalled 4 times (last best: 2.01999)
[2022-03-07 23:33:53] Ep. 1 : Up. 1020000 : Sen. 52,814,651 : Cost 2.28481603 : Time 15266.42s : 14866.78 words/s : gNorm 0.6229
[2022-03-07 23:33:53] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-07 23:33:57] Saving Adam parameters
[2022-03-07 23:34:00] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-07 23:34:11] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-03-07 23:34:13] [valid] Ep. 1 : Up. 1020000 : perplexity : 2.01878 : new best
[2022-03-08 03:48:34] Ep. 1 : Up. 1030000 : Sen. 61,603,455 : Cost 2.28395200 : Time 15280.67s : 14867.23 words/s : gNorm 0.6281
[2022-03-08 03:48:34] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-08 03:48:37] Saving Adam parameters
[2022-03-08 03:48:41] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-08 03:48:52] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-03-08 03:48:54] [valid] Ep. 1 : Up. 1030000 : perplexity : 2.01858 : new best
[2022-03-08 08:03:17] Ep. 1 : Up. 1040000 : Sen. 70,427,669 : Cost 2.28400469 : Time 15283.04s : 14852.05 words/s : gNorm 0.6106
[2022-03-08 08:03:17] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-08 08:03:20] Saving Adam parameters
[2022-03-08 08:03:24] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-08 08:03:34] [valid] Ep. 1 : Up. 1040000 : perplexity : 2.01896 : stalled 1 times (last best: 2.01858)
[2022-03-08 12:17:40] Ep. 1 : Up. 1050000 : Sen. 79,211,349 : Cost 2.28312397 : Time 15262.54s : 14886.27 words/s : gNorm 0.6195
[2022-03-08 12:17:40] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-08 12:17:44] Saving Adam parameters
[2022-03-08 12:17:47] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-08 12:17:58] [valid] Ep. 1 : Up. 1050000 : perplexity : 2.01911 : stalled 2 times (last best: 2.01858)
[2022-03-08 16:32:06] Ep. 1 : Up. 1060000 : Sen. 88,016,176 : Cost 2.28307199 : Time 15265.89s : 14877.67 words/s : gNorm 0.6238
[2022-03-08 16:32:07] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-08 16:32:14] Saving Adam parameters
[2022-03-08 16:32:21] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-08 16:32:35] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-03-08 16:32:38] [valid] Ep. 1 : Up. 1060000 : perplexity : 2.01844 : new best
[2022-03-08 20:47:15] Ep. 1 : Up. 1070000 : Sen. 96,828,380 : Cost 2.28327584 : Time 15308.45s : 14836.57 words/s : gNorm 0.6459
[2022-03-08 20:47:15] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-08 20:47:19] Saving Adam parameters
[2022-03-08 20:47:22] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-08 20:47:33] [valid] Ep. 1 : Up. 1070000 : perplexity : 2.01846 : stalled 1 times (last best: 2.01844)
[2022-03-09 01:02:04] Ep. 1 : Up. 1080000 : Sen. 105,605,925 : Cost 2.28187442 : Time 15288.76s : 14854.99 words/s : gNorm 0.5933
[2022-03-09 01:02:04] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-09 01:02:07] Saving Adam parameters
[2022-03-09 01:02:11] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-09 01:02:21] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-03-09 01:02:23] [valid] Ep. 1 : Up. 1080000 : perplexity : 2.01773 : new best
[2022-03-09 05:17:07] Ep. 1 : Up. 1090000 : Sen. 114,405,705 : Cost 2.28249502 : Time 15303.34s : 14831.53 words/s : gNorm 0.6288
[2022-03-09 05:17:07] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-09 05:17:11] Saving Adam parameters
[2022-03-09 05:17:14] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-09 05:17:25] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-03-09 05:17:28] [valid] Ep. 1 : Up. 1090000 : perplexity : 2.01737 : new best
[2022-03-09 09:32:50] Ep. 1 : Up. 1100000 : Sen. 123,216,854 : Cost 2.28249860 : Time 15342.53s : 14805.42 words/s : gNorm 0.6178
[2022-03-09 09:32:50] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-09 09:32:56] Saving Adam parameters
[2022-03-09 09:33:00] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-09 09:33:25] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-03-09 09:33:31] [valid] Ep. 1 : Up. 1100000 : perplexity : 2.01699 : new best
[2022-03-09 13:49:08] Ep. 1 : Up. 1110000 : Sen. 132,035,340 : Cost 2.28209138 : Time 15377.66s : 14776.64 words/s : gNorm 0.6145
[2022-03-09 13:49:08] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-09 13:49:14] Saving Adam parameters
[2022-03-09 13:49:18] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-09 13:49:39] [valid] Ep. 1 : Up. 1110000 : perplexity : 2.01873 : stalled 1 times (last best: 2.01699)
[2022-03-09 18:04:29] Ep. 1 : Up. 1120000 : Sen. 140,844,597 : Cost 2.28231692 : Time 15321.49s : 14812.56 words/s : gNorm 0.6032
[2022-03-09 18:04:29] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-09 18:04:34] Saving Adam parameters
[2022-03-09 18:04:38] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-09 18:04:53] [valid] Ep. 1 : Up. 1120000 : perplexity : 2.01769 : stalled 2 times (last best: 2.01699)
[2022-03-10 05:30:10] [marian] Marian v1.10.24; 12a1bfa 2021-10-11 16:59:52 +0100
[2022-03-10 05:30:10] [marian] Running on r17g05.bullx as process 85105 with command line:
[2022-03-10 05:30:10] [marian] /projappl/project_2001194/marian-dev/build/marian --task transformer-big --optimizer-delay 2 --no-restore-corpus --early-stopping 10 --valid-freq 10000 --valid-sets /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/val/Tatoeba-dev-v2021-08-07.src.spm32k /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/val/Tatoeba-dev-v2021-08-07.trg.spm32k --valid-metrics perplexity --valid-mini-batch 16 --valid-max-length 100 --valid-log /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.valid1.log --beam-size 6 --normalize 1 --allow-unk --workspace 15000 --model /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz --train-sets /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/train/opusTCv20210807+bt.src.clean.spm32k.gz /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/train/opusTCv20210807+bt.trg.clean.spm32k.gz --vocabs /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.vocab.yml /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.vocab.yml --save-freq 10000 --disp-freq 10000 --log /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.train1.log --devices 0 1 --seed 1111 --tempdir /run/nvme/job_10884129/tmp --shuffle batches --sharding local --overwrite --keep-best
[2022-03-10 05:30:12] [config] after: 0e
[2022-03-10 05:30:12] [config] after-batches: 0
[2022-03-10 05:30:12] [config] after-epochs: 0
[2022-03-10 05:30:12] [config] all-caps-every: 0
[2022-03-10 05:30:12] [config] allow-unk: true
[2022-03-10 05:30:12] [config] authors: false
[2022-03-10 05:30:12] [config] beam-size: 6
[2022-03-10 05:30:12] [config] bert-class-symbol: "[CLS]"
[2022-03-10 05:30:12] [config] bert-mask-symbol: "[MASK]"
[2022-03-10 05:30:12] [config] bert-masking-fraction: 0.15
[2022-03-10 05:30:12] [config] bert-sep-symbol: "[SEP]"
[2022-03-10 05:30:12] [config] bert-train-type-embeddings: true
[2022-03-10 05:30:12] [config] bert-type-vocab-size: 2
[2022-03-10 05:30:12] [config] build-info: ""
[2022-03-10 05:30:12] [config] check-gradient-nan: false
[2022-03-10 05:30:12] [config] check-nan: false
[2022-03-10 05:30:12] [config] cite: false
[2022-03-10 05:30:12] [config] clip-norm: 0
[2022-03-10 05:30:12] [config] cost-scaling:
[2022-03-10 05:30:12] [config]   []
[2022-03-10 05:30:12] [config] cost-type: ce-mean-words
[2022-03-10 05:30:12] [config] cpu-threads: 0
[2022-03-10 05:30:12] [config] data-weighting: ""
[2022-03-10 05:30:12] [config] data-weighting-type: sentence
[2022-03-10 05:30:12] [config] dec-cell: gru
[2022-03-10 05:30:12] [config] dec-cell-base-depth: 2
[2022-03-10 05:30:12] [config] dec-cell-high-depth: 1
[2022-03-10 05:30:12] [config] dec-depth: 6
[2022-03-10 05:30:12] [config] devices:
[2022-03-10 05:30:12] [config]   - 0
[2022-03-10 05:30:12] [config]   - 1
[2022-03-10 05:30:12] [config] dim-emb: 1024
[2022-03-10 05:30:12] [config] dim-rnn: 1024
[2022-03-10 05:30:12] [config] dim-vocabs:
[2022-03-10 05:30:12] [config]   - 55026
[2022-03-10 05:30:12] [config]   - 55026
[2022-03-10 05:30:12] [config] disp-first: 0
[2022-03-10 05:30:12] [config] disp-freq: 10000
[2022-03-10 05:30:12] [config] disp-label-counts: true
[2022-03-10 05:30:12] [config] dropout-rnn: 0
[2022-03-10 05:30:12] [config] dropout-src: 0
[2022-03-10 05:30:12] [config] dropout-trg: 0
[2022-03-10 05:30:12] [config] dump-config: ""
[2022-03-10 05:30:12] [config] dynamic-gradient-scaling:
[2022-03-10 05:30:12] [config]   []
[2022-03-10 05:30:12] [config] early-stopping: 10
[2022-03-10 05:30:12] [config] early-stopping-on: first
[2022-03-10 05:30:12] [config] embedding-fix-src: false
[2022-03-10 05:30:12] [config] embedding-fix-trg: false
[2022-03-10 05:30:12] [config] embedding-normalization: false
[2022-03-10 05:30:12] [config] embedding-vectors:
[2022-03-10 05:30:12] [config]   []
[2022-03-10 05:30:12] [config] enc-cell: gru
[2022-03-10 05:30:12] [config] enc-cell-depth: 1
[2022-03-10 05:30:12] [config] enc-depth: 6
[2022-03-10 05:30:12] [config] enc-type: bidirectional
[2022-03-10 05:30:12] [config] english-title-case-every: 0
[2022-03-10 05:30:12] [config] exponential-smoothing: 0.0001
[2022-03-10 05:30:12] [config] factor-weight: 1
[2022-03-10 05:30:12] [config] factors-combine: sum
[2022-03-10 05:30:12] [config] factors-dim-emb: 0
[2022-03-10 05:30:12] [config] gradient-checkpointing: false
[2022-03-10 05:30:12] [config] gradient-norm-average-window: 100
[2022-03-10 05:30:12] [config] guided-alignment: none
[2022-03-10 05:30:12] [config] guided-alignment-cost: mse
[2022-03-10 05:30:12] [config] guided-alignment-weight: 0.1
[2022-03-10 05:30:12] [config] ignore-model-config: false
[2022-03-10 05:30:12] [config] input-types:
[2022-03-10 05:30:12] [config]   []
[2022-03-10 05:30:12] [config] interpolate-env-vars: false
[2022-03-10 05:30:12] [config] keep-best: true
[2022-03-10 05:30:12] [config] label-smoothing: 0.1
[2022-03-10 05:30:12] [config] layer-normalization: false
[2022-03-10 05:30:12] [config] learn-rate: 0.0002
[2022-03-10 05:30:12] [config] lemma-dependency: ""
[2022-03-10 05:30:12] [config] lemma-dim-emb: 0
[2022-03-10 05:30:12] [config] log: /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.train1.log
[2022-03-10 05:30:12] [config] log-level: info
[2022-03-10 05:30:12] [config] log-time-zone: ""
[2022-03-10 05:30:12] [config] logical-epoch:
[2022-03-10 05:30:12] [config]   - 1e
[2022-03-10 05:30:12] [config]   - 0
[2022-03-10 05:30:12] [config] lr-decay: 0
[2022-03-10 05:30:12] [config] lr-decay-freq: 50000
[2022-03-10 05:30:12] [config] lr-decay-inv-sqrt:
[2022-03-10 05:30:12] [config]   - 8000
[2022-03-10 05:30:12] [config] lr-decay-repeat-warmup: false
[2022-03-10 05:30:12] [config] lr-decay-reset-optimizer: false
[2022-03-10 05:30:12] [config] lr-decay-start:
[2022-03-10 05:30:12] [config]   - 10
[2022-03-10 05:30:12] [config]   - 1
[2022-03-10 05:30:12] [config] lr-decay-strategy: epoch+stalled
[2022-03-10 05:30:12] [config] lr-report: false
[2022-03-10 05:30:12] [config] lr-warmup: 8000
[2022-03-10 05:30:12] [config] lr-warmup-at-reload: false
[2022-03-10 05:30:12] [config] lr-warmup-cycle: false
[2022-03-10 05:30:12] [config] lr-warmup-start-rate: 0
[2022-03-10 05:30:12] [config] max-length: 100
[2022-03-10 05:30:12] [config] max-length-crop: false
[2022-03-10 05:30:12] [config] max-length-factor: 3
[2022-03-10 05:30:12] [config] maxi-batch: 1000
[2022-03-10 05:30:12] [config] maxi-batch-sort: trg
[2022-03-10 05:30:12] [config] mini-batch: 1000
[2022-03-10 05:30:12] [config] mini-batch-fit: true
[2022-03-10 05:30:12] [config] mini-batch-fit-step: 10
[2022-03-10 05:30:12] [config] mini-batch-round-up: true
[2022-03-10 05:30:12] [config] mini-batch-track-lr: false
[2022-03-10 05:30:12] [config] mini-batch-warmup: 0
[2022-03-10 05:30:12] [config] mini-batch-words: 0
[2022-03-10 05:30:12] [config] mini-batch-words-ref: 0
[2022-03-10 05:30:12] [config] model: /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-10 05:30:12] [config] multi-loss-type: sum
[2022-03-10 05:30:12] [config] n-best: false
[2022-03-10 05:30:12] [config] no-nccl: false
[2022-03-10 05:30:12] [config] no-reload: false
[2022-03-10 05:30:12] [config] no-restore-corpus: true
[2022-03-10 05:30:12] [config] normalize: 1
[2022-03-10 05:30:12] [config] normalize-gradient: false
[2022-03-10 05:30:12] [config] num-devices: 0
[2022-03-10 05:30:12] [config] optimizer: adam
[2022-03-10 05:30:12] [config] optimizer-delay: 2
[2022-03-10 05:30:12] [config] optimizer-params:
[2022-03-10 05:30:12] [config]   - 0.9
[2022-03-10 05:30:12] [config]   - 0.998
[2022-03-10 05:30:12] [config]   - 1e-09
[2022-03-10 05:30:12] [config] output-omit-bias: false
[2022-03-10 05:30:12] [config] overwrite: true
[2022-03-10 05:30:12] [config] precision:
[2022-03-10 05:30:12] [config]   - float32
[2022-03-10 05:30:12] [config]   - float32
[2022-03-10 05:30:12] [config] pretrained-model: ""
[2022-03-10 05:30:12] [config] quantize-biases: false
[2022-03-10 05:30:12] [config] quantize-bits: 0
[2022-03-10 05:30:12] [config] quantize-log-based: false
[2022-03-10 05:30:12] [config] quantize-optimization-steps: 0
[2022-03-10 05:30:12] [config] quiet: false
[2022-03-10 05:30:12] [config] quiet-translation: false
[2022-03-10 05:30:12] [config] relative-paths: false
[2022-03-10 05:30:12] [config] right-left: false
[2022-03-10 05:30:12] [config] save-freq: 10000
[2022-03-10 05:30:12] [config] seed: 1111
[2022-03-10 05:30:12] [config] sentencepiece-alphas:
[2022-03-10 05:30:12] [config]   []
[2022-03-10 05:30:12] [config] sentencepiece-max-lines: 2000000
[2022-03-10 05:30:12] [config] sentencepiece-options: ""
[2022-03-10 05:30:12] [config] sharding: local
[2022-03-10 05:30:12] [config] shuffle: batches
[2022-03-10 05:30:12] [config] shuffle-in-ram: false
[2022-03-10 05:30:12] [config] sigterm: save-and-exit
[2022-03-10 05:30:12] [config] skip: false
[2022-03-10 05:30:12] [config] sqlite: ""
[2022-03-10 05:30:12] [config] sqlite-drop: false
[2022-03-10 05:30:12] [config] sync-freq: 200u
[2022-03-10 05:30:12] [config] sync-sgd: true
[2022-03-10 05:30:12] [config] tempdir: /run/nvme/job_10884129/tmp
[2022-03-10 05:30:12] [config] tied-embeddings: false
[2022-03-10 05:30:12] [config] tied-embeddings-all: true
[2022-03-10 05:30:12] [config] tied-embeddings-src: false
[2022-03-10 05:30:12] [config] train-embedder-rank:
[2022-03-10 05:30:12] [config]   []
[2022-03-10 05:30:12] [config] train-sets:
[2022-03-10 05:30:12] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/train/opusTCv20210807+bt.src.clean.spm32k.gz
[2022-03-10 05:30:12] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/train/opusTCv20210807+bt.trg.clean.spm32k.gz
[2022-03-10 05:30:12] [config] transformer-aan-activation: swish
[2022-03-10 05:30:12] [config] transformer-aan-depth: 2
[2022-03-10 05:30:12] [config] transformer-aan-nogate: false
[2022-03-10 05:30:12] [config] transformer-decoder-autoreg: self-attention
[2022-03-10 05:30:12] [config] transformer-depth-scaling: false
[2022-03-10 05:30:12] [config] transformer-dim-aan: 2048
[2022-03-10 05:30:12] [config] transformer-dim-ffn: 4096
[2022-03-10 05:30:12] [config] transformer-dropout: 0.1
[2022-03-10 05:30:12] [config] transformer-dropout-attention: 0
[2022-03-10 05:30:12] [config] transformer-dropout-ffn: 0
[2022-03-10 05:30:12] [config] transformer-ffn-activation: relu
[2022-03-10 05:30:12] [config] transformer-ffn-depth: 2
[2022-03-10 05:30:12] [config] transformer-guided-alignment-layer: last
[2022-03-10 05:30:12] [config] transformer-heads: 16
[2022-03-10 05:30:12] [config] transformer-no-projection: false
[2022-03-10 05:30:12] [config] transformer-pool: false
[2022-03-10 05:30:12] [config] transformer-postprocess: dan
[2022-03-10 05:30:12] [config] transformer-postprocess-emb: d
[2022-03-10 05:30:12] [config] transformer-postprocess-top: ""
[2022-03-10 05:30:12] [config] transformer-preprocess: ""
[2022-03-10 05:30:12] [config] transformer-tied-layers:
[2022-03-10 05:30:12] [config]   []
[2022-03-10 05:30:12] [config] transformer-train-position-embeddings: false
[2022-03-10 05:30:12] [config] tsv: false
[2022-03-10 05:30:12] [config] tsv-fields: 0
[2022-03-10 05:30:12] [config] type: transformer
[2022-03-10 05:30:12] [config] ulr: false
[2022-03-10 05:30:12] [config] ulr-dim-emb: 0
[2022-03-10 05:30:12] [config] ulr-dropout: 0
[2022-03-10 05:30:12] [config] ulr-keys-vectors: ""
[2022-03-10 05:30:12] [config] ulr-query-vectors: ""
[2022-03-10 05:30:12] [config] ulr-softmax-temperature: 1
[2022-03-10 05:30:12] [config] ulr-trainable-transformation: false
[2022-03-10 05:30:12] [config] unlikelihood-loss: false
[2022-03-10 05:30:12] [config] valid-freq: 10000
[2022-03-10 05:30:12] [config] valid-log: /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.valid1.log
[2022-03-10 05:30:12] [config] valid-max-length: 100
[2022-03-10 05:30:12] [config] valid-metrics:
[2022-03-10 05:30:12] [config]   - perplexity
[2022-03-10 05:30:12] [config] valid-mini-batch: 16
[2022-03-10 05:30:12] [config] valid-reset-stalled: false
[2022-03-10 05:30:12] [config] valid-script-args:
[2022-03-10 05:30:12] [config]   []
[2022-03-10 05:30:12] [config] valid-script-path: ""
[2022-03-10 05:30:12] [config] valid-sets:
[2022-03-10 05:30:12] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/val/Tatoeba-dev-v2021-08-07.src.spm32k
[2022-03-10 05:30:12] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/val/Tatoeba-dev-v2021-08-07.trg.spm32k
[2022-03-10 05:30:12] [config] valid-translation-output: ""
[2022-03-10 05:30:12] [config] version: v1.10.24; 12a1bfa 2021-10-11 16:59:52 +0100
[2022-03-10 05:30:12] [config] vocabs:
[2022-03-10 05:30:12] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-03-10 05:30:12] [config]   - /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-03-10 05:30:12] [config] word-penalty: 0
[2022-03-10 05:30:12] [config] word-scores: false
[2022-03-10 05:30:12] [config] workspace: 15000
[2022-03-10 05:30:12] [config] Loaded model has been created with Marian v1.10.24; 12a1bfa 2021-10-11 16:59:52 +0100
[2022-03-10 05:30:12] Using synchronous SGD
[2022-03-10 05:30:12] [comm] Compiled without MPI support. Running as a single process on r17g05.bullx
[2022-03-10 05:30:12] Synced seed 1111
[2022-03-10 05:30:12] [data] Loading vocabulary from JSON/Yaml file /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-03-10 05:30:13] [data] Setting vocabulary size for input 0 to 55,026
[2022-03-10 05:30:13] [data] Loading vocabulary from JSON/Yaml file /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.vocab.yml
[2022-03-10 05:30:13] [data] Setting vocabulary size for input 1 to 55,026
[2022-03-10 05:30:13] [batching] Collecting statistics for batch fitting with step size 10
[2022-03-10 05:30:14] [memory] Extending reserved space to 15104 MB (device gpu0)
[2022-03-10 05:30:16] [memory] Extending reserved space to 15104 MB (device gpu1)
[2022-03-10 05:30:16] [comm] Using NCCL 2.8.3 for GPU communication
[2022-03-10 05:30:16] [comm] Using global sharding
[2022-03-10 05:30:17] [comm] NCCLCommunicators constructed successfully
[2022-03-10 05:30:17] [training] Using 2 GPUs
[2022-03-10 05:30:17] [logits] Applying loss function for 1 factor(s)
[2022-03-10 05:30:17] [memory] Reserving 887 MB, device gpu0
[2022-03-10 05:30:19] [gpu] 16-bit TensorCores enabled for float32 matrix operations
[2022-03-10 05:30:19] [memory] Reserving 887 MB, device gpu0
[2022-03-10 05:30:47] [batching] Done. Typical MB size is 27,468 target words
[2022-03-10 05:30:47] [memory] Extending reserved space to 15104 MB (device gpu0)
[2022-03-10 05:30:47] [memory] Extending reserved space to 15104 MB (device gpu1)
[2022-03-10 05:30:47] [comm] Using NCCL 2.8.3 for GPU communication
[2022-03-10 05:30:47] [comm] Using global sharding
[2022-03-10 05:30:48] [comm] NCCLCommunicators constructed successfully
[2022-03-10 05:30:48] [training] Using 2 GPUs
[2022-03-10 05:30:48] Loading model from /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-10 05:30:50] Loading model from /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-10 05:30:58] Allocating memory for general optimizer shards
[2022-03-10 05:30:58] [memory] Reserving 443 MB, device gpu0
[2022-03-10 05:30:58] [memory] Reserving 443 MB, device gpu1
[2022-03-10 05:30:58] Loading Adam parameters
[2022-03-10 05:30:59] [memory] Reserving 887 MB, device gpu0
[2022-03-10 05:30:59] [memory] Reserving 887 MB, device gpu1
[2022-03-10 05:30:59] [memory] Reserving 887 MB, device gpu0
[2022-03-10 05:31:00] [memory] Reserving 887 MB, device gpu1
[2022-03-10 05:31:00] [training] Master parameters and optimizers restored from training checkpoint /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-10 05:31:00] Training started
[2022-03-10 05:31:28] [training] Batches are processed as 1 process(es) x 2 devices/process
[2022-03-10 05:31:28] [memory] Reserving 887 MB, device gpu0
[2022-03-10 05:31:28] [memory] Reserving 887 MB, device gpu1
[2022-03-10 05:31:30] Parameter type float32, optimization type float32, casting types false
[2022-03-10 09:47:52] Ep. 1 : Up. 1130000 : Sen. 8,809,207 : Cost 2.28024626 : Time 15425.03s : 14722.66 words/s : gNorm 0.6116
[2022-03-10 09:47:52] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-10 09:47:56] Saving Adam parameters
[2022-03-10 09:48:00] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-10 09:48:11] [valid] Ep. 1 : Up. 1130000 : perplexity : 2.01727 : stalled 2 times (last best: 2.01699)
[2022-03-10 14:04:36] Ep. 1 : Up. 1140000 : Sen. 17,604,389 : Cost 2.27874494 : Time 15403.98s : 14740.84 words/s : gNorm 0.6320
[2022-03-10 14:04:37] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-10 14:04:45] Saving Adam parameters
[2022-03-10 14:04:49] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-10 14:05:02] [valid] Ep. 1 : Up. 1140000 : perplexity : 2.01756 : stalled 3 times (last best: 2.01699)
[2022-03-10 18:20:50] Ep. 1 : Up. 1150000 : Sen. 26,392,815 : Cost 2.27961445 : Time 15373.17s : 14764.91 words/s : gNorm 0.6188
[2022-03-10 18:20:50] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-10 18:20:53] Saving Adam parameters
[2022-03-10 18:20:57] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-10 18:21:07] [valid] Ep. 1 : Up. 1150000 : perplexity : 2.01714 : stalled 4 times (last best: 2.01699)
[2022-03-10 22:36:26] Ep. 1 : Up. 1160000 : Sen. 35,198,052 : Cost 2.27842474 : Time 15335.92s : 14810.31 words/s : gNorm 0.5846
[2022-03-10 22:36:26] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-10 22:36:29] Saving Adam parameters
[2022-03-10 22:36:33] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-10 22:36:44] [valid] Ep. 1 : Up. 1160000 : perplexity : 2.0176 : stalled 5 times (last best: 2.01699)
[2022-03-11 02:51:48] Ep. 1 : Up. 1170000 : Sen. 44,010,124 : Cost 2.27871823 : Time 15321.89s : 14822.37 words/s : gNorm 0.6364
[2022-03-11 02:51:48] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-11 02:51:55] Saving Adam parameters
[2022-03-11 02:52:01] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-11 02:52:20] [valid] Ep. 1 : Up. 1170000 : perplexity : 2.01742 : stalled 6 times (last best: 2.01699)
[2022-03-11 07:07:10] Ep. 1 : Up. 1180000 : Sen. 52,814,651 : Cost 2.27874756 : Time 15322.74s : 14812.14 words/s : gNorm 0.6424
[2022-03-11 07:07:11] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-11 07:07:14] Saving Adam parameters
[2022-03-11 07:07:17] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-11 07:07:28] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-03-11 07:07:31] [valid] Ep. 1 : Up. 1180000 : perplexity : 2.01617 : new best
[2022-03-11 11:22:48] Ep. 1 : Up. 1190000 : Sen. 61,603,455 : Cost 2.27795553 : Time 15337.63s : 14812.01 words/s : gNorm 0.6480
[2022-03-11 11:22:48] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-11 11:22:52] Saving Adam parameters
[2022-03-11 11:22:55] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-11 11:23:13] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-03-11 11:23:15] [valid] Ep. 1 : Up. 1190000 : perplexity : 2.0159 : new best
[2022-03-11 15:38:25] Ep. 1 : Up. 1200000 : Sen. 70,427,669 : Cost 2.27808118 : Time 15336.87s : 14799.92 words/s : gNorm 0.6289
[2022-03-11 15:38:25] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-11 15:38:28] Saving Adam parameters
[2022-03-11 15:38:32] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-11 15:38:42] [valid] Ep. 1 : Up. 1200000 : perplexity : 2.01633 : stalled 1 times (last best: 2.0159)
[2022-03-11 19:54:04] Ep. 1 : Up. 1210000 : Sen. 79,211,349 : Cost 2.27724600 : Time 15338.90s : 14812.16 words/s : gNorm 0.6383
[2022-03-11 19:54:04] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-11 19:54:08] Saving Adam parameters
[2022-03-11 19:54:11] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-11 19:54:23] [valid] Ep. 1 : Up. 1210000 : perplexity : 2.01659 : stalled 2 times (last best: 2.0159)
[2022-03-12 00:09:25] Ep. 1 : Up. 1220000 : Sen. 88,016,176 : Cost 2.27725768 : Time 15320.48s : 14824.66 words/s : gNorm 0.6409
[2022-03-12 00:09:25] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-12 00:09:32] Saving Adam parameters
[2022-03-12 00:09:39] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-12 00:09:50] [valid] Ep. 1 : Up. 1220000 : perplexity : 2.01605 : stalled 3 times (last best: 2.0159)
[2022-03-12 04:24:56] Ep. 1 : Up. 1230000 : Sen. 96,828,380 : Cost 2.27751231 : Time 15331.29s : 14814.47 words/s : gNorm 0.6658
[2022-03-12 04:24:56] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-12 04:25:00] Saving Adam parameters
[2022-03-12 04:25:03] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-12 04:25:16] [valid] Ep. 1 : Up. 1230000 : perplexity : 2.01666 : stalled 4 times (last best: 2.0159)
[2022-03-12 08:40:29] Ep. 1 : Up. 1240000 : Sen. 105,605,925 : Cost 2.27617407 : Time 15333.11s : 14812.03 words/s : gNorm 0.6107
[2022-03-12 08:40:29] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-12 08:40:33] Saving Adam parameters
[2022-03-12 08:40:36] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-12 08:40:49] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-03-12 08:40:52] [valid] Ep. 1 : Up. 1240000 : perplexity : 2.01568 : new best
[2022-03-12 12:55:58] Ep. 1 : Up. 1250000 : Sen. 114,405,705 : Cost 2.27684712 : Time 15329.31s : 14806.40 words/s : gNorm 0.6435
[2022-03-12 12:55:59] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-12 12:56:02] Saving Adam parameters
[2022-03-12 12:56:05] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-12 12:56:16] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.best-perplexity.npz
[2022-03-12 12:56:18] [valid] Ep. 1 : Up. 1250000 : perplexity : 2.01557 : new best
[2022-03-12 17:11:26] Ep. 1 : Up. 1260000 : Sen. 123,216,854 : Cost 2.27692056 : Time 15327.17s : 14820.25 words/s : gNorm 0.6360
[2022-03-12 17:11:26] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-12 17:11:29] Saving Adam parameters
[2022-03-12 17:11:33] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-12 17:11:43] [valid] Ep. 1 : Up. 1260000 : perplexity : 2.01559 : stalled 1 times (last best: 2.01557)
[2022-03-12 21:27:19] Ep. 1 : Up. 1270000 : Sen. 132,035,340 : Cost 2.27656817 : Time 15353.01s : 14800.36 words/s : gNorm 0.6331
[2022-03-12 21:27:19] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-12 21:27:23] Saving Adam parameters
[2022-03-12 21:27:27] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-12 21:27:38] [valid] Ep. 1 : Up. 1270000 : perplexity : 2.01708 : stalled 2 times (last best: 2.01557)
[2022-03-13 01:42:34] Ep. 1 : Up. 1280000 : Sen. 140,844,597 : Cost 2.27684593 : Time 15314.40s : 14819.42 words/s : gNorm 0.6181
[2022-03-13 01:42:34] Saving model weights and runtime parameters to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz
[2022-03-13 01:42:37] Saving Adam parameters
[2022-03-13 01:42:41] [training] Saving training checkpoint to /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz and /scratch/project_2001194/Opus-MT-train/tatoeba/work/eng-spa/opusTCv20210807+bt.spm32k-spm32k.transformer-big.model1.npz.optimizer.npz
[2022-03-13 01:42:52] [valid] Ep. 1 : Up. 1280000 : perplexity : 2.01577 : stalled 3 times (last best: 2.01557)
